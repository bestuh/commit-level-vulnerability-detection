	char data[8];
	int brightness;
		return -EIO;
			 ret);
	}
		return -EIO;
			 data[4]);
	}
	return brightness;
	}
}
	char data[8];
	const char *macro_mode;
		return -EIO;
			 ret);
	}
		return -EIO;
			 data[0]);
	}
	return snprintf(buf, PAGE_SIZE, "%s\n", macro_mode);
}
	char data[8];
	int current_profile;
		return -EIO;
			 ret);
	}
		return -EIO;
			 data[7]);
	}
	return snprintf(buf, PAGE_SIZE, "%d\n", current_profile);
}
	if (!fepriv)
		return;

	kfree(fepriv);
int ocfs2_get_block(struct inode *inode, sector_t iblock,
			       struct buffer_head *bh_result, int create)
		down_read(&oi->ip_alloc_sem);
		up_read(&oi->ip_alloc_sem);
{
	cpumask_clear(&mm->context.cpu_attach_mask);
	mm->context.asce_bits = _ASCE_TABLE_LENGTH | _ASCE_USER_BITS;
#endif
	if (oldmm->context.asce_limit < mm->context.asce_limit)
		crst_table_downgrade(mm, oldmm->context.asce_limit);
	spin_lock_init(&mm->context.list_lock);
	INIT_LIST_HEAD(&mm->context.pgtable_list);
	INIT_LIST_HEAD(&mm->context.gmap_list);
	return (pgd_t *) crst_table_alloc(mm);
{
}
#define pgd_free(mm, pgd) crst_table_free(mm, (unsigned long *) pgd)
		ret = sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;
		ret = min_t(int, ret, scat->length - conn->c_xmit_data_off);
		return ret;
		scat = &rm->data.op_sg[sg];
	}
EXPORT_SYMBOL(snd_seq_device_load_drivers);
#else
#define queue_autoload_drivers() /* NOP */
#endif
	put_device(&dev->dev);
	if (
		kmem_cache_free(rds_conn_slab, conn);
	/* racing with another thread binding seems ok here */
	if (daddr == 0 || rs->rs_bound_addr == 0) {
	if (daddr == 0 || rs->rs_bound_addr == 0) {
		ret = -ENOTCONN; /* XXX not a great errno */
	}
	if (iov_count) {
{
	NAPI_GRO_CB(skb)->frag0 = NULL;
	NAPI_GRO_CB(skb)->frag0_len = 0;
}
	}
	sysctl_head_finish(head);
#define KEY_FLAG_INSTANTIATED	0	/* set if key has been instantiated */
#define KEY_FLAG_DEAD		1	/* set if key type has been deleted */
#define KEY_FLAG_REVOKED	2	/* set if key had been revoked */
#define KEY_FLAG_IN_QUOTA	3	/* set if key consumes quota */
#define KEY_FLAG_USER_CONSTRUCT	4	/* set if key is being constructed in userspace */
#define KEY_FLAG_NEGATIVE	5	/* set if key is negative */
#define KEY_FLAG_ROOT_CAN_CLEAR	6	/* set if key can be cleared by root without permission */
#define KEY_FLAG_INVALIDATED	7	/* set if key has been invalidated */
#define KEY_FLAG_BUILTIN	8	/* set if key is built in to the kernel */
#define KEY_FLAG_ROOT_CAN_INVAL	9	/* set if key can be invalidated by root without permission */
#define KEY_FLAG_KEEP		10	/* set if key should not be removed */
#define KEY_FLAG_UID_KEYRING	11	/* set if key is a user or user session keyring */
	unsigned long		flags;		/* status flags (change with bitops) */
 * key_is_instantiated - Determine if a key has been positively instantiated
static inline bool key_is_instantiated(const struct key *key)
{
	return test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&
		!test_bit(KEY_FLAG_NEGATIVE, &key->flags);
{
}
	if (key_is_instantiated(key)) {
	seq_puts(m, key->description);
	key_payload_reserve(key, 0);
	if (key_is_instantiated(key))
	if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
		return -ENOKEY;
			list_entry(keys->next, struct key, graveyard_link);
		list_del(&key->graveyard_link);
		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&
		    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&
		    key->type->destroy)
			key->type->destroy(key);
		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
		atomic_dec(&key->user->nkeys);
			atomic_dec(&key->user->nikeys);
	if (!test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
			set_bit(KEY_FLAG_INSTANTIATED, &key->flags);
			atomic_inc(&key->user->nikeys);
	if (!test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
		smp_wmb();
		set_bit(KEY_FLAG_NEGATIVE, &key->flags);
		set_bit(KEY_FLAG_INSTANTIATED, &key->flags);
		atomic_inc(&key->user->nikeys);
		now = current_kernel_time();
		/* updating a negative key instantiates it */
		clear_bit(KEY_FLAG_NEGATIVE, &key->flags);
	if (ret == 0)
		/* updating a negative key instantiates it */
		clear_bit(KEY_FLAG_NEGATIVE, &key->flags);
	if (ret == 0)
	if (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {
		ret = -ENOKEY;
		goto error2;
	}
		if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
			atomic_dec(&key->user->nikeys);
	if (key_is_instantiated(keyring)) {
		if (keyring->keys.nr_leaves_on_tree != 0)
	unsigned long kflags = key->flags;
	const struct key *key = keyring_ptr_to_key(object);
		if (kflags & (1 << KEY_FLAG_NEGATIVE)) {
			smp_rmb();
			kleave(" = %d [neg]", ctx->skipped_ret);
	char xbuf[16];
	int rc;
#define showflag(KEY, LETTER, FLAG) \
		   showflag(key, 'I', KEY_FLAG_INSTANTIATED),
		   key->serial,
		   showflag(key, 'R', KEY_FLAG_REVOKED),
		   showflag(key, 'N', KEY_FLAG_NEGATIVE),
		   showflag(key, 'U', KEY_FLAG_USER_CONSTRUCT),
		   showflag(key, 'i', KEY_FLAG_INVALIDATED),
	    !test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
	if (!(lflags & KEY_LOOKUP_PARTIAL) &&
		goto invalid_key;
	if (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {
		smp_rmb();
	}
		return -ERESTARTSYS;
	return key_validate(key);
	if (key_is_instantiated(key))
	seq_puts(m, key->description);
		seq_printf(m, " pid:%d ci:%zu", rka->pid, rka->callout_len);
	if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
		return -ENOKEY;
	if (!test_bit(KEY_FLAG_NEGATIVE, &key->flags))
	key->expiry = prep->expiry;
		zap = dereference_key_locked(key);
	if (key_is_instantiated(key))
	seq_puts(m, key->description);
		seq_printf(m, ": %u", key->datalen);
			__mark_reg_known(regs + insn->dst_reg, insn->imm);
			regs[insn->dst_reg].type = SCALAR_VALUE;
		cursor = shortcut->back_pointer;
	} else {
	BUG_ON(!ptr);
	}
	node = assoc_array_ptr_to_node(cursor);
int hns_nic_net_xmit_hw(struct net_device *ndev,
			struct sk_buff *skb,
			struct hns_nic_ring_data *ring_data)
{
	wmb(); /* commit all data before submit */
	int ret;
	ret = hns_nic_net_xmit_hw(ndev, skb,
				  &tx_ring_data(priv, skb->queue_mapping));
	if (ret == NETDEV_TX_OK) {
		netif_trans_update(ndev);
		ndev->stats.tx_bytes += skb->len;
		ndev->stats.tx_packets++;
	}
	return (netdev_tx_t)ret;
	assert(skb->queue_mapping < ndev->ae_handle->q_num);
}
int hns_nic_net_xmit_hw(struct net_device *ndev,
			struct sk_buff *skb,
			struct hns_nic_ring_data *ring_data);
int hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h);
static int pcrypt_init_instance(struct crypto_instance *inst,
static void pcrypt_free(struct crypto_instance *inst)
{
	struct pcrypt_instance_ctx *ctx = crypto_instance_ctx(inst);

	crypto_drop_aead(&ctx->spawn);
	kfree(inst);
}

	.free = pcrypt_free,
	.state_renewal_ops = &nfs41_state_renewal_ops,
};
static inline void __d_set_inode_and_type(struct dentry *dentry,
	const char *old_name;
	struct dentry *dentry = NULL, *trap;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_oldname_free(old_name);
	if (error) {
		goto exit;
	fsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,
	d_move(old_dentry, dentry);
		d_is_dir(old_dentry),
	fsnotify_oldname_free(old_name);
		NULL, old_dentry);
	unlock_rename(new_dir, old_dir);
	const unsigned char *old_name;
	unsigned max_links = new_dir->i_sb->s_max_links;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
	dget(new_dentry);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	if (!error) {
			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
	fsnotify_oldname_free(old_name);
	}
	else if (p_inode->i_fsnotify_mask & mask) {
		if (path)
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
		else
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
	}
#if defined(CONFIG_FSNOTIFY)	/* notify helpers */

/*
 * fsnotify_oldname_init - save off the old filename before we change it
 */
static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)
{
	return kstrdup(name, GFP_KERNEL);
}

/*
 * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init
 */
static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
	kfree(old_name);
}

#else	/* CONFIG_FSNOTIFY */

static inline const char *fsnotify_oldname_init(const unsigned char *name)
{
	return NULL;
}

static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
}

#endif	/*  CONFIG_FSNOTIFY */

	struct n_hdlc_buf *link;
struct n_hdlc_buf {
	int		  count;
	struct n_hdlc_buf *head;
	struct n_hdlc_buf *tail;
struct n_hdlc_buf_list {
	int		  count;
 * @tbuf - currently transmitting tx buffer
	struct n_hdlc_buf	*tbuf;
static void n_hdlc_buf_put(struct n_hdlc_buf_list *list,
 	spin_lock_irqsave(&n_hdlc->tx_buf_list.spinlock, flags);
	spin_unlock_irqrestore(&n_hdlc->tx_buf_list.spinlock, flags);
	kfree(n_hdlc->tbuf);
	/* get current transmit buffer or get new transmit */
	/* buffer from list of pending transmit buffers */

	tbuf = n_hdlc->tbuf;
	if (!tbuf)
		tbuf = n_hdlc_buf_get(&n_hdlc->tx_buf_list);

	while (tbuf) {
			n_hdlc->tbuf = tbuf;
		if (actual == -ERESTARTSYS) {
			break;

			/* this tx buffer is done */
			n_hdlc->tbuf = NULL;

			n_hdlc_buf_put(&n_hdlc->tx_free_buf_list, tbuf);

			/* buffer not accepted by driver */
			/* set this buffer as pending buffer */
			n_hdlc->tbuf = tbuf;
					__FILE__,__LINE__,tbuf);
			break;

	unsigned long flags;
	if (debuglevel >= DEBUG_LEVEL_INFO)
		if (n_hdlc->rx_buf_list.head)
			count = n_hdlc->rx_buf_list.head->count;
		spin_lock_irqsave(&n_hdlc->rx_buf_list.spinlock,flags);
		else
		if (n_hdlc->tx_buf_list.head)
			count += n_hdlc->tx_buf_list.head->count;
		spin_lock_irqsave(&n_hdlc->tx_buf_list.spinlock,flags);
		spin_unlock_irqrestore(&n_hdlc->tx_buf_list.spinlock,flags);
		if (n_hdlc->rx_buf_list.head)
			mask |= POLLIN | POLLRDNORM;	/* readable */
				n_hdlc->tx_free_buf_list.head)
		if (!tty_is_writelocked(tty) &&
			mask |= POLLOUT | POLLWRNORM;	/* writable */

	spin_lock_init(&n_hdlc->tx_buf_list.spinlock);
 * @list - pointer to buffer list
static void n_hdlc_buf_put(struct n_hdlc_buf_list *list,
			   struct n_hdlc_buf *buf)
	spin_lock_irqsave(&list->spinlock,flags);

	buf->link=NULL;
	if (list->tail)
		list->tail->link = buf;
	else
		list->head = buf;
	list->tail = buf;
	(list->count)++;

	spin_unlock_irqrestore(&list->spinlock,flags);

	unsigned long flags;
}	/* end of n_hdlc_buf_put() */
 * @list - pointer to HDLC buffer list
static struct n_hdlc_buf* n_hdlc_buf_get(struct n_hdlc_buf_list *list)
{
	spin_lock_irqsave(&list->spinlock,flags);

	buf = list->head;
	struct n_hdlc_buf *buf;
	if (buf) {
		list->head = buf->link;
		(list->count)--;
	if (buf) {
	}
	if (!list->head)
		list->tail = NULL;

	spin_unlock_irqrestore(&list->spinlock,flags);
	}
	return buf;

#define MSR_TM_ACTIVE(x) (((x) & MSR_TS_MASK) != 0) /* Transaction active? */
#define MSR_TM_TRANSACTIONAL(x)	(((x) & MSR_TS_MASK) == MSR_TS_T)
	/* Get the top half of the MSR */
	if (__get_user(msr_hi, &tm_sr->mc_gregs[PT_MSR]))
		return 1;
	/* Pull in MSR TM from user context */
	regs->msr = (regs->msr & ~MSR_TS_MASK) | ((msr_hi<<32) & MSR_TS_MASK);
	err |= __get_user(msr, &sc->gp_regs[PT_MSR]);
	aio_free_ring(ctx);
	int vm_shared = dst_vma->vm_flags & VM_SHARED;
		struct address_space *mapping = dst_vma->vm_file->f_mapping;
		pgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);
	if (vm_shared) {
		ret = huge_add_to_page_cache(page, mapping, idx);
	pch->chan = chan;
	chan->ppp = pch;
int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
	if (ret == -EEXIST)
				    btrfs_inode_type(inode), index);
		goto fail_dir_item;
		return -ENOTEMPTY;
	down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);
	BUG_ON(ret == -EEXIST);
	if (ret) {
	atomic_t ucount[UCOUNT_COUNTS];
	spin_unlock_irq(&ucounts_lock);
		spin_lock_irqsave(&ucounts_lock, flags);
		spin_unlock_irqrestore(&ucounts_lock, flags);
		cifs_dbg(VFS, "BAD_NETWORK_NAME: %s\n", tree);
				fdput(f);
#define IPSKB_FORWARDED		1
#define IPSKB_XFRM_TUNNEL_SIZE	2
#define IPSKB_XFRM_TRANSFORMED	4
#define IPSKB_FRAG_COMPLETE	8
#define IPSKB_REROUTED		16
	if (rt->rt_flags&RTCF_DOREDIRECT && !opt->srr && !skb_sec_path(skb))
		ip_rt_send_redirect(skb);
	if (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&
	    (IN_DEV_SHARED_MEDIA(out_dev) ||
	     inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res)))) {
		flags |= RTCF_DOREDIRECT;
		do_cache = false;
	}
	    (IN_DEV_SHARED_MEDIA(out_dev) ||
		r->rtm_flags |= RTM_F_NOTIFY;
	if (stringset == ETH_SS_STATS)
{
		return ARRAY_SIZE(g_gmac_stats_string);
	if (stringset == ETH_SS_STATS)
{
		return ETH_PPE_STATIC_NUM;
	if (stringset == ETH_SS_STATS)
{
		return HNS_RING_STATIC_REG_NUM;
	if (stringset == ETH_SS_STATS)
{
		return ARRAY_SIZE(g_xgmac_stats_string);
static struct lock_class_key macsec_netdev_addr_lock_key;
	int i;
	u8 *data = (u8 *)(&dj_report->device_index);
	for (i = 0; i < report->field[0]->report_count; i++)
		report->field[0]->value[i] = data[i];
bool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
{
static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
	else
		iif = skb->dev->ifindex;

		err = PTR_ERR(fn);
		goto out;
	trace_kvm_emulate_insn_failed(vcpu);
		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
	if (keyring)
		__key_link_end(keyring, &key->index_key, edit);
		if (!mp->ports && !mp->mglist &&
		    netif_running(br->dev))
		if (!mp->ports && !mp->mglist &&
		    netif_running(br->dev))
	u8 offset[NF_CT_EXT_NUM];
	u8 len;
	struct rcu_head rcu;
	char data[0];
		return -EINVAL;
		list_del(&asoc->asocs);
	tm_reclaim(thr, thr->regs->msr, cause);
static void create_pit_timer(struct kvm_kpit_state *ps, u32 val, int is_period)
	struct kvm_timer *pt = &ps->pit_timer;
			create_pit_timer(ps, val, 0);
		if (!(ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)) {
		}
			create_pit_timer(ps, val, 1);
		if (!(ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)){
		}
	struct cm_id_private *cm_id_priv;

			: : "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
			"2:\n\t"
			:   "memory");
			: : "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
			"2:\n\t"
			:   "memory");

	asm volatile(xstate_fault
		     : "0" (0)
		     : "memory");

			: : "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
			"2:\n\t"
			:   "memory");
			: : "D" (fx), "m" (*fx), "a" (lmask), "d" (hmask)
			"2:\n\t"
			:   "memory");

	asm volatile(xstate_fault
		     : "0" (0)
		     : "memory");

static void sas_probe_devices(struct work_struct *work)
{
	struct sas_discovery_event *ev = to_sas_discovery_event(work);
	struct asd_sas_port *port = ev->port;

	clear_bit(DISCE_PROBE, &port->disc.pending);
	sas_discover_event(dev->port, DISCE_PROBE);
{
	struct sas_discovery_event *ev = to_sas_discovery_event(work);
	struct asd_sas_port *port = ev->port;
void sas_unregister_dev(struct asd_sas_port *port, struct domain_device *dev)
	SAS_DPRINTK("DONE DISCOVERY on port %d, pid:%d, result:%d\n", port->id,
}
		[DISCE_PROBE] = sas_probe_devices,
			sas_port_delete(phy->port);
		if (phy->port->num_phys == 0)
		phy->port = NULL;
	while (res == 0 && dev) {
	res = sas_find_bcast_dev(port_dev, &dev);
		struct expander_device *ex = &dev->ex_dev;

		dev = NULL;
		res = sas_find_bcast_dev(port_dev, &dev);
void sas_free_device(struct kref *kref);
			sas_unregister_dev(port, dev);
			continue;
		sas_unregister_domain_devices(port, gone);
		sas_port_delete(port->port);
	spin_lock_init(&port->phy_list_lock);
	DISCE_PROBE,
	DISCE_DESTRUCT,
	enum   sas_linkrate linkrate;
	struct list_head	phy_list;
};
	int mapping = (*event_map)[config];
	return mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;
#include <linux/init.h>
	return register_key_type(&key_type_big_key);
}
{
	big_key_skcipher = crypto_alloc_skcipher(big_key_alg_name,
						 0, CRYPTO_ALG_ASYNC);
	if (IS_ERR(big_key_skcipher)) {
		big_key_skcipher = NULL;
device_initcall(big_key_init);
	struct list_head auto_asconf_splist;
	spinlock_t addr_wq_lock;
	struct sk_buff_head pd_lobby;
	local_bh_disable();
	bh_lock_sock(sk);
	local_bh_enable();
	bh_unlock_sock(sk);
	if (val == 0 && sp->do_auto_asconf) {
	}
	return 0;
	sock_prot_inuse_add(net, sk->sk_prot, 1);
	if (net->sctp.default_auto_asconf) {
	if (net->sctp.default_auto_asconf) {
	local_bh_enable();
/* Cleanup any SCTP per socket resources.  */
static void sctp_destroy_sock(struct sock *sk)
		inet_sk_copy_descendant(newsk, oldsk);
		inet_sk_copy_descendant(newsk, oldsk);
	if (need_zonelists_rebuild)
		build_all_zonelists(NULL, zone);
		zone_pcp_update(zone);
	zone->zone_pgdat->node_present_pages += onlined_pages;
	if (onlined_pages) {
		kswapd_run(zone_to_nid(zone));
		node_set_state(zone_to_nid(zone), N_HIGH_MEMORY);
	}
		ipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));
		fptr->reserved = 0;
	flush_spe_to_thread(src);
			int max = vfio_pci_get_irq_count(vdev, hdr.index);
			    hdr.start >= max || hdr.start + hdr.count > max)
/* Maximum logical block in a file; ext4_extent's ee_block is __le32 */
 * returns allocated block in subsequent extent or EXT_MAX_BLOCK.
		if (b2 == EXT_MAX_BLOCK)
		b2 = ext4_ext_next_allocated_block(path);
			goto out;
		len1 = EXT_MAX_BLOCK - b1;
	if (b1 + len1 < b1) {
		newext->ee_len = cpu_to_le16(len1);
	while (block < last && block != EXT_MAX_BLOCK) {
		num = last - block;
			if (end == EXT_MAX_BLOCK) {
				ext_debug("  bad truncate %u:%u\n",
			if (end == EXT_MAX_BLOCK) {
				ext_debug("  bad truncate %u:%u\n",
			if (end != EXT_MAX_BLOCK) {
		if (num == 0) {
	err = ext4_ext_remove_space(inode, last_block, EXT_MAX_BLOCK);
		if (last_blk >= EXT_MAX_BLOCK)
			last_blk = EXT_MAX_BLOCK-1;
		last_blk = (start + len - 1) >> inode->i_sb->s_blocksize_bits;
		len_blks = ((ext4_lblk_t) last_blk) - start_blk + 1;
	if ((orig_start > EXT_MAX_BLOCK) ||
	    (donor_start > EXT_MAX_BLOCK) ||
	    (*len > EXT_MAX_BLOCK) ||
	    (orig_start + *len > EXT_MAX_BLOCK))  {
		ext4_debug("ext4 move extent: Can't handle over [%u] blocks "
			"[ino:orig %lu, donor %lu]\n", EXT_MAX_BLOCK,
		ext4_debug("ext4 move extent: Can't handle over [%u] blocks "
			orig_inode->i_ino, donor_inode->i_ino);
	/* 32-bit extent-start container, ee_block */
	res = 1LL << 32;
	res <<= blkbits;
	res -= 1;
	unsigned long secure;
	blkif->st_ds_req++;
				   GFP_KERNEL, secure);
		kvm_ioapic_scan_entry(vcpu, vcpu->arch.ioapic_handled_vectors);
			kvm_x86_ops->sync_pir_to_irr(vcpu);
	}
	for (pass = 0; pass < 10; pass++) {
		proglen = do_jit(prog, addrs, image, oldproglen, &ctx);
	if (serial->num_ports < 2)
		return -1;
				    midi->out_ep->name, err);
			return err;
{
	kfree(req->buf);
	kfree(req->buf);
	usb_ep_free_request(ep, req);
#define KEY_FLAG_KEEP		10	/* set if key should not be removed */
#define KEY_ALLOC_BYPASS_RESTRICTION	0x0008	/* Override the check on restricted keyrings */
		key->flags |= 1 << KEY_FLAG_BUILTIN;
 * All named keyrings in the current user namespace are searched, provided they
 * grant Search permission directly to the caller (unless this check is
 * skipped).  Keyrings whose usage points have reached zero or who have been
 * revoked are skipped.
{
					   KEY_NEED_SEARCH) < 0)
				continue;
						    KEY_ALLOC_IN_QUOTA,
						    cred, user_keyring_perm,
						    NULL, NULL);
					      KEY_ALLOC_IN_QUOTA,
					      cred, user_keyring_perm,
					      NULL, NULL);
	bl	load_fp_state
	bl	load_vr_state
	ld	r4, HSTATE_KVM_VCPU(r13)
	/* Turn on TM so we can access TFHAR/TFIAR/TEXASR */
	mfmsr	r8
	li	r0, 1
	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
	mtmsrd	r8

	b	2f
	/* Turn on TM. */
	mfmsr	r8
	li	r0, 1
	rldimi	r8, r0, MSR_TM_LG, 63-MSR_TM_LG
	mtmsrd	r8

	ld	r5, VCPU_MSR(r9)
	beq	1f	/* TM not active in guest. */

	li	r3, TM_CAUSE_KVM_RESCHED

	li	r5, 0
	mtmsrd	r5, 1

	/* All GPRs are volatile at this point. */
	TRECLAIM(R3)

	/* Temporarily store r13 and r9 so we have some regs to play with */
	SET_SCRATCH0(r13)
	GET_PACA(r13)
	std	r9, PACATMSCRATCH(r13)
	ld	r9, HSTATE_KVM_VCPU(r13)

	/* Get a few more GPRs free. */
	std	r29, VCPU_GPRS_TM(29)(r9)
	std	r30, VCPU_GPRS_TM(30)(r9)
	std	r31, VCPU_GPRS_TM(31)(r9)

	/* Save away PPR and DSCR soon so don't run with user values. */
	mfspr	r31, SPRN_PPR
	HMT_MEDIUM
	mfspr	r30, SPRN_DSCR
	ld	r29, HSTATE_DSCR(r13)
	mtspr	SPRN_DSCR, r29

	/* Save all but r9, r13 & r29-r31 */
	reg = 0
	.rept	29
	.if (reg != 9) && (reg != 13)
	std	reg, VCPU_GPRS_TM(reg)(r9)
	.endif
	reg = reg + 1
	.endr
	/* ... now save r13 */
	GET_SCRATCH0(r4)
	std	r4, VCPU_GPRS_TM(13)(r9)
	/* ... and save r9 */
	ld	r4, PACATMSCRATCH(r13)
	std	r4, VCPU_GPRS_TM(9)(r9)

	/* Reload stack pointer and TOC. */
	ld	r1, HSTATE_HOST_R1(r13)
	ld	r2, PACATOC(r13)

	/* Set MSR RI now we have r1 and r13 back. */
	li	r5, MSR_RI
	mtmsrd	r5, 1

	/* Save away checkpinted SPRs. */
	std	r31, VCPU_PPR_TM(r9)
	std	r30, VCPU_DSCR_TM(r9)
	mflr	r5
	mfcr	r6
	mfctr	r7
	mfspr	r8, SPRN_AMR
	mfspr	r10, SPRN_TAR
	std	r5, VCPU_LR_TM(r9)
	stw	r6, VCPU_CR_TM(r9)
	std	r7, VCPU_CTR_TM(r9)
	std	r8, VCPU_AMR_TM(r9)
	std	r10, VCPU_TAR_TM(r9)

	/* Restore r12 as trap number. */
	lwz	r12, VCPU_TRAP(r9)

	/* Save FP/VSX. */
	addi	r3, r9, VCPU_FPRS_TM
	bl	store_fp_state
	addi	r3, r9, VCPU_VRS_TM
	bl	store_vr_state
	mfspr	r6, SPRN_VRSAVE
	stw	r6, VCPU_VRSAVE_TM(r9)
1:
	/*
	 * We need to save these SPRs after the treclaim so that the software
	 * error code is recorded correctly in the TEXASR.  Also the user may
	 * change these outside of a transaction, so they must always be
	 * context switched.
	 */
	mfspr	r5, SPRN_TFHAR
	mfspr	r6, SPRN_TFIAR
	mfspr	r7, SPRN_TEXASR
	std	r5, VCPU_TFHAR(r9)
	std	r6, VCPU_TFIAR(r9)
	std	r7, VCPU_TEXASR(r9)
2:
BEGIN_FTR_SECTION
#endif
	if (pte_none(*pte))
		return 0;
	spinlock_t *ptl;
	ptl = huge_pte_lock(hstate_vma(vma), vma->vm_mm, (pte_t *)pmd);
	if (vmx->nested.vmxon) {
{
			return 1;
		kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,
		return 1;
	if (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,
 * If a process keyring is specified then this will be created if it doesn't
 * yet exist.  The old setting will be returned if successful.
 * Install a fresh thread keyring directly to new credentials.  This keyring is
 * allowed to overrun the quota.
 * Install a fresh thread keyring, discarding the old one.

 * Install a process keyring directly to a credentials struct.
 * Returns -EEXIST if there was already a process keyring, 0 if one installed,
 * and other value on any other error
	if (new->process_keyring)
 * Make sure a process keyring is installed for the current process.  The
 * existing process keyring is not replaced.
 * Returns 0 if there is a process keyring by the end of this function, some
 * error otherwise.
		return ret != -EEXIST ? ret : 0;
 * Install a session keyring directly to a credentials struct.
 * Install a session keyring, discarding the old one.  If a keyring is not
 * supplied, an empty one is invented.
	 * "NMI executing" but before IRET.
	/* Ah, it is within the NMI stack, treat it as nested */
	jb	first_nmi
	/* Clear "NMI executing". */
	movq	$0, 5*8(%rsp)
void unix_inflight(struct file *fp);
void unix_notinflight(struct file *fp);
void unix_gc(void);
	short			max;
	struct file		*fp[SCM_MAX_FD];
		fpl->max = SCM_MAX_FD;
	}
	}
	return num;
			fput(fpl->fp[i]);
		kfree(fpl);
		new_fpl->max = new_fpl->count;
	}
		unix_notinflight(scm->fp->fp[i]);
	for (i = scm->fp->count-1; i >= 0; i--)
}
		unix_inflight(scm->fp->fp[i]);
	for (i = scm->fp->count - 1; i >= 0; i--)
	return max_level;
void unix_inflight(struct file *fp)
{
	}
	spin_unlock(&unix_gc_lock);
void unix_notinflight(struct file *fp)
{
	}
	spin_unlock(&unix_gc_lock);
	lock_sock(sk);
	if (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))
	err = -EINVAL;
	if (sk->sk_state != TCP_CLOSE)
		sk->sk_send_head = NULL;
}
			if (flags & MSG_PEEK) {
#define BPF_MAX_VAR_OFF	(1ULL << 31)
#define BPF_MAX_VAR_SIZ	INT_MAX
	__update_reg_bounds(dst_reg);
	switch (opcode) {
		kmem_cache_free(kiocb_cachep, req);
		kmem_cache_free(kiocb_cachep, req);
	}
	}
}
	put_ioctx(ctx);
	work_done = xenvif_tx_action(vif, budget);
	vif->credit_bytes = vif->remaining_credit = ~0UL;
	xenvif_carrier_off(vif);
	netdev_err(vif->dev, "fatal error; disabling device\n");
}
			continue;
			xenvif_fatal_tx_err(vif);
		}
	if (priv->suspend)
	case 0x2f8:
		return true;
int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
			       "iommu failed to map pfn=%llx\n", pfn);
			goto unmap_pages;
unmap_pages:
	return r;
			return rold->umin_value == 0 &&
			       rold->umax_value == U64_MAX &&
			       rold->smin_value == S64_MIN &&
			       rold->smax_value == S64_MAX &&
#define netdev_printk(level, netdev, format, args...)		\
	dev_printk(level, (netdev)->dev.parent,			\
		   "%s: " format,				\
		   netdev_name(netdev), ##args)

#define netdev_emerg(dev, format, args...)			\
	netdev_printk(KERN_EMERG, dev, format, ##args)
#define netdev_alert(dev, format, args...)			\
	netdev_printk(KERN_ALERT, dev, format, ##args)
#define netdev_crit(dev, format, args...)			\
	netdev_printk(KERN_CRIT, dev, format, ##args)
#define netdev_err(dev, format, args...)			\
	netdev_printk(KERN_ERR, dev, format, ##args)
#define netdev_warn(dev, format, args...)			\
	netdev_printk(KERN_WARNING, dev, format, ##args)
#define netdev_notice(dev, format, args...)			\
	netdev_printk(KERN_NOTICE, dev, format, ##args)
#define netdev_info(dev, format, args...)			\
	netdev_printk(KERN_INFO, dev, format, ##args)
static void __net_exit netdev_exit(struct net *net)
	list_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {
		if (ack->subh.addip_hdr->serial == serial) {
	.quad sys32_ptrace	/* ptrace */
	.quad compat_sys_stime	/* stime */		/* 25 */
	.quad sys_alarm
asmlinkage long sys32_ptrace(long request, u32 pid, u32 addr, u32 data)
{
	struct task_struct *child;
	struct pt_regs *childregs;
{
	void __user *datap = compat_ptr(data);
	case PTRACE_TRACEME:
	case PTRACE_ATTACH:
	case PTRACE_KILL:
	case PTRACE_CONT:
	case PTRACE_SINGLESTEP:
	case PTRACE_SINGLEBLOCK:
	case PTRACE_DETACH:
	case PTRACE_SYSCALL:
	case PTRACE_OLDSETOPTIONS:
	case PTRACE_SETOPTIONS:
	case PTRACE_SET_THREAD_AREA:
	case PTRACE_GET_THREAD_AREA:
#ifdef X86_BTS
	case PTRACE_BTS_CONFIG:
	case PTRACE_BTS_STATUS:
	case PTRACE_BTS_SIZE:
	case PTRACE_BTS_GET:
	case PTRACE_BTS_CLEAR:
	case PTRACE_BTS_DRAIN:
#endif
		return sys_ptrace(request, pid, addr, data);

	default:
		return -EINVAL;

	case PTRACE_PEEKTEXT:
	case PTRACE_PEEKDATA:
	case PTRACE_POKEDATA:
	case PTRACE_POKETEXT:
	case PTRACE_POKEUSR:
	case PTRACE_PEEKUSR:
	case PTRACE_GETREGS:
	case PTRACE_SETREGS:
	case PTRACE_SETFPREGS:
	case PTRACE_GETFPREGS:
	case PTRACE_SETFPXREGS:
	case PTRACE_GETFPXREGS:
	case PTRACE_GETEVENTMSG:
	case PTRACE_SETSIGINFO:
	case PTRACE_GETSIGINFO:
		break;
	}

	child = ptrace_get_task_struct(pid);
	if (IS_ERR(child))
		return PTR_ERR(child);

	ret = ptrace_check_attach(child, request == PTRACE_KILL);
	if (ret < 0)
		goto out;

	childregs = task_pt_regs(child);

	switch (request) {
	default:
 out:
	put_task_struct(child);
#endif /* __KERNEL__ */
}
		ops->destroy(dev);
		return ret;
	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);
	if (pmd_none(pmdval))
#endif
		return 1;
		if (!pmd_trans_huge(pmdval))
			pmd_clear_bad(pmd);
	if (unlikely(pmd_bad(pmdval))) {
		return 1;
	clear_user_return_notifier(tsk);
	stackend = end_of_stack(tsk);
	if (!rq->skip_clock_update) {
		int cpu = cpu_of(rq);
		u64 irq_time;
{
		rq->clock = sched_clock_cpu(cpu);
		irq_time = irq_time_cpu(cpu);
		if (rq->clock - irq_time > rq->clock_task)
			rq->clock_task = rq->clock - irq_time;
		sched_irq_time_avg_update(rq, irq_time);
	}
}
	if (test_tsk_need_resched(rq->curr))
		rq->skip_clock_update = 1;
	clear_tsk_need_resched(prev);
	next = pick_next_task(rq);
		++*switch_count;
static int ceph_key_preparse(struct key_preparsed_payload *prep)
		  const void *src2, size_t src2_len);
int ceph_crypto_init(void);
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
int xt_compat_check_entry_offsets(const void *base,
			     void __user **dstptr, unsigned int *size);
				  unsigned int target_offset,
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e, e->target_offset,
					    e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
/* see xt_check_entry_offsets */
int xt_compat_check_entry_offsets(const void *base,
				  unsigned int target_offset,
{
	const struct compat_xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
{
	const struct xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
		hwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0;
		hwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;
	} else {
	new->user	= get_uid(old->user);
	new->group_info	= get_group_info(old->group_info);
{
	if (policy->version != 0)
			return -ENOMEM;
				    unsigned int txqs, unsigned int rxqs);
#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \
static int dev_get_valid_name(struct net *net,
			      struct net_device *dev,
			      const char *name)
{
}
		return -ENODEV;
	b->mtu = dev->mtu;

	case NETDEV_CHANGEMTU:
		tipc_reset_bearer(net, b);
#include "core.h"
#include <net/genetlink.h>
#endif	/* _TIPC_BEARER_H */
		ub->ifindex = dev->ifindex;
		b->mtu = dev->mtu - sizeof(struct iphdr)
			sk_wait_data(sk, &timeo, NULL);
	u8 last_lock;
	struct i2c_client *i2c_client_demod;
	u8 obuf[0x40], ibuf[0x40];
	struct dvb_usb_device *d = i2c_get_adapdata(adap);
		return -ENODEV;
	if (mutex_lock_interruptible(&d->i2c_mutex) < 0)
		return -EAGAIN;
			obuf[0] = msg[0].buf[0] + 0x36;
			obuf[1] = 3;
			obuf[2] = 0;
			if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 0, 0) < 0)
		case SU3000_STREAM_CTRL:
				err("i2c transfer failed.");
			obuf[0] = 0x10;
			if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 2, 0) < 0)
		case DW2102_RC_QUERY:
				err("i2c transfer failed.");
			msg[0].buf[1] = ibuf[0];
			msg[0].buf[0] = ibuf[1];
				err("i2c transfer failed.");
			break;
			obuf[0] = 0x08;
			obuf[1] = msg[0].addr;
			obuf[2] = msg[0].len;
			memcpy(&obuf[3], msg[0].buf, msg[0].len);
			if (dvb_usb_generic_rw(d, obuf, msg[0].len + 3,
						ibuf, 1, 0) < 0)
				err("i2c transfer failed.");
		obuf[0] = 0x09;
		obuf[1] = msg[0].len;
		obuf[2] = msg[1].len;
		obuf[3] = msg[0].addr;
		memcpy(&obuf[4], msg[0].buf, msg[0].len);

		if (dvb_usb_generic_rw(d, obuf, msg[0].len + 4,
					ibuf, msg[1].len + 1, 0) < 0)
			err("i2c transfer failed.");
		memcpy(msg[1].buf, &ibuf[1], msg[1].len);
		break;
	}
	mutex_unlock(&d->i2c_mutex);
	u8 obuf[] = {0xde, 0};
	struct dw2102_state *state = (struct dw2102_state *)d->priv;
	if (i && !state->initialized) {
		state->initialized = 1;
		return dvb_usb_generic_rw(d, obuf, 2, NULL, 0, 0);
	}
	return 0;
}
static int su3000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x02;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(ds3000_attach, &su3000_ds3000_config,
					&d->dev->i2c_adap);
	if (d->fe_adap[0].fe == NULL)
		return -EIO;
	if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
				&dw2104_ts2020_config,
		info("Attached DS3000/TS2020!");
static int t220_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x87, 0 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x86;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x80;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x80;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(cxd2820r_attach, &cxd2820r_config,
					&d->dev->i2c_adap, NULL);
	if (d->fe_adap[0].fe != NULL) {
		if (dvb_attach(tda18271_attach, d->fe_adap[0].fe, 0x60,
					&d->dev->i2c_adap, &tda18271_config)) {
			info("Attached TDA18271HD/CXD2820R!");
static int m88rs2000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[] = { 0x51 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(m88rs2000_attach, &s421_m88rs2000_config,
					&d->dev->i2c_adap);
	if (d->fe_adap[0].fe == NULL)
		return -EIO;
	if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
				&dw2104_ts2020_config,
		info("Attached RS2000/TS2020!");
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x02;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	 !((__vma)->vm_flags & VM_NOHUGEPAGE))
	   ((__vma)->vm_flags & VM_HUGEPAGE))) &&			\
		goto out;
	VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
		    (vma->vm_flags & VM_NOHUGEPAGE)) {
			progress++;

		if (!vma->anon_vma || vma->vm_ops || vma->vm_file) {
			khugepaged_scan.address = vma->vm_end;
			progress++;
			continue;
		}
		VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
		if (hstart >= hend) {
			progress++;
			continue;
		}
		hend = vma->vm_end & HPAGE_PMD_MASK;
		if (khugepaged_scan.address < hstart)
		if (khugepaged_scan.address > hend) {
			khugepaged_scan.address = hend + HPAGE_PMD_SIZE;
			progress++;
			continue;
		}
		BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);
			khugepaged_scan.address = hstart;
	BUG_ON(khugepaged_scan.mm_slot != mm_slot);
	spin_lock(&khugepaged_mm_lock);
		BUG_ON(khugepaged_thread != current);
		mutex_unlock(&khugepaged_mutex);
		khugepaged_loop();
		BUG_ON(khugepaged_thread != current);
		khugepaged_loop();
	else if (vcpu->arch.apic_base & X2APIC_ENABLE) {
		msr_bitmap = vmx_msr_bitmap_nested;
		if (is_long_mode(vcpu))
	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
}
	if (enable_apicv) {
		for (msr = 0x800; msr <= 0x8ff; msr++)
			vmx_disable_intercept_msr_read_x2apic(msr);

		/* According SDM, in x2apic mode, the whole id reg is used.
		 * But in KVM, it only use the highest eight bits. Need to
		 * intercept it */
		vmx_enable_intercept_msr_read_x2apic(0x802);
		/* TMCCT */
		vmx_enable_intercept_msr_read_x2apic(0x839);
		/* TPR */
		vmx_disable_intercept_msr_write_x2apic(0x808);
		/* EOI */
		vmx_disable_intercept_msr_write_x2apic(0x80b);
		/* SELF-IPI */
		vmx_disable_intercept_msr_write_x2apic(0x83f);
	}
		evt->event.lun[1] = tpg->tport_tpgt & 0xFF;
		evt->event.lun[0] = 0x01;
		if (lun->unpacked_lun >= 256)
	struct vhost_scsi_tpg *tpg;
		return sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;
		rds_cong_map_updated(conn->c_fcong, ~(u64) 0);
	}
{
		return sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;
		rds_cong_map_updated(conn->c_fcong, ~(u64) 0);

	return sizeof(struct rds_header) + be32_to_cpu(rm->m_inc.i_hdr.h_len);
	rds_inc_put(&rm->m_inc);
}
		u32 mode = (*(u32 *)valp) & PSR_AA32_MODE_MASK;
	if (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {
		switch (mode) {
		case PSR_AA32_MODE_USR:
		case PSR_AA32_MODE_FIQ:
		case PSR_AA32_MODE_UND:
		case PSR_MODE_EL0t:
		case PSR_MODE_EL1h:
			break;
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		break;
			   unsigned long size)
static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
{
	end_gfn = gfn + (size >> PAGE_SHIFT);
	pfn     = gfn_to_pfn_memslot(slot, gfn);
	gfn    += 1;
		pfn = kvm_pin_pages(slot, gfn, page_size);
		if (is_error_noslot_pfn(pfn)) {
			       "iommu failed to map pfn=%llx\n", pfn);
			goto unmap_pages;
	int delta_munlocked;
	int nr = pagevec_count(pvec);
	struct pagevec pvec_putback;
				__munlock_isolation_failed(page);
		}
	delta_munlocked = -nr + pagevec_count(&pvec_putback);
		    (int)(req->tp_block_size -
			  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)
		if (po->tp_version >= TPACKET_V3 &&
			goto out;
	struct rb_node *node, *prev;
	NET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);
	node = &tp->ooo_last_skb->rbnode;
		rb_erase(node, &tp->out_of_order_queue);
		tcp_drop(sk, rb_to_skb(node));
		tcp_drop(sk, rb_to_skb(node));
		node = prev;
	if (!netlink_ns_capable(skb, net->user_ns, CAP_NET_ADMIN)) {
	cifsFileInfo_put(file->private_data);
	file->private_data = NULL;
{
#include <linux/buffer_head.h>
#include <linux/gfs2_ondisk.h>
	struct gfs2_sbd *sdp = GFS2_SB(inode);
	struct buffer_head *dibh = mp->mp_bh[0];
	const unsigned end_of_metadata = height - 1;
	int eob = 0;
				*ptr++ = cpu_to_be64(bn++);
			break;
static int empty_write_end(struct page *page, unsigned from,
			   unsigned to, int mode)
{
	struct inode *inode = page->mapping->host;
	struct gfs2_inode *ip = GFS2_I(inode);
	struct buffer_head *bh;
	unsigned offset, blksize = 1 << inode->i_blkbits;
	pgoff_t end_index = i_size_read(inode) >> PAGE_CACHE_SHIFT;

	zero_user(page, from, to-from);
	mark_page_accessed(page);

	if (page->index < end_index || !(mode & FALLOC_FL_KEEP_SIZE)) {
		if (!gfs2_is_writeback(ip))
			gfs2_page_add_databufs(ip, page, from, to);

		block_commit_write(page, from, to);
		return 0;
	}

	offset = 0;
	bh = page_buffers(page);
	while (offset < to) {
		if (offset >= from) {
			set_buffer_uptodate(bh);
			mark_buffer_dirty(bh);
			clear_buffer_new(bh);
			write_dirty_buffer(bh, WRITE);
		}
		offset += blksize;
		bh = bh->b_this_page;
	}

	offset = 0;
	bh = page_buffers(page);
	while (offset < to) {
		if (offset >= from) {
			wait_on_buffer(bh);
			if (!buffer_uptodate(bh))
				return -EIO;
		}
		offset += blksize;
		bh = bh->b_this_page;
	}
	return 0;
}

static int needs_empty_write(sector_t block, struct inode *inode)
{
	int error;
	struct buffer_head bh_map = { .b_state = 0, .b_blocknr = 0 };

	bh_map.b_size = 1 << inode->i_blkbits;
	error = gfs2_block_map(inode, block, &bh_map, 0);
	if (unlikely(error))
		return error;
	return !buffer_mapped(&bh_map);
}

static int write_empty_blocks(struct page *page, unsigned from, unsigned to,
			      int mode)
{
	struct inode *inode = page->mapping->host;
	unsigned start, end, next, blksize;
	sector_t block = page->index << (PAGE_CACHE_SHIFT - inode->i_blkbits);
	int ret;

	blksize = 1 << inode->i_blkbits;
	next = end = 0;
	while (next < from) {
		next += blksize;
		block++;
	}
	start = next;
	do {
		next += blksize;
		ret = needs_empty_write(block, inode);
		if (unlikely(ret < 0))
			return ret;
		if (ret == 0) {
			if (end) {
				ret = __block_write_begin(page, start, end - start,
							  gfs2_block_map);
				if (unlikely(ret))
					return ret;
				ret = empty_write_end(page, start, end, mode);
				if (unlikely(ret))
					return ret;
				end = 0;
			}
			start = next;
		}
		else
			end = next;
		block++;
	} while (next < to);

	if (end) {
		ret = __block_write_begin(page, start, end - start, gfs2_block_map);
		if (unlikely(ret))
			return ret;
		ret = empty_write_end(page, start, end, mode);
		if (unlikely(ret))
			return ret;
	}

	return 0;
}

	u64 start = offset >> PAGE_CACHE_SHIFT;
	u64 end = (offset + len - 1) >> PAGE_CACHE_SHIFT;
	pgoff_t curr;
	struct page *page;
	unsigned int from, to;

	if (!end_offset)
		end_offset = PAGE_CACHE_SIZE;
	int error;
		goto out;
	if (unlikely(error))
	curr = start;
	offset = start << PAGE_CACHE_SHIFT;
	from = start_offset;
	to = PAGE_CACHE_SIZE;
	while (curr <= end) {
						   AOP_FLAG_NOFS);
		if (unlikely(!page)) {
			error = -ENOMEM;
			goto out;
		}
		if (curr == end)
			to = end_offset;
		error = write_empty_blocks(page, from, to, mode);
		if (!error && offset + to > inode->i_size &&
		    !(mode & FALLOC_FL_KEEP_SIZE)) {
			i_size_write(inode, offset + to);
		}
		unlock_page(page);
		page_cache_release(page);
		if (error)
			goto out;
		curr++;
		offset += PAGE_CACHE_SIZE;
		from = 0;
			goto out;
	}
	}
	brelse(dibh);

out:
	return error;
	loff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;
	next = (next + 1) << sdp->sd_sb.sb_bsize_shift;
		max_bytes = bytes;
		al->al_requested = data_blocks + ind_blocks;
	BH_Escaped = BH_PrivateStart + 1,
};
TAS_BUFFER_FNS(Escaped, escaped)
#define PV_POWER5p	0x003B
#define PV_970FX	0x003C
		put_io_context(ioc);
	}
}
static int __net_init sctp_net_init(struct net *net)
	/* Initialize the control inode/socket for handling OOTB packets.  */
	if ((status = sctp_ctl_sock_init(net))) {
		pr_err("Failed to initialize the SCTP control sock\n");
		goto err_ctl_sock_init;
	}

err_ctl_sock_init:
	sctp_dbg_objcnt_exit(net);
	sctp_proc_exit(net);
static void __net_exit sctp_net_exit(struct net *net)
	/* Free the control endpoint.  */
	inet_ctl_sock_destroy(net->sctp.ctl_sock);

	status = sctp_v4_protosw_init();
	if (status)
err_add_protocol:
	sctp_v6_protosw_exit();
err_protosw_init:
	sctp_v4_pf_exit();
		if (perf_event_overflow(event, 1, &data, regs)) {
	if (alpha_perf_event_set_period(event, hwc, idx)) {
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
static void ptrace_hbptriggered(struct perf_event *bp, int unused,
				     struct perf_sample_data *data,
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, regs->ARM_pc);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, addr);
	if (fault & VM_FAULT_MAJOR)
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0, regs, addr);
	if (fault & VM_FAULT_MAJOR)
	else if (fault & VM_FAULT_MINOR)
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0, regs, addr);
	else if (fault & VM_FAULT_MINOR)
	if (perf_event_overflow(event, 0, data, regs))
		mipspmu->disable_event(idx);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return simulate_ll(regs, opcode);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return simulate_sc(regs, opcode);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		switch (rd) {
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return 0;
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		      1, 0, regs, 0);
			1, 0, regs, regs->cp0_badvaddr);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
			1, 0, xcp, 0);
      emul:
	MIPS_FPU_EMU_INC_STATS(emulated);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
	if (unlikely(fault & VM_FAULT_ERROR)) {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
				1, 0, regs, address);
	if (fault & VM_FAULT_MAJOR) {
		tsk->maj_flt++;
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
				1, 0, regs, address);
	} else {
		tsk->min_flt++;
			1, 0, regs, 0);					\
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
		__PPC_WARN_EMULATED(type);				\
			1, 0, regs, regs->dar);				\
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
		__PPC_WARN_EMULATED(type);				\
			       struct pt_regs *regs, int nmi)
static void record_and_restart(struct perf_event *event, unsigned long val,
{
		if (perf_event_overflow(event, nmi, &data, regs))
			power_pmu_stop(event, 0);
			record_and_restart(event, val, regs, nmi);
			found = 1;
		}
			       struct pt_regs *regs, int nmi)
static void record_and_restart(struct perf_event *event, unsigned long val,
{
		if (perf_event_overflow(event, nmi, &data, regs))
			fsl_emb_pmu_stop(event, 0);
				record_and_restart(event, val, regs, nmi);
				found = 1;
			} else {
void ptrace_triggered(struct perf_event *bp, int nmi,
#ifdef CONFIG_HAVE_HW_BREAKPOINT
		      struct perf_sample_data *data, struct pt_regs *regs)
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		current->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		current->min_flt++;
				     regs, address);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
	address = trans_exc_code & __FAIL_ADDR_MASK;
	flags = FAULT_FLAG_ALLOW_RETRY;
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			tsk->maj_flt++;
				      regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			tsk->min_flt++;
				      regs, address);
void ptrace_triggered(struct perf_event *bp, int nmi,
		      struct perf_sample_data *data, struct pt_regs *regs)
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0,
		unaligned_fixups_notify(current, instruction, regs);
			      regs, address);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		tsk->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		tsk->min_flt++;
				     regs, address);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		tsk->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		tsk->min_flt++;
				     regs, address);
		if (perf_event_overflow(event, 1, &data, regs))
			sparc_pmu_stop(event, 0);
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
		switch (dir) {
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
		addr = compute_effective_address(regs, insn);
		switch(dir) {
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
						 ((insn >> 25) & 0x1f));
		switch (asi) {
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	if (insn & 0x2000) {
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
		die_if_kernel("lddfmna from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
		die_if_kernel("stdfmna from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
		die_if_kernel("unfinished/unimplemented FPop from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			      regs, address);
		current->maj_flt++;
	} else {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			      regs, address);
		current->min_flt++;
	}
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			      regs, address);
		current->maj_flt++;
	} else {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			      regs, address);
		current->min_flt++;
	}
		if (perf_event_overflow(event, 1, &data, regs))
			x86_pmu_stop(event, 0);
		if (perf_event_overflow(event, 1, &data, regs))
			x86_pmu_stop(event, 0);
	if (perf_output_begin(&handle, event, header.size * (top - at), 1, 1))
		return 1;
	if (perf_event_overflow(event, 1, &data, &regs))
		x86_pmu_stop(event, 0);
		if (perf_event_overflow(event, 1, &data, regs))
			continue;
			x86_pmu_stop(event, 0);
static void kgdb_hw_overflow_handler(struct perf_event *event, int nmi,
		struct perf_sample_data *data, struct pt_regs *regs)
static void ptrace_triggered(struct perf_event *bp, int nmi,
			     struct perf_sample_data *data,
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			tsk->maj_flt++;
				      regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			tsk->min_flt++;
				      regs, address);
typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
					struct perf_sample_data *,
	int				nmi;
extern int perf_event_overflow(struct perf_event *event, int nmi,
				 struct perf_sample_data *data,
extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
static __always_inline void
{
		__perf_sw_event(event_id, nr, nmi, regs, addr);
		}
	}
	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
{
			     int nmi, int sample);
			     struct perf_event *event, unsigned int size,
extern void perf_output_end(struct perf_output_handle *handle);
perf_sw_event(u32 event_id, u64 nr, int nmi,
		     struct pt_regs *regs, u64 addr)			{ }
static inline void
static inline void
static void perf_event_output(struct perf_event *event, int nmi,
				struct perf_sample_data *data,
	if (perf_output_begin(&handle, event, header.size, nmi, 1))
		goto exit;
	ret = perf_output_begin(&handle, event, read_event.header.size, 0, 0);
	perf_event_header__init_id(&read_event.header, &sample, event);
	if (ret)
				task_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
				comm_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
				mmap_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
				throttle_event.header.size, 1, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
static int __perf_event_overflow(struct perf_event *event, int nmi,
				   int throttle, struct perf_sample_data *data,
		if (nmi) {
			event->pending_disable = 1;
			irq_work_queue(&event->pending);
		} else
			perf_event_disable(event);
		event->pending_kill = POLL_HUP;
	}
		event->overflow_handler(event, nmi, data, regs);
	if (event->overflow_handler)
	else
		perf_event_output(event, nmi, data, regs);
	else
		if (nmi) {
			event->pending_wakeup = 1;
			irq_work_queue(&event->pending);
		} else
			perf_event_wakeup(event);
	if (event->fasync && event->pending_kill) {
	}
int perf_event_overflow(struct perf_event *event, int nmi,
			  struct perf_sample_data *data,
	return __perf_event_overflow(event, nmi, 1, data, regs);
{
}
				    int nmi, struct perf_sample_data *data,
static void perf_swevent_overflow(struct perf_event *event, u64 overflow,
				    struct pt_regs *regs)
		if (__perf_event_overflow(event, nmi, throttle,
	for (; overflow; overflow--) {
					    data, regs)) {
			       int nmi, struct perf_sample_data *data,
static void perf_swevent_event(struct perf_event *event, u64 nr,
			       struct pt_regs *regs)
		return perf_swevent_overflow(event, 1, nmi, data, regs);
	if (nr == 1 && hwc->sample_period == 1 && !event->attr.freq)
	perf_swevent_overflow(event, 0, nmi, data, regs);
}
				    u64 nr, int nmi,
static void do_perf_sw_event(enum perf_type_id type, u32 event_id,
				    struct perf_sample_data *data,
			perf_swevent_event(event, nr, nmi, data, regs);
		if (perf_swevent_match(event, type, event_id, data, regs))
	}
void __perf_sw_event(u32 event_id, u64 nr, int nmi,
			    struct pt_regs *regs, u64 addr)
{
	do_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, nmi, &data, regs);
			perf_swevent_event(event, count, 1, &data, regs);
		if (perf_tp_event_match(event, &data, regs))
	}
		perf_swevent_event(bp, 1, 1, &sample, regs);
	if (!bp->hw.state && !perf_exclude_event(bp, regs))
}
			if (perf_event_overflow(event, 0, &data, regs))
		if (!(event->attr.exclude_idle && current->pid == 0))
				ret = HRTIMER_NORESTART;

	if (handle->nmi) {
		handle->event->pending_wakeup = 1;
		irq_work_queue(&handle->event->pending);
	} else
		perf_event_wakeup(handle->event);
}
		      int nmi, int sample)
		      struct perf_event *event, unsigned int size,
{
	handle->nmi	= nmi;
		perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);
		p->se.nr_migrations++;
	}
static void watchdog_overflow_callback(struct perf_event *event, int nmi,
		 struct perf_sample_data *data,
static void sample_hbp_handler(struct perf_event *bp, int nmi,
			       struct perf_sample_data *data,
	if (!opt->srr || !rt)
		return 0;
		usb_autopm_put_interface_async(to_usb_interface(hub->intfdev));
}
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
	skb_reserve(skb, sk->sk_prot->max_header);
	idr_destroy(&group->inotify_data.idr);
	free_uid(group->inotify_data.user);
	struct user_struct *user = group->inotify_data.user;
	atomic_dec(&user->inotify_devs);

static struct fsnotify_group *inotify_new_group(struct user_struct *user, unsigned int max_events)
{
	group->inotify_data.user = user;
	group->inotify_data.fa = NULL;
	struct user_struct *user;
	user = get_current_user();
	if (unlikely(atomic_read(&user->inotify_devs) >=
			inotify_max_user_instances)) {
		ret = -EMFILE;
		goto out_free_uid;
	}

	group = inotify_new_group(user, inotify_max_queued_events);
	if (IS_ERR(group)) {
		ret = PTR_ERR(group);
		goto out_free_uid;
	}

	atomic_inc(&user->inotify_devs);
	if (ret >= 0)
		return ret;
				  O_RDONLY | flags);
	fsnotify_put_group(group);
	atomic_dec(&user->inotify_devs);
out_free_uid:
	free_uid(user);
			int data_len = elt->length -
					sizeof(struct oz_get_desc_rsp) + 1;
			u16 offs = le16_to_cpu(get_unaligned(&body->offset));
			u16 total_size =
				(struct oz_get_desc_rsp *)usb_hdr;
				le16_to_cpu(get_unaligned(&body->total_size));
		npoints = (size - 6) / 8;
		msc->ntouches = 0;
	kfree_skb(skb);
 *	NET_RX_DROP     (packet was dropped)
	skb_set_dev(skb, dev);
			      void **p, void *end,
			      void *dbuf, void *ticket_buf)
	dlen = ceph_x_decrypt(secret, p, end, dbuf,
		dlen = ceph_x_decrypt(&old_key, p, end, ticket_buf,
		ceph_decode_32_safe(p, end, dlen, bad);
		ceph_decode_need(p, end, dlen, bad);
out:
	return ret;
	char *dbuf;
	char *ticket_buf;
	dbuf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
	if (!dbuf)
		return -ENOMEM;

	ret = -ENOMEM;
	ticket_buf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
	if (!ticket_buf)
		goto out_dbuf;

		ret = process_one_ticket(ac, secret, &p, end,
					 dbuf, ticket_buf);
	while (num--) {
		if (ret)
			goto out;
		if (ret)
	kfree(ticket_buf);
	kfree(dbuf);
		return PTR_ERR(th);
			    != arpt_next_entry(e)) {
				jumpstack[stackidx++] = e;
			    !(e->ip.flags & IPT_F_GOTO))
			if (table_base + v != ipt_next_entry(e) &&
			    !(e->ipv6.flags & IP6T_F_GOTO)) {
		}
		mutex_lock(&vdev->vdev_mutex);
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi);
void stop_discard_thread(struct f2fs_sb_info *sbi);
void clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi)
{
	__wait_discard_cmd(sbi, false);
}
	f2fs_wait_discard_bios(sbi);
out:
	f2fs_wait_discard_bios(sbi);
	struct nlmsghdr *nlh;
	int len, err = -ENOBUFS;
	mutex_unlock(nlk->cb_mutex);
	module_put(cb->module);
	mutex_unlock(nlk->cb_mutex);
	return 0;
	char *line, *p;
{
	int i;
	ssize_t ret = -EFAULT;
	int i;
	size_t len = iov_length(iv, count);
	size_t len = iov_length(iv, count);
	line = kmalloc(len + 1, GFP_KERNEL);
	if (line == NULL)
		return -ENOMEM;
	/*
	 * copy all vectors into a single string, to ensure we do
	 * not interleave our log line with other printk calls
	 */
	p = line;
	for (i = 0; i < count; i++) {
		if (copy_from_user(p, iv[i].iov_base, iv[i].iov_len))
	for (i = 0; i < count; i++) {
			goto out;
		p += iv[i].iov_len;
			goto out;
	}
	p[0] = '\0';
	}
	ret = printk("%s", line);
	/* printk can add a prefix */
	if (ret > len)
		ret = len;
out:
	kfree(line);
out:
	return ret;
#ifdef CONFIG_PRINTK
asmlinkage __printf(1, 0)
int vprintk(const char *fmt, va_list args);
asmlinkage __printf(1, 2) __cold
#define __LOG_BUF_LEN	(1 << CONFIG_LOG_BUF_SHIFT)

 * logbuf_lock protects log_buf, log_start, log_end, con_start and logged_chars
 * It is also used in interesting ways to provide interlocking in
 * console_unlock();.
 */
static DEFINE_RAW_SPINLOCK(logbuf_lock);

#define LOG_BUF_MASK (log_buf_len-1)
#define LOG_BUF(idx) (log_buf[(idx) & LOG_BUF_MASK])

/*
 * The indices into log_buf are not constrained to log_buf_len - they
 * must be masked before subscripting
 */
static unsigned log_start;	/* Index into log_buf: next char to be read by syslog() */
static unsigned con_start;	/* Index into log_buf: next char to be sent to consoles */
static unsigned log_end;	/* Index into log_buf: most-recently-written-char + 1 */

/*
#ifdef CONFIG_PRINTK
static char __log_buf[__LOG_BUF_LEN];
static int log_buf_len = __LOG_BUF_LEN;
static unsigned logged_chars; /* Number of chars produced since last read+clear operation */
static int saved_console_loglevel = -1;
static char *log_buf = __log_buf;
	VMCOREINFO_SYMBOL(log_end);
	VMCOREINFO_SYMBOL(logged_chars);
	VMCOREINFO_SYMBOL(log_buf_len);
}
	unsigned start, dest_idx, offset;

	offset = start = min(con_start, log_start);
	dest_idx = 0;
	while (start != log_end) {
		start++;
		dest_idx++;
	}
	log_start -= offset;
	con_start -= offset;
	log_end -= offset;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
int do_syslog(int type, char __user *buf, int len, bool from_file)
	unsigned i, j, limit, count;
	int do_clear = 0;
	char c;
{
	int error;
							(log_start - log_end));
		error = wait_event_interruptible(log_wait,
		if (error)
		i = 0;
		raw_spin_lock_irq(&logbuf_lock);
		while (!error && (log_start != log_end) && i < len) {
			c = LOG_BUF(log_start);
			log_start++;
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,buf);
			buf++;
			i++;
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (!error)
			error = i;
			goto out;
		break;
		do_clear = 1;
	case SYSLOG_ACTION_READ_CLEAR:
		count = len;
		if (count > log_buf_len)
			count = log_buf_len;
		raw_spin_lock_irq(&logbuf_lock);
		if (count > logged_chars)
			count = logged_chars;
		if (do_clear)
			logged_chars = 0;
		limit = log_end;
		/*
		 * __put_user() could sleep, and while we sleep
		 * printk() could overwrite the messages
		 * we try to copy to user space. Therefore
		 * the messages are copied in reverse. <manfreds>
		 */
		for (i = 0; i < count && !error; i++) {
			j = limit-1-i;
			if (j + log_buf_len < log_end)
				break;
			c = LOG_BUF(j);
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,&buf[count-1-i]);
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (error)
			break;
		error = i;
		if (i != count) {
			int offset = count-error;
			/* buffer overflow during copy, correct user buffer. */
			for (i = 0; i < error; i++) {
				if (__get_user(c,&buf[i+offset]) ||
				    __put_user(c,&buf[i])) {
					error = -EFAULT;
					break;
				}
				cond_resched();
			}
		}
		}
		break;
		logged_chars = 0;
		break;
	case SYSLOG_ACTION_CLEAR:
		error = log_end - log_start;
	case SYSLOG_ACTION_SIZE_UNREAD:
		break;
	syslog_data[2] = log_buf + log_end -
		(logged_chars < log_buf_len ? logged_chars : log_buf_len);
	syslog_data[3] = log_buf + log_end;
	syslog_data[1] = log_buf + log_buf_len;
}
/*
 * Call the console drivers on a range of log_buf
 */
static void __call_console_drivers(unsigned start, unsigned end)
{
	struct console *con;

	for_each_console(con) {
		if (exclusive_console && con != exclusive_console)
			continue;
		if ((con->flags & CON_ENABLED) && con->write &&
				(cpu_online(smp_processor_id()) ||
				(con->flags & CON_ANYTIME)))
			con->write(con, &LOG_BUF(start), end - start);
	}
}

 * Write out chars from start to end - 1 inclusive
 */
static void _call_console_drivers(unsigned start,
				unsigned end, int msg_log_level)
{
	trace_console(&LOG_BUF(0), start, end, log_buf_len);

	if ((msg_log_level < console_loglevel || ignore_loglevel) &&
			console_drivers && start != end) {
		if ((start & LOG_BUF_MASK) > (end & LOG_BUF_MASK)) {
			/* wrapped write */
			__call_console_drivers(start & LOG_BUF_MASK,
						log_buf_len);
			__call_console_drivers(0, end & LOG_BUF_MASK);
		} else {
			__call_console_drivers(start, end);
		}
	}
}

/*
 * Parse the syslog header <[0-9]*>. The decimal value represents 32bit, the
 * lower 3 bit are the log level, the rest are the log facility. In case
 * userspace passes usual userspace syslog messages to /dev/kmsg or
 * /dev/ttyprintk, the log prefix might contain the facility. Printk needs
 * to extract the correct log level for in-kernel processing, and not mangle
 * the original value.
 *
 * If a prefix is found, the length of the prefix is returned. If 'level' is
 * passed, it will be filled in with the log level without a possible facility
 * value. If 'special' is passed, the special printk prefix chars are accepted
 * and returned. If no valid header is found, 0 is returned and the passed
 * variables are not touched.
 */
static size_t log_prefix(const char *p, unsigned int *level, char *special)
{
	unsigned int lev = 0;
	char sp = '\0';
	size_t len;

	if (p[0] != '<' || !p[1])
		return 0;
	if (p[2] == '>') {
		/* usual single digit level number or special char */
		switch (p[1]) {
		case '0' ... '7':
			lev = p[1] - '0';
			break;
		case 'c': /* KERN_CONT */
		case 'd': /* KERN_DEFAULT */
			sp = p[1];
			break;
		default:
			return 0;
		}
		len = 3;
	} else {
		/* multi digit including the level and facility number */
		char *endp = NULL;

		lev = (simple_strtoul(&p[1], &endp, 10) & 7);
		if (endp == NULL || endp[0] != '>')
			return 0;
		len = (endp + 1) - p;
	}

	/* do not accept special char if not asked for */
	if (sp && !special)
		return 0;

	if (special) {
		*special = sp;
		/* return special char, do not touch level */
		if (sp)
			return len;
	}

	if (level)
		*level = lev;
	return len;
}

/*
static void call_console_drivers(unsigned start, unsigned end)
{
	unsigned cur_index, start_print;
	static int msg_level = -1;

	BUG_ON(((int)(start - end)) > 0);

	cur_index = start;
	start_print = start;
	while (cur_index != end) {
		if (msg_level < 0 && ((end - cur_index) > 2)) {
			/* strip log prefix */
			cur_index += log_prefix(&LOG_BUF(cur_index), &msg_level, NULL);
			start_print = cur_index;
		}
		while (cur_index != end) {
			char c = LOG_BUF(cur_index);

			cur_index++;
			if (c == '\n') {
				if (msg_level < 0) {
					/*
					 * printk() has already given us loglevel tags in
					 * the buffer.  This code is here in case the
					 * log buffer has wrapped right round and scribbled
					 * on those tags
					 */
					msg_level = default_message_loglevel;
				}
				_call_console_drivers(start_print, cur_index, msg_level);
				msg_level = -1;
				start_print = cur_index;
				break;
			}
		}
	}
	_call_console_drivers(start_print, end, msg_level);
}
{
static void emit_log_char(char c)
{
	LOG_BUF(log_end) = c;
	log_end++;
	if (log_end - log_start > log_buf_len)
		log_start = log_end - log_buf_len;
	if (log_end - con_start > log_buf_len)
		con_start = log_end - log_buf_len;
	if (logged_chars < log_buf_len)
		logged_chars++;
}
#if defined(CONFIG_PRINTK_TIME)
static bool printk_time = 1;
#else
static bool printk_time = 0;
#endif
module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);

static bool always_kmsg_dump;
module_param_named(always_kmsg_dump, always_kmsg_dump, bool, S_IRUGO | S_IWUSR);

/**
 * printk - print a kernel message
 * @fmt: format string
 *
 * This is printk().  It can be called from any context.  We want it to work.
 *
 * We try to grab the console_lock.  If we succeed, it's easy - we log the output and
 * call the console drivers.  If we fail to get the semaphore we place the output
 * into the log buffer and return.  The current holder of the console_sem will
 * notice the new output in console_unlock(); and will send it to the
 * consoles before releasing the lock.
 *
 * One effect of this deferred printing is that code which calls printk() and
 * then changes console_loglevel may break. This is because console_loglevel
 * is inspected when the actual printing occurs.
 *
 * See also:
 * printf(3)
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */

asmlinkage int printk(const char *fmt, ...)
{
	va_list args;
	int r;

#ifdef CONFIG_KGDB_KDB
	if (unlikely(kdb_trap_printk)) {
		va_start(args, fmt);
		r = vkdb_printf(fmt, args);
		va_end(args);
		return r;
	}
#endif
	va_start(args, fmt);
	r = vprintk(fmt, args);
	va_end(args);

	return r;
}

/* cpu currently holding logbuf_lock */
static volatile unsigned int printk_cpu = UINT_MAX;

	printk_cpu = UINT_MAX;
	}
	if (wake)
static const char recursion_bug_msg [] =
		KERN_CRIT "BUG: recent printk recursion!\n";
static int recursion_bug;
static int new_text_line = 1;
static char printk_buf[1024];
asmlinkage int vprintk(const char *fmt, va_list args)
{
	int printed_len = 0;
	int current_log_level = default_message_loglevel;
{
	unsigned long flags;
	char *p;
	size_t plen;
	char special;
	int this_cpu;
	if (unlikely(printk_cpu == this_cpu)) {
	printk_cpu = this_cpu;
	raw_spin_lock(&logbuf_lock);
	if (recursion_bug) {
		recursion_bug = 0;
		strcpy(printk_buf, recursion_bug_msg);
		printed_len = strlen(recursion_bug_msg);
		recursion_bug = 0;
	}
	/* Emit the output into the temporary buffer */
	printed_len += vscnprintf(printk_buf + printed_len,
				  sizeof(printk_buf) - printed_len, fmt, args);
	p = printk_buf;
	/* Read log level and handle special printk prefix */
	plen = log_prefix(p, &current_log_level, &special);
	if (plen) {
		p += plen;
		switch (special) {
		case 'c': /* Strip <c> KERN_CONT, continue line */
			plen = 0;
			break;
		case 'd': /* Strip <d> KERN_DEFAULT, start new line */
			plen = 0;
		default:
			if (!new_text_line) {
				emit_log_char('\n');
				new_text_line = 1;
			}
	/*
	 * Copy the output into log_buf. If the caller didn't provide
	 * the appropriate log prefix, we insert them here
	 */
	for (; *p; p++) {
		if (new_text_line) {
			new_text_line = 0;

			if (plen) {
				/* Copy original log prefix */
				int i;

				for (i = 0; i < plen; i++)
					emit_log_char(printk_buf[i]);
				printed_len += plen;
			} else {
				/* Add log prefix */
				emit_log_char('<');
				emit_log_char(current_log_level + '0');
				emit_log_char('>');
				printed_len += 3;
			}
			if (printk_time) {
				/* Add the current time stamp */
				char tbuf[50], *tp;
				unsigned tlen;
				unsigned long long t;
				unsigned long nanosec_rem;

				t = cpu_clock(printk_cpu);
				nanosec_rem = do_div(t, 1000000000);
				tlen = sprintf(tbuf, "[%5lu.%06lu] ",
						(unsigned long) t,
						nanosec_rem / 1000);

				for (tp = tbuf; tp < tbuf + tlen; tp++)
					emit_log_char(*tp);
				printed_len += tlen;
			}
			if (!*p)
				break;
		}
		emit_log_char(*p);
		if (*p == '\n')
			new_text_line = 1;
	}
	 * Try to acquire and then immediately release the
	 * console semaphore. The release will do all the
	 * actual magic (print out buffers, wake up klogd,
	 * etc).
	 * The console_trylock_for_printk() function
	 * will release 'logbuf_lock' regardless of whether it
	 * actually gets the semaphore or not.
EXPORT_SYMBOL(printk);
}
EXPORT_SYMBOL(vprintk);
#else
static void call_console_drivers(unsigned start, unsigned end)
{
 * Delayed printk facility, for scheduler-internal messages:
 * If there is output waiting for klogd, we wake it up.
{
	unsigned long flags;
	unsigned _con_start, _log_end;
	unsigned wake_klogd = 0, retry = 0;
	unsigned long flags;
	for ( ; ; ) {
again:
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		wake_klogd |= log_start - log_end;
		if (con_start == log_end)
			break;			/* Nothing to print */
		_con_start = con_start;
		_log_end = log_end;
		con_start = log_end;		/* Flush */
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock(&logbuf_lock);
		raw_spin_unlock(&logbuf_lock);
		stop_critical_timings();	/* don't trace print latency */
		call_console_drivers(_con_start, _log_end);
		stop_critical_timings();	/* don't trace print latency */
		start_critical_timings();
	if (con_start != log_end)
		retry = 1;
	raw_spin_lock(&logbuf_lock);
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
		con_start = log_start;
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	unsigned long end;
	unsigned chars;
{
	struct kmsg_dumper *dumper;
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	end = log_end & LOG_BUF_MASK;
	chars = logged_chars;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	if (chars > end) {
		s1 = log_buf + log_buf_len - chars + end;
		l1 = chars - end;
		s2 = log_buf;
		l2 = end;
	} else {
		s2 = log_buf + end - chars;
		l2 = chars;
	}
	}
		goto err_unregister_v4l2_dev;
				ret);
	}
err_unregister_v4l2_dev:
	if (!rc && cifs_sb->prepathlen && tcon) {
	struct scatterlist sg[1];
	sg_init_one(sg, buf, count);
	return __send_to_port(port, sg, 1, count, (void *)buf, false);
}
	kgid_t group = current_egid();
	kgid_t low, high;
	kgid_t low, high;
			if (gid_lte(low, gid) && gid_lte(gid, high))
		if (be32_to_cpu(btree->hashval) == lasthash)
		btree = dp->d_ops->node_tree_p(node);
			break;
	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT |
		EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |
		EXT4_EX_NOCACHE;
	if (mode & FALLOC_FL_KEEP_SIZE)
		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;
		if (partial_end)
		((bfn1 == bfn2) || ((bfn1+1) == bfn2));
#else
	unsigned int main_segs, blocks_per_seg;
	int i;
	fsmeta += le32_to_cpu(raw_super->segment_count_sit);
	fsmeta += le32_to_cpu(raw_super->segment_count_nat);
	fsmeta = le32_to_cpu(raw_super->segment_count_ckpt);
	fsmeta += le32_to_cpu(ckpt->rsvd_segment_count);
	if (unlikely(f2fs_cp_error(sbi))) {
	if (assoc_desc->bFirstInterface != ifnum) {
		dev_err(d, "Not found matching IAD interface\n");
	if (min > map->max_apic_id)
		if (regset->core_note_type &&
		do_thread_regset_writeback(t->task, regset);
		    (!regset->active || regset->active(t->task, regset))) {
	if (!access_ok(VERIFY_WRITE, data, size))
	if (!access_ok(VERIFY_READ, data, size))
		ctxt->memopp->addr.mem.ea = address_mask(ctxt,
	if (!report)
	}
		return -EINVAL;
			if (ep_loop_check(ep, tfile) != 0)
			error = -ELOOP;
				goto error_tgt_fput;
				goto error_tgt_fput;
done:
	return (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;
	int ret;
	unsigned long flags;
	 * Only remove the buffer from done_list if v4l2_buffer can handle all
	 * the planes.
	if (should_follow_link(dentry, nd->flags & LOOKUP_FOLLOW))
		csum = csum_sub(csum,
				csum_partial(skb_transport_header(skb) + tlen,
					     offset, 0));
	/* it might be unflagged overflow */
		key_payload_reserve(keyring,
				    keyring->datalen - KEYQUOTA_LINK_BYTES);
		assoc_array_cancel_edit(edit);
				if (!peers(n, last_dest)) {
				}
			}
		}
	if (!handle->h_transaction) {
		err = jbd2_journal_stop(handle);
		return handle->h_err ? handle->h_err : err;
	if (!handle->h_transaction) {
	}
	err = handle->h_err;
#include <linux/fs.h>
	if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {
			return -EINVAL;
	/* filesystem's fallocate may need to take i_mutex */
	up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
	up_read(&current->mm->mmap_sem);
				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
				offset, end - start);
	down_read(&current->mm->mmap_sem);
	key_ref_t key_ref, skey_ref;
	int rc;
	int groups_per_flex = 0;
	ext4_group_t flex_group;
	size_t size;
	groups_per_flex = 1 << sbi->s_log_groups_per_flex;
	sbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;
		sbi->s_log_groups_per_flex = 0;
	}
	int converted_op_size; /* the valid value width after perceived conversion */
	int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
};
		regs = cur_regs(env);
		if (class == BPF_ALU || class == BPF_ALU64) {
	struct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;
	       sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));
	env->insn_aux_data = new_data;
	if (ret == 0)
				if (unlikely(t == 0)) {
					while (unlikely(*ip == 0)) {
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
			vmx->nested.pi_desc_page = NULL;
		}
	struct socket_wq	peer_wq;
};
static int unix_writable(const struct sock *sk)
		}
		sock_put(skpair); /* It may now die */
	init_waitqueue_head(&u->peer_wait);
	unix_insert_socket(unix_sockets_unbound(sk), sk);
		unix_peer(sk) = other;
		unix_state_double_unlock(sk, other);
	int data_len = 0;
	unix_state_lock(other);
	unix_state_lock(other);
	err = -EPERM;
	if (sock_flag(other, SOCK_DEAD)) {
		err = 0;
		unix_state_lock(sk);
			unix_peer(sk) = NULL;
			unix_state_unlock(sk);
	if (unix_peer(other) != sk && unix_recvq_full(other)) {
		if (!timeo) {
			err = -EAGAIN;
			goto out_unlock;
		}
		timeo = unix_wait_for_peer(other, timeo);
		err = sock_intr_errno(timeo);
		if (signal_pending(current))
			goto out_free;
		goto restart;
	}
	if (sock_flag(other, SOCK_RCVTSTAMP))
out_unlock:
	unix_state_unlock(other);
			sock_poll_wait(file, &unix_sk(other)->peer_wait, wait);
		return -EINVAL;
		return -EINVAL;
		pr_debug("payload len = 0\n");
	}
		return -EINVAL;
	if (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)
	return NF_ACCEPT;
}
#ifdef CONFIG_SWAP
		*prev = vma;
		*prev = vma;
	*prev = vma;
		incoming_msg = (struct nlmsghdr *)kvp_recv_buffer;
					 NULL);
		}
		if (buffer[1] != USB_DT_CS_INTERFACE) {
	atomic_add(skb->truesize, &sk->sk_rmem_alloc);
	int result, err = 0, retries = 0;
      retry:
			      cgc->buffer, cgc->buflen,
			      (unsigned char *)cgc->sense, &sshdr,
	result = scsi_execute(SDev, cgc->cmd, cgc->data_direction,
			      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);
{
	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
	priv->read_urb = port->serial->port[1]->interrupt_in_urb;
	priv->read_urb->context = port;
	/*
	 */
	clear_bit(0x80, vmx_io_bitmap_a);
	u64 umin_val, umax_val;
		coerce_reg_to_size(dst_reg, 4);
		coerce_reg_to_size(&src_reg, 4);
	}
		if (umax_val > 63) {
			/* Shifts greater than 63 are undefined.  This includes
			 * shifts by a negative number.
	case BPF_LSH:
		if (umax_val > 63) {
			/* Shifts greater than 63 are undefined.  This includes
			 * shifts by a negative number.
	case BPF_RSH:
	__reg_deduce_bounds(dst_reg);
		max_delay = IGMPV3_MRC(ih3->code)*(HZ/IGMP_TIMER_SCALE);
	} else { /* v3 */
	hlist_del(&pin->m_list);
	hlist_del(&pin->s_list);
	spin_lock(&pin_lock);
	spin_unlock(&pin_lock);
	init_waitqueue_head(&p->wait);
	p->kill = kill;
	port->port.count = 0;
	usb_autopm_put_interface(serial->interface);
		nlh = nlmsg_hdr(skb);
		/* BPF_RSH is an unsigned shift, so make the appropriate casts */
				/* Sign bit will be cleared */
		} else {
		}
		}
		if (src_known)
	if (o_direct) {
		iocb->private = &overwrite;
		struct kvm_one_reg reg;
		if (copy_from_user(&reg, argp, sizeof(reg)))
		if (copy_from_user(&reg_list, user_list, sizeof(reg_list)))
