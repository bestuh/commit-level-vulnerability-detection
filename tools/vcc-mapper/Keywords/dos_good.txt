	fe->frontend_priv = NULL;
}
	count = be32_to_cpu(aclp->acl_cnt);
static int ocfs2_dio_get_block(struct inode *inode, sector_t iblock,
		/* This is the fast path for re-write. */
		ret = ocfs2_get_block(inode, iblock, bh_result, create);

		if (buffer_mapped(bh_result) &&
		get_block = ocfs2_get_block;
	if (iov_iter_rw(iter) == READ)
	else
		get_block = ocfs2_dio_get_block;
	else
	mm->context.asce_bits |= _ASCE_TYPE_REGION3;
	mm->context.asce_limit = STACK_TOP_MAX;
	crst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));
	if (trans == NULL) {
		conn = ERR_PTR(-ENODEV);
		goto out;
	}

	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
	freq_reg = devm_ioremap(dev, res->start, resource_size(res));
	std	r8, VCPU_WORT(r9)
8:
	return pskb_may_pull(skb, hlen) ? skb->data + offset : NULL;
		return 0;
	if (!dir_emit_dots(file, ctx))
	if (dctx->bytes) {
	ghash_flush(ctx, dctx);
		int reject_error;
		int err = PTR_ERR(key->payload.data[dns_key_error]);
	if (key_is_instantiated(key) &&
	    (size_t)key->payload.data[big_key_len] > BIG_KEY_FILE_THRESHOLD)
		seq_printf(m, ": %zu [%s]",
		key->reject_error = -error;
			ctx->result = ERR_PTR(key->reject_error);
		return key->reject_error;
		}
		assoc = (src + req->cryptlen + auth_tag_len);
			return -ENOMEM;
		scatterwalk_map_and_copy(src, req->src, 0, req->cryptlen, 0);
		scatterwalk_map_and_copy(dst, req->dst, 0, req->cryptlen, 1);
	} else {
		kfree(src);
	err = aead_register_instance(tmpl, inst);
	return ret;
	if (num_clips && clips_ptr) {
		clips = kzalloc(num_clips * sizeof(*clips), GFP_KERNEL);
	unsigned long flags;
	if (n_hdlc->tbuf) {
		n_hdlc_buf_put(&n_hdlc->tx_free_buf_list, n_hdlc->tbuf);
		n_hdlc->tbuf = NULL;
	}
{
	ret = -EEXIST;
		if (!tmp) {
		iov_iter_advance(ii, tmp);
	pch->chan_net = net;
	spin_unlock_bh(&pn->all_channels_lock);
	atomic_t count;
	kuid_t uid;
		atomic_set(&new->count, 0);
		new->uid = uid;
	if (!atomic_add_unless(&ucounts->count, 1, INT_MAX))
	}
		ucounts = NULL;
		ucounts = NULL;
	if (atomic_dec_and_test(&ucounts->count)) {
		hlist_del_init(&ucounts->node);
		hlist_del_init(&ucounts->node);
		kfree(ucounts);
	}
}
		tcon->bad_network_name = true;
	}
		rc = -EFAULT;
			sizeof(sipx->sipx_node));
		if (copy_to_user(arg, &ifr, sizeof(ifr)))
			break;
		if (copy_to_user(arg, &ifr, sizeof(ifr)))
		ipxitf_put(ipxif);
		rc = 0;
		pointer_desc = "stack ";
		break;
	(NETIF_F_SG | NETIF_F_HIGHDMA | NETIF_F_FRAGLIST)
#define MACSEC_FEATURES \
	return error;
		kfree_skb(skb);
}
	return error;
		kfree_skb(skb);
}
	if (A > skb->len - sizeof(struct nlattr))
	if (A > skb->len - sizeof(struct nlattr))
	if (nla->nla_len > A - skb->len)
	nla = (struct nlattr *) &skb->data[A];
		return 0;
		if (skb_headroom(skb) < (tnl_hlen + frag_hdr_sz)) {
		tnl_hlen = skb_tnl_header_len(skb);
			if (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))
	} else if (ieee80211_is_action(mgmt->frame_control)) {
		af_params = kzalloc(sizeof(*af_params), GFP_KERNEL);
		iif = l3mdev_master_ifindex(skb_dst(skb)->dev);
	ENCSTRCT                sEncryption;
					memcpy((void *)lp->StationName, (void *)&pLtv->u.u8[2], (size_t)pLtv->u.u16[0]);
					memset(lp->StationName, 0, sizeof(lp->StationName));
					pLtv->u.u16[0] = CNV_INT_TO_LITTLE(pLtv->u.u16[0]);
	unsigned long flags;
	int         ret = 0;

	memcpy(lp->StationName, extra, wrqu->data.length);
	memset(lp->StationName, 0, sizeof(lp->StationName));
	if (!is_guest_mode(vcpu)) {
					if (filter[i].jf)
						t_offset += is_near(f_offset) ? 2 : 6;
				if (filter[i].jt != 0) {
					EMIT_COND_JMP(t_op, t_offset);
	if (buf)
	nfs4_write_cached_acl(inode, pages, res.acl_data_offset, res.acl_len);
		_copy_from_pages(buf, pages, res.acl_data_offset, res.acl_len);
		_copy_from_pages(buf, pages, res.acl_data_offset, res.acl_len);
out_ok:
	if (!asoc->temp) {
{
{
	interval = muldiv64(val, NSEC_PER_SEC, KVM_PIT_FREQ);
int ib_update_cm_av(struct ib_cm_id *id, const u8 *smac, const u8 *alt_smac)
{

	cm_id_priv = container_of(id, struct cm_id_private, id);

	if (smac != NULL)
		memcpy(cm_id_priv->av.smac, smac, sizeof(cm_id_priv->av.smac));

	if (alt_smac != NULL)
		memcpy(cm_id_priv->alt_av.smac, alt_smac,
		       sizeof(cm_id_priv->alt_av.smac));

	return 0;
}
EXPORT_SYMBOL(ib_update_cm_av);

	u8 smac[ETH_ALEN];
	u8 alt_smac[ETH_ALEN];
	u8 *psmac = smac;
	u8 *palt_smac = alt_smac;
	int is_iboe = ((rdma_node_get_transport(cm_id->device->node_type) ==
			RDMA_TRANSPORT_IB) &&
		       (rdma_port_get_link_layer(cm_id->device,
			ib_event->param.req_rcvd.port) ==
			IB_LINK_LAYER_ETHERNET));
	if (is_iboe) {
		if (ib_event->param.req_rcvd.primary_path != NULL)
			rdma_addr_find_smac_by_sgid(
				&ib_event->param.req_rcvd.primary_path->sgid,
				psmac, NULL);
		else
			psmac = NULL;
		if (ib_event->param.req_rcvd.alternate_path != NULL)
			rdma_addr_find_smac_by_sgid(
				&ib_event->param.req_rcvd.alternate_path->sgid,
				palt_smac, NULL);
		else
			palt_smac = NULL;
	}
	if (is_iboe)
		ib_update_cm_av(cm_id, psmac, palt_smac);
int ib_update_cm_av(struct ib_cm_id *id, const u8 *smac, const u8 *alt_smac);
		"1:"XSAVEOPT,
		"1:"XSAVE,
		X86_FEATURE_XSAVEOPT,
		"1:"XSAVES,
		X86_FEATURE_XSAVEOPT,
		X86_FEATURE_XSAVES,
		"1: " XRSTORS,
		"1: " XRSTOR,
		X86_FEATURE_XSAVES,
	sas_discover_event(dev->port, DISCE_PROBE);
static void sas_destruct_devices(struct work_struct *work)

	clear_bit(DISCE_DESTRUCT, &port->disc.pending);
		sas_discover_event(dev->port, DISCE_DESTRUCT);
	mutex_unlock(&ha->disco_mutex);
		[DISCE_DESTRUCT] = sas_destruct_devices,
	INIT_LIST_HEAD(&port->destroy_list);
	struct list_head destroy_list;
{

/*
 * Initialize big_key crypto and RNG algorithms
 */
static int __init big_key_crypto_init(void)
{
	int ret = -EINVAL;
	/* init RNG */
	big_key_rng = crypto_alloc_rng(big_key_rng_name, 0, 0);
	if (IS_ERR(big_key_rng)) {
		big_key_rng = NULL;
		return -EFAULT;
	}
	ret = crypto_rng_reset(big_key_rng, NULL, crypto_rng_seedsize(big_key_rng));
	if (ret)
		goto error;
		ret = -EFAULT;
		goto error;
	}
error:
	crypto_free_rng(big_key_rng);
	big_key_rng = NULL;
late_initcall(big_key_crypto_init);
	usage = mem_cgroup_usage(memcg, type == _MEMSWAP);

	synchronize_rcu();
	mutex_unlock(&memcg->thresholds_lock);
	struct list_head auto_asconf_list;
		list_add_tail(&sp->auto_asconf_list,
	} else
		sp->do_auto_asconf = 1;
		sp->do_auto_asconf = 0;
		sp->do_auto_asconf = 0;
	struct list_head tmplist;
	if (oldsp->do_auto_asconf) {
		memcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));
		memcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));
	} else
			regno, tn_buf);
	}
	else
	if (cdc_ether) {
		dev->hard_mtu = le16_to_cpu(cdc_ether->wMaxSegmentSize);
		struct vfio_irq_set hdr;
		u8 *data = NULL;
		int ret = 0;
		u8 *data = NULL;
		if (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||
		    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |
		if (!(hdr.flags & VFIO_IRQ_SET_DATA_NONE)) {
			size_t size;
			if (hdr.flags & VFIO_IRQ_SET_DATA_BOOL)
				size = sizeof(uint8_t);
			else if (hdr.flags & VFIO_IRQ_SET_DATA_EVENTFD)
				size = sizeof(int32_t);
			else
				return -EINVAL;
			if (hdr.argsz - minsz < hdr.count * size ||
				return -EINVAL;
	vdev->ctx = kzalloc(nvec * sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
	if (!vdev->ctx)
#define EXT_MAX_BLOCK	0xffffffff
		return EXT_MAX_BLOCK;
	if (depth == 0 && path->p_ext == NULL)
	return EXT_MAX_BLOCK;
}
 * returns first allocated block from next leaf or EXT_MAX_BLOCK
		return EXT_MAX_BLOCK;
	if (depth == 0)
	return EXT_MAX_BLOCK;
}
	    && next != EXT_MAX_BLOCK) {
	if (le32_to_cpu(newext->ee_block) > le32_to_cpu(fex->ee_block)
		ext_debug("next leaf block - %d\n", next);
		len = EXT_MAX_BLOCK;
		lblock = 0;
		ext_debug("cache gap(whole file):");
			>> EXT4_BLOCK_SIZE_BITS(sb);

	if (err == -EOPNOTSUPP) {
	int ret;
	char *name_mute, *name_micmute;
			free_ep_req(midi->out_ep, req);
extern struct key *find_keyring_by_name(const char *name, bool skip_perm_check);
struct key *find_keyring_by_name(const char *name, bool skip_perm_check)
			if (!skip_perm_check &&
			    key_permission(make_key_ref(keyring, 0),
	b	skip_tm
END_FTR_SECTION_IFCLR(CPU_FTR_TM)

	/* Turn on TM/FP/VSX/VMX so we can restore them. */
	mfmsr	r5
	li	r6, MSR_TM >> 32
	sldi	r6, r6, 32
	or	r5, r5, r6
	ori	r5, r5, MSR_FP
	oris	r5, r5, (MSR_VEC | MSR_VSX)@h
	mtmsrd	r5

	/*
	 * The user may change these outside of a transaction, so they must
	 * always be context switched.
	 */
	ld	r5, VCPU_TFHAR(r4)
	ld	r6, VCPU_TFIAR(r4)
	ld	r7, VCPU_TEXASR(r4)
	mtspr	SPRN_TFHAR, r5
	mtspr	SPRN_TFIAR, r6
	mtspr	SPRN_TEXASR, r7

	ld	r5, VCPU_MSR(r4)
	rldicl. r5, r5, 64 - MSR_TS_S_LG, 62
	beq	skip_tm	/* TM not active in guest */

	/* Make sure the failure summary is set, otherwise we'll program check
	 * when we trechkpt.  It's possible that this might have been not set
	 * on a kvmppc_set_one_reg() call but we shouldn't let this crash the
	 * host.
	 */
	oris	r7, r7, (TEXASR_FS)@h
	mtspr	SPRN_TEXASR, r7

	/*
	 * We need to load up the checkpointed state for the guest.
	 * We need to do this early as it will blow away any GPRs, VSRs and
	 * some SPRs.
	 */

	mr	r31, r4
	addi	r3, r31, VCPU_FPRS_TM
	addi	r3, r31, VCPU_VRS_TM
	mr	r4, r31
	lwz	r7, VCPU_VRSAVE_TM(r4)
	mtspr	SPRN_VRSAVE, r7

	ld	r5, VCPU_LR_TM(r4)
	lwz	r6, VCPU_CR_TM(r4)
	ld	r7, VCPU_CTR_TM(r4)
	ld	r8, VCPU_AMR_TM(r4)
	ld	r9, VCPU_TAR_TM(r4)
	mtlr	r5
	mtcr	r6
	mtctr	r7
	mtspr	SPRN_AMR, r8
	mtspr	SPRN_TAR, r9

	/*
	 * Load up PPR and DSCR values but don't put them in the actual SPRs
	 * till the last moment to avoid running with userspace PPR and DSCR for
	 * too long.
	 */
	ld	r29, VCPU_DSCR_TM(r4)
	ld	r30, VCPU_PPR_TM(r4)

	std	r2, PACATMSCRATCH(r13) /* Save TOC */

	/* Clear the MSR RI since r1, r13 are all going to be foobar. */
	li	r5, 0
	mtmsrd	r5, 1

	/* Load GPRs r0-r28 */
	reg = 0
	.rept	29
	ld	reg, VCPU_GPRS_TM(reg)(r31)
	reg = reg + 1
	.endr

	mtspr	SPRN_DSCR, r29
	mtspr	SPRN_PPR, r30

	/* Load final GPRs */
	ld	29, VCPU_GPRS_TM(29)(r31)
	ld	30, VCPU_GPRS_TM(30)(r31)
	ld	31, VCPU_GPRS_TM(31)(r31)

	/* TM checkpointed state is now setup.  All GPRs are now volatile. */
	TRECHKPT

	/* Now let's get back the state we need. */
	HMT_MEDIUM
	GET_PACA(r13)
	ld	r29, HSTATE_DSCR(r13)
	mtspr	SPRN_DSCR, r29
	ld	r1, HSTATE_HOST_R1(r13)
	ld	r2, PACATMSCRATCH(r13)

	/* Set the MSR RI since we have our registers back. */
	li	r5, MSR_RI
	mtmsrd	r5, 1
skip_tm:
BEGIN_FTR_SECTION
#endif
END_FTR_SECTION_IFCLR(CPU_FTR_TM)
	rldicl. r5, r5, 64 - MSR_TS_S_LG, 62
	/* Clear the MSR RI since r1, r13 are all going to be foobar. */
	page = pte_page(huge_ptep_get((pte_t *)pmd));
	nid = page_to_nid(page);
	if (!to_vmx(vcpu)->nested.vmxon) {
		/* _system ok, as hardware has verified cpl=0 */
	/* ok to use *_system, as hardware has verified cpl=0 */
		if (ret < 0) {
			if (ret != -EEXIST)
				goto error;
			ret = 0;
		}
		ret = install_process_keyring_to_cred(new);
		goto set;
	keyring = keyring_alloc("_tid", new->uid, new->gid, new,
	BUG_ON(new->thread_keyring);
		return -EEXIST;
		abort_creds(new);
	}
	fp->f_cred->user->unix_inflight++;
	fp->f_cred->user->unix_inflight--;
	if (!sock_flag(sk, SOCK_ZAPPED))
		return -EINVAL;
	if (!sock_flag(sk, SOCK_ZAPPED))
		return -EINVAL;
				if (*off >= skb->len) {
					*off -= skb->len;
	if ((skb_headroom(skb) < frag_hdr_sz) &&
	    pskb_expand_head(skb, frag_hdr_sz, 0, GFP_ATOMIC))
	switch (opcode) {
static void kiocb_batch_free(struct kiocb_batch *batch)
{
	list_for_each_entry_safe(req, n, &batch->head, ki_batch) {
		list_del(&req->ki_batch);
	kiocb_batch_free(&batch);
					 rx_work_todo(vif) ||
					 kthread_should_stop());
					 kthread_should_stop());
		if (kthread_should_stop())
		priv->suspend(dev);
	if (priv->resume)
		priv->resume(dev);
		unsigned int rand;
		state->pos = 0;
		state->rand = rand;
		state->count = count;
		ret = true;
	return (state->list[state->pos++] + state->rand) % state->count;
{
}
	kvm_iommu_put_pages(kvm, slot->base_gfn, gfn);
static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
{
	unsigned long i;

	for (i = 0; i < npages; ++i)
		kvm_release_pfn_clean(pfn + i);
}

			/* if we knew anything about the old value, we're not
			 * equal, because we can't know anything about the
			 * scalar value of the pointer in the new value.
		} else {
			       tnum_is_unknown(rold->var_off);
		}
	spinlock_t lock;
	u8 *in_out_buffer;
	unsigned long flags;
	spin_lock_irqsave(&dev->lock, flags);
	spin_unlock_irqrestore(&dev->lock, flags);
exit:
	return ret <= 0 ? ret : -EIO;
	unsigned long flags;
	spin_lock_irqsave(&dev->lock, flags);
	spin_unlock_irqrestore(&dev->lock, flags);
	unsigned long flags;
	spin_lock_irqsave(&dev->lock, flags);
	spin_unlock_irqrestore(&dev->lock, flags);
exit:
	unsigned long flags;
	spin_lock_irqsave(&dev->lock, flags);
	spin_unlock_irqrestore(&dev->lock, flags);
	spin_unlock_irqrestore(&dev->lock, flags);
fail:
	return ret < 0 ? ret : -EIO;
	spin_lock_init(&dev->lock);
		mutex_unlock(&kvm->lock);
	buzz = kzalloc(sizeof(*buzz), GFP_KERNEL);
	mutex_lock(&mut);
	mutex_unlock(&mut);
	mutex_unlock(&file->mut);
		bh->b_size = map.m_len << inode->i_blkbits;
		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
	}
	rq->skip_clock_update = 0;
	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
		hlist_nulls_del(&sk->sk_nulls_node);
		sock_put(sk);
			}
					 union_desc->bMasterInterface0);
					 union_desc->bSlaveInterface0);
	ret = hid_hw_start(hdev, HID_CONNECT_DEFAULT);
{
	BUG_ON(!PageLocked(page));
	BUG_ON(!PageLocked(page));
			mlock_vma_page(page);   /* no-op if already mlocked */
			if (page == check_page)
		if (locked_vma) {
				ret = SWAP_MLOCK;
				ret = SWAP_MLOCK;
			continue;	/* don't unmap */
#define transparent_hugepage_defrag(__vma)				\
	unsigned long tpgt;
	int ret;
	if (kstrtoul(name + 5, 10, &tpgt) || tpgt > UINT_MAX)
		return ERR_PTR(-EINVAL);
		return ERR_PTR(-EINVAL);
	}
		sk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);
		sk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);
			kvm_unpin_pages(kvm, pfn, page_size);
		hlist_add_head(&mp->mglist, &br->mglist);
	if (!port) {
		mod_timer(&mp->timer, now + br->multicast_membership_interval);
		*log_num = nlogs;
	return headcount;
			break;
usbtv_audio_fail:
	usbtv_video_free(usbtv);
static void recalculate_apic_map(struct kvm *kvm)
			new->cid_mask = new->lid_mask = 0xffff;
			new->cid_shift = 16;
		} else if (kvm_apic_sw_enabled(apic) &&
	if (count < DJREPORT_SHORT_LENGTH - 2)
		count = DJREPORT_SHORT_LENGTH - 2;
		err = nla_parse_nested(sock, TIPC_NLA_SOCK_MAX,
		sk_mem_reclaim(sk);
		if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&
		    !tcp_under_memory_pressure(sk))
			break;
static struct net *get_target_net(struct sk_buff *skb, int netnsid)
{
	net = get_net_ns_by_id(sock_net(skb->sk), netnsid);
	if (!net)
		put_net(net);
			tgt_net = get_target_net(skb, netnsid);
			netnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);
			if (IS_ERR(tgt_net)) {
		tgt_net = get_target_net(skb, netnsid);
		netnsid = nla_get_s32(tb[IFLA_IF_NETNSID]);
		if (IS_ERR(tgt_net))
	if ((err = xfrm_migrate_check(m, num_migrate)) < 0)
	if (A > skb->len - sizeof(struct nlattr))
	if (A > skb->len - sizeof(struct nlattr))
	if (nla->nla_len > A - skb->len)
	nla = (struct nlattr *) &skb->data[A];
		return 0;
	unsigned int start_offset = offset & ~PAGE_CACHE_MASK;
	unsigned int end_offset = (offset + len) & ~PAGE_CACHE_MASK;
		page = grab_cache_page_write_begin(inode->i_mapping, curr,
		calc_max_reserv(ip, len, &max_bytes, &data_blocks, &ind_blocks);
	while (buflen > 0) {
		union_desc = (struct usb_cdc_union_desc *)buf;
		if (union_desc->bDescriptorType == USB_DT_CS_INTERFACE &&
			return union_desc;
			dev_dbg(&intf->dev, "Found union header\n");
		}
	strb	wzr, [x0]
	b.mi	5f
5:	mov	x0, #0
			if ((int)val < 0)
			val = read_pmc(i + 1);
				write_pmc(i + 1, 0);
{
{
static struct pernet_operations sctp_net_ops = {
	.init = sctp_net_init,
	.exit = sctp_net_exit,
};
	status = register_pernet_subsys(&sctp_net_ops);
	if (status)
		goto err_register_pernet_subsys;
	if (status)
	unregister_pernet_subsys(&sctp_net_ops);
err_register_pernet_subsys:
	unregister_pernet_subsys(&sctp_net_ops);
	if (type == HUB_INIT2)
		goto init2;
	if (type == HUB_INIT3)
		goto init3;
		goto init3;
					msecs_to_jiffies(delay));
			return;		/* Continues at init3: below */
	INTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0x3fffffffffull, RSP_0),
	INTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0x3fffffffffull, RSP_1),
static struct extra_reg intel_snb_extra_regs[] __read_mostly = {
	EVENT_EXTRA_END
		x86_pmu.extra_regs = intel_snb_extra_regs;
		x86_pmu.extra_regs = intel_snb_extra_regs;
	strncpy(extra_response->key, key, strlen(key) + 1);
	strncpy(extra_response->value, NOTUNDERSTOOD,
			strlen(NOTUNDERSTOOD) + 1);
				char *tmpptr = key + strlen(key);
				*tmpptr = '=';
struct iscsi_extra_response {
	char key[64];
struct iscsi_extra_response {
	char value[32];
	return vcpu->arch.apic->pending_events;
{
}
		npoints = (size - 4) / 9;
		msc->ntouches = 0;
	if (!(dev->flags & IFF_UP))
		return NET_RX_DROP;

	if (skb->len > (dev->mtu + dev->hard_header_len))
		return NET_RX_DROP;

		return NET_RX_DROP;
#define TEMP_TICKET_BUF_LEN	256

			  void **p, void *end, void *obuf, size_t olen)
static int ceph_x_decrypt(struct ceph_crypto_key *secret,
{
	ret = ceph_decrypt2(secret, &head, &head_len, obuf, &olen,
			    *p, len);
	dout("ceph_x_decrypt len %d\n", len);
	if (ret)
			      struct ceph_crypto_key *secret,
{
	struct ceph_x_ticket_handler *th;
	void *dp, *dend;
	struct ceph_crypto_key old_key;
	void *tp, *tpend;
			      TEMP_TICKET_BUF_LEN);
	if (dlen <= 0) {
	tp = ticket_buf;
				      TEMP_TICKET_BUF_LEN);
		dout(" encrypted ticket\n");
		if (dlen < 0) {
		}
		dlen = ceph_decode_32(&tp);
	}
	ret = 0;
out:
out_dbuf:
	return ret;
	ret = -EINVAL;
	goto out;
bad:
}
	struct ceph_x_authorize_reply reply;
	void *p = au->reply_buf;
	ret = ceph_x_decrypt(&th->session_key, &p, end, &reply, sizeof(reply));
	if (ret < 0)
		ioapic->rtc_status.pending_eoi = ret;
				ioapic->rtc_status.dest_map);
	} else
				jumpstack[stackidx++] = e;
				jumpstack[stackidx++] = e;
				jumpstack[stackidx++] = e;
		d = vhost_get_vq_desc(vq->dev, vq, vq->iov + seg,
		}
				      ARRAY_SIZE(vq->iov) - seg, &out,
				      &in, log, log_num);
		if (d == vq->num) {
	__drop_discard_cmd(sbi);
	mark_discard_range_all(sbi);
			BUG();
		} else {
		}
		}
	} else if (r->CRn == 14 && (r->CRm & 12) == 8) {
		BUG();
	} else {
	}
	nlk->cb_running = false;
	consume_skb(cb->skb);
	free = __LOG_BUF_LEN - log_end;
		unsigned log_idx_mask = start & (__LOG_BUF_LEN - 1);

		log_buf[dest_idx] = __log_buf[log_idx_mask];
	new_log_buf_len = 0;
		if (len > MAX_RDS_PS_NAME) {
		len = control->size - 1;
			rval = -ERANGE;
		if (len > MAX_RDS_RADIO_TEXT) {
		len = control->size - 1;
			rval = -ERANGE;
	struct group_info *group_info = get_current_groups();
	int i, j, count = group_info->ngroups;
	for (i = 0; i < group_info->nblocks; i++) {
				return 0;
		}
	return -EACCES;
}

		/*
		 * If we have a partial block after EOF we have to allocate
		 * the entire block.
		 */
			max_blocks += 1;
	if (max_blocks > 0) {
	if (max_blocks > 0) {
	return __BIOVEC_PHYS_MERGEABLE(vec1, vec2) &&
	assoc_desc = udev->actconfig->intf_assoc[0];
	if (ctxt->rip_relative)
		} else
	ret = call_bufop(q, verify_planes_array, *vb, pb);
	if (!ret)
	path->mnt = mntget(nd->path.mnt);
	path->dentry = dentry;
		return 1;
	follow_mount(path);
	if (offset != 0)
#define ARCH_P4_CNTRVAL_MASK	((1ULL << ARCH_P4_CNTRVAL_BITS) - 1)
	rdmsrl(hwc->event_base + hwc->idx, v);
	if (!(v & ARCH_P4_CNTRVAL_MASK))
		return 1;
	if (edit && !edit->dead_leaf) {
static struct mount *last_dest, *last_source, *dest_master;
static struct user_namespace *user_ns;
static struct mountpoint *mp;
		struct mount *n, *p;
		for (n = m; ; n = p) {
			if (p == dest_master || IS_MNT_MARKED(p)) {
				while (last_dest->mnt_master != p) {
					last_source = last_source->mnt_master;
					last_dest = last_source->mnt_parent;
				}
					last_source = last_source->mnt_master;
					last_dest = last_source->mnt_parent;
			p = n->mnt_master;
				break;
		type = CL_SLAVE;
	last_dest = dest_mnt;
	last_source = source_mnt;
	memcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);
	if (config->desc.bDescriptorType != USB_DT_CONFIG ||
	nintf = nintf_orig = config->desc.bNumInterfaces;
	if (pskb_expand_head(skb_out, 0, size - skb->len, GFP_ATOMIC) < 0) {
		kfree_skb(skb_out);
	int error;
	struct inet_sock *inet = inet_sk(sk);
	int err = 0;
	dccp_clear_xmit_timers(sk);
	char xbuf[12];

	if (groups_per_flex < 2) {
				insn_idx++;
			} else {
int lzo1x_decompress_safe(const unsigned char *in, size_t in_len,
						t += 255;
					t += 15 + *ip++;
					}
				}
					t += 255;
				t += 31 + *ip++;
				}
					t += 255;
				t += 7 + *ip++;
				}
	other = unix_peer_get(sk);
	if (other) {
		if (unix_peer(other) != sk) {
			if (unix_recvq_full(other))
				writable = 0;
		}
		sock_put(other);
	writable = unix_writable(sk);
	}
		ha->optrom_region_size = start + size > ha->optrom_size ?
		    ha->optrom_size - start : size;
		ha->optrom_region_start = start;
		ha->optrom_region_size = start + size > ha->optrom_size ?
		    ha->optrom_size - start : size;
		ha->optrom_region_start = start;
static void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)
	skcipher->setkey = alg->setkey;
	skcipher->encrypt = alg->encrypt;
	} else if (keyring == new->session_keyring) {
		ret = 0;
		if (len < 0 || addr.nl_pid) {
			syslog(LOG_ERR, "recvfrom failed; pid:%u error:%d %s",
	tlb_gather_mmu(&tlb, mm, 0, -1);
		if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))
			unmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,
	}
	tlb_finish_mmu(&tlb, 0, -1);
	if (used_address && used_address->name_len == msg_sys->msg_namelen &&
	    !memcmp(&used_address->name, msg->msg_name,
		    used_address->name_len)) {
		memcpy(&used_address->name, msg->msg_name,
		       used_address->name_len);
		used_address->name_len = msg_sys->msg_namelen;
	}
		if (skb->len && (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))
			put_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,
	struct mct_u232_private *priv;
	void *pwMIDQData = mpu->dev->mappedbase + MIDQ_DATA_BUFF;
	while (readw(mpu->dev->MIDQ + JQS_wTail) !=
	       readw(mpu->dev->MIDQ + JQS_wHead)) {
		u16 wTmp, val;
		val = readw(pwMIDQData + 2 * readw(mpu->dev->MIDQ + JQS_wHead));

			if (test_bit(MSNDMIDI_MODE_BIT_INPUT_TRIGGER,
				     &mpu->mode))
				snd_rawmidi_receive(mpu->substream_input,
						    (unsigned char *)&val, 1);

		wTmp = readw(mpu->dev->MIDQ + JQS_wHead) + 1;
		if (wTmp > readw(mpu->dev->MIDQ + JQS_wSize))
			writew(0,  mpu->dev->MIDQ + JQS_wHead);
		else
			writew(wTmp,  mpu->dev->MIDQ + JQS_wHead);
	spin_lock_irqsave(&mpu->input_lock, flags);
	}
	}
	spin_unlock_irqrestore(&mpu->input_lock, flags);
	void *pwDSPQData = chip->mappedbase + DSPQ_DATA_BUFF;
	while (readw(chip->DSPQ + JQS_wTail) != readw(chip->DSPQ + JQS_wHead)) {
		u16 wTmp;

		snd_msnd_eval_dsp_msg(chip,
			readw(pwDSPQData + 2 * readw(chip->DSPQ + JQS_wHead)));

		wTmp = readw(chip->DSPQ + JQS_wHead) + 1;
		if (wTmp > readw(chip->DSPQ + JQS_wSize))
			writew(0, chip->DSPQ + JQS_wHead);
		else
			writew(wTmp, chip->DSPQ + JQS_wHead);
	}
	}
	 * Allow direct access to the PC debug port (it is often used for I/O
	 * delays, but the vmexits simply slow things down).
static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,
	if (BPF_CLASS(insn->code) != BPF_ALU64) {
		/* 32-bit ALU ops are (32,32)->64 */
	name.name = buf;
		if (nlh->nlmsg_len < sizeof(*nlh) ||
		    skb->len < nlh->nlmsg_len) {
		if (dst_reg->smin_value < 0) {
			if (umin_val) {
				dst_reg->smin_value = 0;
			} else {
				/* Lost sign bit information */
				dst_reg->smin_value = S64_MIN;
				dst_reg->smax_value = S64_MAX;
			}
			dst_reg->smin_value =
				(u64)(dst_reg->smin_value) >> umax_val;
		rp[0] = 1;
		res->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;
		res->sign = 0;
	/* Make sure they initialize the vcpu with KVM_ARM_VCPU_INIT */
	if (unlikely(vcpu->arch.target < 0))
		return -ENOEXEC;
