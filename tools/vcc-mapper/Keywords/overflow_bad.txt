struct xfrm_replay_state_esn {
	if ((p->flags & XFRM_STATE_ESN) && !rt)
		return -EINVAL;
	struct xfrm_replay_state_esn *up;
	up = nla_data(rp);
	if (xfrm_replay_state_esn_len(replay_esn) !=
			xfrm_replay_state_esn_len(up))
		return -EINVAL;
	if (iov_count) {
{

{
	if (!(priv->stations[sta_id].used & IWL_STA_DRIVER_ACTIVE))
	}
}
		iwl_sta_ucode_activate(priv, sta_id);
		ret = 0;
		IWL_DEBUG_INFO(priv, "REPLY_ADD_STA PASSED\n");
		break;
int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
	if (ret == -EEXIST)
				    btrfs_inode_type(inode), index);
		goto fail_dir_item;
		return -ENOTEMPTY;
	down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);
	BUG_ON(ret == -EEXIST);
	if (ret) {
	if (stringset == ETH_SS_STATS)
{
		return ARRAY_SIZE(g_gmac_stats_string);
	if (stringset == ETH_SS_STATS)
{
		return ETH_PPE_STATIC_NUM;
	if (stringset == ETH_SS_STATS)
{
		return HNS_RING_STATIC_REG_NUM;
	if (stringset == ETH_SS_STATS)
{
		return ARRAY_SIZE(g_xgmac_stats_string);
static struct lock_class_key macsec_netdev_addr_lock_key;
		unsigned long ptr_size;
		struct rlimit *rlim;
		rlim = current->signal->rlim;
		if (size > READ_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4)
			goto fail;
	int j = DIV_ROUND_UP(len, 2), rc;
	rc = hex2bin(dst, src, j);
	if (rc < 0)
		pr_debug("CHAP string contains non hex digit symbols\n");
	}
			int max = vfio_pci_get_irq_count(vdev, hdr.index);
			    hdr.start >= max || hdr.start + hdr.count > max)
	char buffer[sizeof("4294967296 65635")];
	u_int16_t port;
	exp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;
		nf_ct_helper_log(skb, exp->master, "all ports in use");
	if (port == 0) {
		return NF_DROP;
	ret = nf_nat_mangle_tcp_packet(skb, exp->master, ctinfo,
				       protoff, matchoff, matchlen, buffer,
				       strlen(buffer));
	if (ret != NF_ACCEPT) {
		nf_ct_helper_log(skb, exp->master, "cannot mangle packet");
	if (ret != NF_ACCEPT) {
		nf_ct_unexpect_related(exp);
	}
	return ret;
#define BPF_MAX_VAR_OFF	(1ULL << 31)
#define BPF_MAX_VAR_SIZ	INT_MAX
	__update_reg_bounds(dst_reg);
	switch (opcode) {
	if (len <= 0x7f) {
	if (iter < CIPSO_V4_TAG_MAXCNT)
		doi_def->tags[iter] = CIPSO_V4_TAG_INVALID;
		u32 vmacache_seqnum;                   /* per-thread vmacache */
		struct rb_root mm_rb;
#ifdef CONFIG_MMU
	u32 seqnum;
struct vmacache {
	struct vm_area_struct *vmas[VMACACHE_SIZE];
		VMACACHE_FULL_FLUSHES,
extern void vmacache_flush_all(struct mm_struct *mm);

	/* deal with overflows */
	if (unlikely(mm->vmacache_seqnum == 0))
		vmacache_flush_all(mm);
	pr_emerg("mm %px mmap %px seqnum %d task_size %lu\n"
{
#ifdef CONFIG_MMU
		mm, mm->mmap, mm->vmacache_seqnum, mm->task_size,
#ifdef CONFIG_MMU
 * Flush vma caches for threads that share a given mm.
 *
 * The operation is safe because the caller holds the mmap_sem
 * exclusively and other threads accessing the vma cache will
 * have mmap_sem held at least for read, so no extra locking
 * is required to maintain the vma cache.
 */
void vmacache_flush_all(struct mm_struct *mm)
{
	struct task_struct *g, *p;

	count_vm_vmacache_event(VMACACHE_FULL_FLUSHES);

	rcu_read_lock();
	for_each_process_thread(g, p) {
		/*
		 * Only flush the vmacache pointers as the
		 * mm seqnum is already set and curr's will
		 * be set upon invalidation when the next
		 * lookup is done.
		 */
		if (mm == p->mm)
			vmacache_flush(p);
	}
	rcu_read_unlock();
}

/*
		return -ENODEV;
	b->mtu = dev->mtu;

	case NETDEV_CHANGEMTU:
		tipc_reset_bearer(net, b);
#include "core.h"
#include <net/genetlink.h>
#endif	/* _TIPC_BEARER_H */
		ub->ifindex = dev->ifindex;
		b->mtu = dev->mtu - sizeof(struct iphdr)
				    tmp) {
			if (request->ssids[i].ssid_len >
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		break;
		    (int)(req->tp_block_size -
			  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)
		if (po->tp_version >= TPACKET_V3 &&
			goto out;
	int slot_count = 0;
	int ret = 0;
				space_args.total_spaces++;
			}
			}
#define PV_POWER5p	0x003B
#define PV_970FX	0x003C
		if (perf_event_overflow(event, 1, &data, regs)) {
	if (alpha_perf_event_set_period(event, hwc, idx)) {
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
		if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
static void ptrace_hbptriggered(struct perf_event *bp, int unused,
				     struct perf_sample_data *data,
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, regs->ARM_pc);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, addr);
	if (fault & VM_FAULT_MAJOR)
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0, regs, addr);
	if (fault & VM_FAULT_MAJOR)
	else if (fault & VM_FAULT_MINOR)
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0, regs, addr);
	else if (fault & VM_FAULT_MINOR)
	if (perf_event_overflow(event, 0, data, regs))
		mipspmu->disable_event(idx);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return simulate_ll(regs, opcode);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return simulate_sc(regs, opcode);
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		switch (rd) {
				1, 0, regs, 0);
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		return 0;
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
		      1, 0, regs, 0);
			1, 0, regs, regs->cp0_badvaddr);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
			1, 0, xcp, 0);
      emul:
	MIPS_FPU_EMU_INC_STATS(emulated);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
	if (unlikely(fault & VM_FAULT_ERROR)) {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
				1, 0, regs, address);
	if (fault & VM_FAULT_MAJOR) {
		tsk->maj_flt++;
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
				1, 0, regs, address);
	} else {
		tsk->min_flt++;
			1, 0, regs, 0);					\
		perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
		__PPC_WARN_EMULATED(type);				\
			1, 0, regs, regs->dar);				\
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
		__PPC_WARN_EMULATED(type);				\
			       struct pt_regs *regs, int nmi)
static void record_and_restart(struct perf_event *event, unsigned long val,
{
		if (perf_event_overflow(event, nmi, &data, regs))
			power_pmu_stop(event, 0);
			record_and_restart(event, val, regs, nmi);
			found = 1;
		}
			       struct pt_regs *regs, int nmi)
static void record_and_restart(struct perf_event *event, unsigned long val,
{
		if (perf_event_overflow(event, nmi, &data, regs))
			fsl_emb_pmu_stop(event, 0);
				record_and_restart(event, val, regs, nmi);
				found = 1;
			} else {
void ptrace_triggered(struct perf_event *bp, int nmi,
#ifdef CONFIG_HAVE_HW_BREAKPOINT
		      struct perf_sample_data *data, struct pt_regs *regs)
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		current->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		current->min_flt++;
				     regs, address);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
	address = trans_exc_code & __FAIL_ADDR_MASK;
	flags = FAULT_FLAG_ALLOW_RETRY;
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			tsk->maj_flt++;
				      regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			tsk->min_flt++;
				      regs, address);
void ptrace_triggered(struct perf_event *bp, int nmi,
		      struct perf_sample_data *data, struct pt_regs *regs)
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0,
		unaligned_fixups_notify(current, instruction, regs);
			      regs, address);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		tsk->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		tsk->min_flt++;
				     regs, address);
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
		tsk->maj_flt++;
				     regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
		tsk->min_flt++;
				     regs, address);
		if (perf_event_overflow(event, 1, &data, regs))
			sparc_pmu_stop(event, 0);
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
		switch (dir) {
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
		addr = compute_effective_address(regs, insn);
		switch(dir) {
		perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, addr);
						 ((insn >> 25) & 0x1f));
		switch (asi) {
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	if (insn & 0x2000) {
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
		die_if_kernel("lddfmna from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, sfar);
		die_if_kernel("stdfmna from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);
		die_if_kernel("unfinished/unimplemented FPop from kernel", regs);
	if (test_thread_flag(TIF_32BIT))
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			      regs, address);
		current->maj_flt++;
	} else {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			      regs, address);
		current->min_flt++;
	}
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			      regs, address);
		current->maj_flt++;
	} else {
		perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			      regs, address);
		current->min_flt++;
	}
		if (perf_event_overflow(event, 1, &data, regs))
			x86_pmu_stop(event, 0);
		if (perf_event_overflow(event, 1, &data, regs))
			x86_pmu_stop(event, 0);
	if (perf_output_begin(&handle, event, header.size * (top - at), 1, 1))
		return 1;
	if (perf_event_overflow(event, 1, &data, &regs))
		x86_pmu_stop(event, 0);
		if (perf_event_overflow(event, 1, &data, regs))
			continue;
			x86_pmu_stop(event, 0);
static void kgdb_hw_overflow_handler(struct perf_event *event, int nmi,
		struct perf_sample_data *data, struct pt_regs *regs)
static void ptrace_triggered(struct perf_event *bp, int nmi,
			     struct perf_sample_data *data,
	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
			tsk->maj_flt++;
				      regs, address);
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
			tsk->min_flt++;
				      regs, address);
typedef void (*perf_overflow_handler_t)(struct perf_event *, int,
					struct perf_sample_data *,
	int				nmi;
extern int perf_event_overflow(struct perf_event *event, int nmi,
				 struct perf_sample_data *data,
extern void __perf_sw_event(u32, u64, int, struct pt_regs *, u64);
perf_sw_event(u32 event_id, u64 nr, int nmi, struct pt_regs *regs, u64 addr)
static __always_inline void
{
		__perf_sw_event(event_id, nr, nmi, regs, addr);
		}
	}
	perf_sw_event(PERF_COUNT_SW_CONTEXT_SWITCHES, 1, 1, NULL, 0);
{
			     int nmi, int sample);
			     struct perf_event *event, unsigned int size,
extern void perf_output_end(struct perf_output_handle *handle);
perf_sw_event(u32 event_id, u64 nr, int nmi,
		     struct pt_regs *regs, u64 addr)			{ }
static inline void
static inline void
static void perf_event_output(struct perf_event *event, int nmi,
				struct perf_sample_data *data,
	if (perf_output_begin(&handle, event, header.size, nmi, 1))
		goto exit;
	ret = perf_output_begin(&handle, event, read_event.header.size, 0, 0);
	perf_event_header__init_id(&read_event.header, &sample, event);
	if (ret)
				task_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
				comm_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
				mmap_event->event_id.header.size, 0, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
				throttle_event.header.size, 1, 0);
	ret = perf_output_begin(&handle, event,
	if (ret)
static int __perf_event_overflow(struct perf_event *event, int nmi,
				   int throttle, struct perf_sample_data *data,
		if (nmi) {
			event->pending_disable = 1;
			irq_work_queue(&event->pending);
		} else
			perf_event_disable(event);
		event->pending_kill = POLL_HUP;
	}
		event->overflow_handler(event, nmi, data, regs);
	if (event->overflow_handler)
	else
		perf_event_output(event, nmi, data, regs);
	else
		if (nmi) {
			event->pending_wakeup = 1;
			irq_work_queue(&event->pending);
		} else
			perf_event_wakeup(event);
	if (event->fasync && event->pending_kill) {
	}
int perf_event_overflow(struct perf_event *event, int nmi,
			  struct perf_sample_data *data,
	return __perf_event_overflow(event, nmi, 1, data, regs);
{
}
				    int nmi, struct perf_sample_data *data,
static void perf_swevent_overflow(struct perf_event *event, u64 overflow,
				    struct pt_regs *regs)
		if (__perf_event_overflow(event, nmi, throttle,
	for (; overflow; overflow--) {
					    data, regs)) {
			       int nmi, struct perf_sample_data *data,
static void perf_swevent_event(struct perf_event *event, u64 nr,
			       struct pt_regs *regs)
		return perf_swevent_overflow(event, 1, nmi, data, regs);
	if (nr == 1 && hwc->sample_period == 1 && !event->attr.freq)
	perf_swevent_overflow(event, 0, nmi, data, regs);
}
				    u64 nr, int nmi,
static void do_perf_sw_event(enum perf_type_id type, u32 event_id,
				    struct perf_sample_data *data,
			perf_swevent_event(event, nr, nmi, data, regs);
		if (perf_swevent_match(event, type, event_id, data, regs))
	}
void __perf_sw_event(u32 event_id, u64 nr, int nmi,
			    struct pt_regs *regs, u64 addr)
{
	do_perf_sw_event(PERF_TYPE_SOFTWARE, event_id, nr, nmi, &data, regs);
			perf_swevent_event(event, count, 1, &data, regs);
		if (perf_tp_event_match(event, &data, regs))
	}
		perf_swevent_event(bp, 1, 1, &sample, regs);
	if (!bp->hw.state && !perf_exclude_event(bp, regs))
}
			if (perf_event_overflow(event, 0, &data, regs))
		if (!(event->attr.exclude_idle && current->pid == 0))
				ret = HRTIMER_NORESTART;

	if (handle->nmi) {
		handle->event->pending_wakeup = 1;
		irq_work_queue(&handle->event->pending);
	} else
		perf_event_wakeup(handle->event);
}
		      int nmi, int sample)
		      struct perf_event *event, unsigned int size,
{
	handle->nmi	= nmi;
		perf_sw_event(PERF_COUNT_SW_CPU_MIGRATIONS, 1, 1, NULL, 0);
		p->se.nr_migrations++;
	}
static void watchdog_overflow_callback(struct perf_event *event, int nmi,
		 struct perf_sample_data *data,
static void sample_hbp_handler(struct perf_event *bp, int nmi,
			       struct perf_sample_data *data,
			int data_len = elt->length -
					sizeof(struct oz_get_desc_rsp) + 1;
			u16 offs = le16_to_cpu(get_unaligned(&body->offset));
			u16 total_size =
				(struct oz_get_desc_rsp *)usb_hdr;
				le16_to_cpu(get_unaligned(&body->total_size));
		npoints = (size - 6) / 8;
		msc->ntouches = 0;
			      void **p, void *end,
			      void *dbuf, void *ticket_buf)
	dlen = ceph_x_decrypt(secret, p, end, dbuf,
		dlen = ceph_x_decrypt(&old_key, p, end, ticket_buf,
		ceph_decode_32_safe(p, end, dlen, bad);
		ceph_decode_need(p, end, dlen, bad);
out:
	return ret;
	char *dbuf;
	char *ticket_buf;
	dbuf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
	if (!dbuf)
		return -ENOMEM;

	ret = -ENOMEM;
	ticket_buf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
	if (!ticket_buf)
		goto out_dbuf;

		ret = process_one_ticket(ac, secret, &p, end,
					 dbuf, ticket_buf);
	while (num--) {
		if (ret)
			goto out;
		if (ret)
	kfree(ticket_buf);
	kfree(dbuf);
		return PTR_ERR(th);
	char *line, *p;
{
	int i;
	ssize_t ret = -EFAULT;
	int i;
	size_t len = iov_length(iv, count);
	size_t len = iov_length(iv, count);
	line = kmalloc(len + 1, GFP_KERNEL);
	if (line == NULL)
		return -ENOMEM;
	/*
	 * copy all vectors into a single string, to ensure we do
	 * not interleave our log line with other printk calls
	 */
	p = line;
	for (i = 0; i < count; i++) {
		if (copy_from_user(p, iv[i].iov_base, iv[i].iov_len))
	for (i = 0; i < count; i++) {
			goto out;
		p += iv[i].iov_len;
			goto out;
	}
	p[0] = '\0';
	}
	ret = printk("%s", line);
	/* printk can add a prefix */
	if (ret > len)
		ret = len;
out:
	kfree(line);
out:
	return ret;
#ifdef CONFIG_PRINTK
asmlinkage __printf(1, 0)
int vprintk(const char *fmt, va_list args);
asmlinkage __printf(1, 2) __cold
#define __LOG_BUF_LEN	(1 << CONFIG_LOG_BUF_SHIFT)

 * logbuf_lock protects log_buf, log_start, log_end, con_start and logged_chars
 * It is also used in interesting ways to provide interlocking in
 * console_unlock();.
 */
static DEFINE_RAW_SPINLOCK(logbuf_lock);

#define LOG_BUF_MASK (log_buf_len-1)
#define LOG_BUF(idx) (log_buf[(idx) & LOG_BUF_MASK])

/*
 * The indices into log_buf are not constrained to log_buf_len - they
 * must be masked before subscripting
 */
static unsigned log_start;	/* Index into log_buf: next char to be read by syslog() */
static unsigned con_start;	/* Index into log_buf: next char to be sent to consoles */
static unsigned log_end;	/* Index into log_buf: most-recently-written-char + 1 */

/*
#ifdef CONFIG_PRINTK
static char __log_buf[__LOG_BUF_LEN];
static int log_buf_len = __LOG_BUF_LEN;
static unsigned logged_chars; /* Number of chars produced since last read+clear operation */
static int saved_console_loglevel = -1;
static char *log_buf = __log_buf;
	VMCOREINFO_SYMBOL(log_end);
	VMCOREINFO_SYMBOL(logged_chars);
	VMCOREINFO_SYMBOL(log_buf_len);
}
	unsigned start, dest_idx, offset;

	offset = start = min(con_start, log_start);
	dest_idx = 0;
	while (start != log_end) {
		start++;
		dest_idx++;
	}
	log_start -= offset;
	con_start -= offset;
	log_end -= offset;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
int do_syslog(int type, char __user *buf, int len, bool from_file)
	unsigned i, j, limit, count;
	int do_clear = 0;
	char c;
{
	int error;
							(log_start - log_end));
		error = wait_event_interruptible(log_wait,
		if (error)
		i = 0;
		raw_spin_lock_irq(&logbuf_lock);
		while (!error && (log_start != log_end) && i < len) {
			c = LOG_BUF(log_start);
			log_start++;
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,buf);
			buf++;
			i++;
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (!error)
			error = i;
			goto out;
		break;
		do_clear = 1;
	case SYSLOG_ACTION_READ_CLEAR:
		count = len;
		if (count > log_buf_len)
			count = log_buf_len;
		raw_spin_lock_irq(&logbuf_lock);
		if (count > logged_chars)
			count = logged_chars;
		if (do_clear)
			logged_chars = 0;
		limit = log_end;
		/*
		 * __put_user() could sleep, and while we sleep
		 * printk() could overwrite the messages
		 * we try to copy to user space. Therefore
		 * the messages are copied in reverse. <manfreds>
		 */
		for (i = 0; i < count && !error; i++) {
			j = limit-1-i;
			if (j + log_buf_len < log_end)
				break;
			c = LOG_BUF(j);
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,&buf[count-1-i]);
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (error)
			break;
		error = i;
		if (i != count) {
			int offset = count-error;
			/* buffer overflow during copy, correct user buffer. */
			for (i = 0; i < error; i++) {
				if (__get_user(c,&buf[i+offset]) ||
				    __put_user(c,&buf[i])) {
					error = -EFAULT;
					break;
				}
				cond_resched();
			}
		}
		}
		break;
		logged_chars = 0;
		break;
	case SYSLOG_ACTION_CLEAR:
		error = log_end - log_start;
	case SYSLOG_ACTION_SIZE_UNREAD:
		break;
	syslog_data[2] = log_buf + log_end -
		(logged_chars < log_buf_len ? logged_chars : log_buf_len);
	syslog_data[3] = log_buf + log_end;
	syslog_data[1] = log_buf + log_buf_len;
}
/*
 * Call the console drivers on a range of log_buf
 */
static void __call_console_drivers(unsigned start, unsigned end)
{
	struct console *con;

	for_each_console(con) {
		if (exclusive_console && con != exclusive_console)
			continue;
		if ((con->flags & CON_ENABLED) && con->write &&
				(cpu_online(smp_processor_id()) ||
				(con->flags & CON_ANYTIME)))
			con->write(con, &LOG_BUF(start), end - start);
	}
}

 * Write out chars from start to end - 1 inclusive
 */
static void _call_console_drivers(unsigned start,
				unsigned end, int msg_log_level)
{
	trace_console(&LOG_BUF(0), start, end, log_buf_len);

	if ((msg_log_level < console_loglevel || ignore_loglevel) &&
			console_drivers && start != end) {
		if ((start & LOG_BUF_MASK) > (end & LOG_BUF_MASK)) {
			/* wrapped write */
			__call_console_drivers(start & LOG_BUF_MASK,
						log_buf_len);
			__call_console_drivers(0, end & LOG_BUF_MASK);
		} else {
			__call_console_drivers(start, end);
		}
	}
}

/*
 * Parse the syslog header <[0-9]*>. The decimal value represents 32bit, the
 * lower 3 bit are the log level, the rest are the log facility. In case
 * userspace passes usual userspace syslog messages to /dev/kmsg or
 * /dev/ttyprintk, the log prefix might contain the facility. Printk needs
 * to extract the correct log level for in-kernel processing, and not mangle
 * the original value.
 *
 * If a prefix is found, the length of the prefix is returned. If 'level' is
 * passed, it will be filled in with the log level without a possible facility
 * value. If 'special' is passed, the special printk prefix chars are accepted
 * and returned. If no valid header is found, 0 is returned and the passed
 * variables are not touched.
 */
static size_t log_prefix(const char *p, unsigned int *level, char *special)
{
	unsigned int lev = 0;
	char sp = '\0';
	size_t len;

	if (p[0] != '<' || !p[1])
		return 0;
	if (p[2] == '>') {
		/* usual single digit level number or special char */
		switch (p[1]) {
		case '0' ... '7':
			lev = p[1] - '0';
			break;
		case 'c': /* KERN_CONT */
		case 'd': /* KERN_DEFAULT */
			sp = p[1];
			break;
		default:
			return 0;
		}
		len = 3;
	} else {
		/* multi digit including the level and facility number */
		char *endp = NULL;

		lev = (simple_strtoul(&p[1], &endp, 10) & 7);
		if (endp == NULL || endp[0] != '>')
			return 0;
		len = (endp + 1) - p;
	}

	/* do not accept special char if not asked for */
	if (sp && !special)
		return 0;

	if (special) {
		*special = sp;
		/* return special char, do not touch level */
		if (sp)
			return len;
	}

	if (level)
		*level = lev;
	return len;
}

/*
static void call_console_drivers(unsigned start, unsigned end)
{
	unsigned cur_index, start_print;
	static int msg_level = -1;

	BUG_ON(((int)(start - end)) > 0);

	cur_index = start;
	start_print = start;
	while (cur_index != end) {
		if (msg_level < 0 && ((end - cur_index) > 2)) {
			/* strip log prefix */
			cur_index += log_prefix(&LOG_BUF(cur_index), &msg_level, NULL);
			start_print = cur_index;
		}
		while (cur_index != end) {
			char c = LOG_BUF(cur_index);

			cur_index++;
			if (c == '\n') {
				if (msg_level < 0) {
					/*
					 * printk() has already given us loglevel tags in
					 * the buffer.  This code is here in case the
					 * log buffer has wrapped right round and scribbled
					 * on those tags
					 */
					msg_level = default_message_loglevel;
				}
				_call_console_drivers(start_print, cur_index, msg_level);
				msg_level = -1;
				start_print = cur_index;
				break;
			}
		}
	}
	_call_console_drivers(start_print, end, msg_level);
}
{
static void emit_log_char(char c)
{
	LOG_BUF(log_end) = c;
	log_end++;
	if (log_end - log_start > log_buf_len)
		log_start = log_end - log_buf_len;
	if (log_end - con_start > log_buf_len)
		con_start = log_end - log_buf_len;
	if (logged_chars < log_buf_len)
		logged_chars++;
}
#if defined(CONFIG_PRINTK_TIME)
static bool printk_time = 1;
#else
static bool printk_time = 0;
#endif
module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);

static bool always_kmsg_dump;
module_param_named(always_kmsg_dump, always_kmsg_dump, bool, S_IRUGO | S_IWUSR);

/**
 * printk - print a kernel message
 * @fmt: format string
 *
 * This is printk().  It can be called from any context.  We want it to work.
 *
 * We try to grab the console_lock.  If we succeed, it's easy - we log the output and
 * call the console drivers.  If we fail to get the semaphore we place the output
 * into the log buffer and return.  The current holder of the console_sem will
 * notice the new output in console_unlock(); and will send it to the
 * consoles before releasing the lock.
 *
 * One effect of this deferred printing is that code which calls printk() and
 * then changes console_loglevel may break. This is because console_loglevel
 * is inspected when the actual printing occurs.
 *
 * See also:
 * printf(3)
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */

asmlinkage int printk(const char *fmt, ...)
{
	va_list args;
	int r;

#ifdef CONFIG_KGDB_KDB
	if (unlikely(kdb_trap_printk)) {
		va_start(args, fmt);
		r = vkdb_printf(fmt, args);
		va_end(args);
		return r;
	}
#endif
	va_start(args, fmt);
	r = vprintk(fmt, args);
	va_end(args);

	return r;
}

/* cpu currently holding logbuf_lock */
static volatile unsigned int printk_cpu = UINT_MAX;

	printk_cpu = UINT_MAX;
	}
	if (wake)
static const char recursion_bug_msg [] =
		KERN_CRIT "BUG: recent printk recursion!\n";
static int recursion_bug;
static int new_text_line = 1;
static char printk_buf[1024];
asmlinkage int vprintk(const char *fmt, va_list args)
{
	int printed_len = 0;
	int current_log_level = default_message_loglevel;
{
	unsigned long flags;
	char *p;
	size_t plen;
	char special;
	int this_cpu;
	if (unlikely(printk_cpu == this_cpu)) {
	printk_cpu = this_cpu;
	raw_spin_lock(&logbuf_lock);
	if (recursion_bug) {
		recursion_bug = 0;
		strcpy(printk_buf, recursion_bug_msg);
		printed_len = strlen(recursion_bug_msg);
		recursion_bug = 0;
	}
	/* Emit the output into the temporary buffer */
	printed_len += vscnprintf(printk_buf + printed_len,
				  sizeof(printk_buf) - printed_len, fmt, args);
	p = printk_buf;
	/* Read log level and handle special printk prefix */
	plen = log_prefix(p, &current_log_level, &special);
	if (plen) {
		p += plen;
		switch (special) {
		case 'c': /* Strip <c> KERN_CONT, continue line */
			plen = 0;
			break;
		case 'd': /* Strip <d> KERN_DEFAULT, start new line */
			plen = 0;
		default:
			if (!new_text_line) {
				emit_log_char('\n');
				new_text_line = 1;
			}
	/*
	 * Copy the output into log_buf. If the caller didn't provide
	 * the appropriate log prefix, we insert them here
	 */
	for (; *p; p++) {
		if (new_text_line) {
			new_text_line = 0;

			if (plen) {
				/* Copy original log prefix */
				int i;

				for (i = 0; i < plen; i++)
					emit_log_char(printk_buf[i]);
				printed_len += plen;
			} else {
				/* Add log prefix */
				emit_log_char('<');
				emit_log_char(current_log_level + '0');
				emit_log_char('>');
				printed_len += 3;
			}
			if (printk_time) {
				/* Add the current time stamp */
				char tbuf[50], *tp;
				unsigned tlen;
				unsigned long long t;
				unsigned long nanosec_rem;

				t = cpu_clock(printk_cpu);
				nanosec_rem = do_div(t, 1000000000);
				tlen = sprintf(tbuf, "[%5lu.%06lu] ",
						(unsigned long) t,
						nanosec_rem / 1000);

				for (tp = tbuf; tp < tbuf + tlen; tp++)
					emit_log_char(*tp);
				printed_len += tlen;
			}
			if (!*p)
				break;
		}
		emit_log_char(*p);
		if (*p == '\n')
			new_text_line = 1;
	}
	 * Try to acquire and then immediately release the
	 * console semaphore. The release will do all the
	 * actual magic (print out buffers, wake up klogd,
	 * etc).
	 * The console_trylock_for_printk() function
	 * will release 'logbuf_lock' regardless of whether it
	 * actually gets the semaphore or not.
EXPORT_SYMBOL(printk);
}
EXPORT_SYMBOL(vprintk);
#else
static void call_console_drivers(unsigned start, unsigned end)
{
 * Delayed printk facility, for scheduler-internal messages:
 * If there is output waiting for klogd, we wake it up.
{
	unsigned long flags;
	unsigned _con_start, _log_end;
	unsigned wake_klogd = 0, retry = 0;
	unsigned long flags;
	for ( ; ; ) {
again:
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		wake_klogd |= log_start - log_end;
		if (con_start == log_end)
			break;			/* Nothing to print */
		_con_start = con_start;
		_log_end = log_end;
		con_start = log_end;		/* Flush */
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock(&logbuf_lock);
		raw_spin_unlock(&logbuf_lock);
		stop_critical_timings();	/* don't trace print latency */
		call_console_drivers(_con_start, _log_end);
		stop_critical_timings();	/* don't trace print latency */
		start_critical_timings();
	if (con_start != log_end)
		retry = 1;
	raw_spin_lock(&logbuf_lock);
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
		con_start = log_start;
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	unsigned long end;
	unsigned chars;
{
	struct kmsg_dumper *dumper;
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	end = log_end & LOG_BUF_MASK;
	chars = logged_chars;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	if (chars > end) {
		s1 = log_buf + log_buf_len - chars + end;
		l1 = chars - end;
		s2 = log_buf;
		l2 = end;
	} else {
		s2 = log_buf + end - chars;
		l2 = chars;
	}
	}
	kgid_t group = current_egid();
	kgid_t low, high;
	kgid_t low, high;
			if (gid_lte(low, gid) && gid_lte(gid, high))
	/* it might be unflagged overflow */
	key_ref_t key_ref, skey_ref;
	int rc;
				if (unlikely(t == 0)) {
					while (unlikely(*ip == 0)) {
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
		return -EINVAL;
		return -EINVAL;
		pr_debug("payload len = 0\n");
	}
		return -EINVAL;
	if (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)
	return NF_ACCEPT;
}
	int result, err = 0, retries = 0;
      retry:
			      cgc->buffer, cgc->buflen,
			      (unsigned char *)cgc->sense, &sshdr,
	result = scsi_execute(SDev, cgc->cmd, cgc->data_direction,
			      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);
