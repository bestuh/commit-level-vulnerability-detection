		if (inet->hdrincl)
			goto done;
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			   RT_SCOPE_UNIVERSE,
			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
			   inet_sk_flowi_flags(sk) |
			   daddr, saddr, 0, 0, sk->sk_uid);
	if (!inet->hdrincl) {
		rfv.msg = msg;
	if (inet->hdrincl)
		err = raw_send_hdrinc(sk, &fl4, msg, len,
			struct x86_exception *fault);
			unsigned int bytes,
			 struct x86_exception *fault);
			 unsigned long addr, void *val, unsigned int bytes,
	return ctxt->ops->read_std(ctxt, linear, data, size, &ctxt->exception);
{
}
{
}
	return ctxt->ops->read_std(ctxt, linear, data, size, &ctxt->exception);
		return rc;
}
	r = ops->read_std(ctxt, base + 102, &io_bitmap_ptr, 2, NULL);
#endif
	if (r != X86EMUL_CONTINUE)
	r = ops->read_std(ctxt, base + io_bitmap_ptr + port/8, &perm, 2, NULL);
		return false;
	if (r != X86EMUL_CONTINUE)
			     struct x86_exception *exception)
			     gva_t addr, void *val, unsigned int bytes,
{
	return kvm_read_guest_virt_helper(addr, val, bytes, vcpu, 0, exception);
	struct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);
}
			      unsigned int bytes, struct x86_exception *exception)
static int emulator_write_std(struct x86_emulate_ctxt *ctxt, gva_t addr, void *val,
{
	struct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);
					   PFERR_WRITE_MASK, exception);
	return kvm_write_guest_virt_helper(addr, val, bytes, vcpu,
}
	    kvm_read_guest_virt(&vcpu->arch.emulate_ctxt,
				kvm_get_linear_rip(vcpu), sig, sizeof(sig), &e) == 0 &&
	if (force_emulation_prefix &&
	    memcmp(sig, "\xf\xbkvm", sizeof(sig)) == 0) {
static inline void __d_set_inode_and_type(struct dentry *dentry,
	const char *old_name;
	struct dentry *dentry = NULL, *trap;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_oldname_free(old_name);
	if (error) {
		goto exit;
	fsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,
	d_move(old_dentry, dentry);
		d_is_dir(old_dentry),
	fsnotify_oldname_free(old_name);
		NULL, old_dentry);
	unlock_rename(new_dir, old_dir);
	const unsigned char *old_name;
	unsigned max_links = new_dir->i_sb->s_max_links;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
	dget(new_dentry);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	if (!error) {
			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
	fsnotify_oldname_free(old_name);
	}
	else if (p_inode->i_fsnotify_mask & mask) {
		if (path)
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
		else
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
	}
#if defined(CONFIG_FSNOTIFY)	/* notify helpers */

/*
 * fsnotify_oldname_init - save off the old filename before we change it
 */
static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)
{
	return kstrdup(name, GFP_KERNEL);
}

/*
 * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init
 */
static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
	kfree(old_name);
}

#else	/* CONFIG_FSNOTIFY */

static inline const char *fsnotify_oldname_init(const unsigned char *name)
{
	return NULL;
}

static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
}

#endif	/*  CONFIG_FSNOTIFY */

	struct n_hdlc_buf *link;
struct n_hdlc_buf {
	int		  count;
	struct n_hdlc_buf *head;
	struct n_hdlc_buf *tail;
struct n_hdlc_buf_list {
	int		  count;
 * @tbuf - currently transmitting tx buffer
	struct n_hdlc_buf	*tbuf;
static void n_hdlc_buf_put(struct n_hdlc_buf_list *list,
 	spin_lock_irqsave(&n_hdlc->tx_buf_list.spinlock, flags);
	spin_unlock_irqrestore(&n_hdlc->tx_buf_list.spinlock, flags);
	kfree(n_hdlc->tbuf);
	/* get current transmit buffer or get new transmit */
	/* buffer from list of pending transmit buffers */

	tbuf = n_hdlc->tbuf;
	if (!tbuf)
		tbuf = n_hdlc_buf_get(&n_hdlc->tx_buf_list);

	while (tbuf) {
			n_hdlc->tbuf = tbuf;
		if (actual == -ERESTARTSYS) {
			break;

			/* this tx buffer is done */
			n_hdlc->tbuf = NULL;

			n_hdlc_buf_put(&n_hdlc->tx_free_buf_list, tbuf);

			/* buffer not accepted by driver */
			/* set this buffer as pending buffer */
			n_hdlc->tbuf = tbuf;
					__FILE__,__LINE__,tbuf);
			break;

	unsigned long flags;
	if (debuglevel >= DEBUG_LEVEL_INFO)
		if (n_hdlc->rx_buf_list.head)
			count = n_hdlc->rx_buf_list.head->count;
		spin_lock_irqsave(&n_hdlc->rx_buf_list.spinlock,flags);
		else
		if (n_hdlc->tx_buf_list.head)
			count += n_hdlc->tx_buf_list.head->count;
		spin_lock_irqsave(&n_hdlc->tx_buf_list.spinlock,flags);
		spin_unlock_irqrestore(&n_hdlc->tx_buf_list.spinlock,flags);
		if (n_hdlc->rx_buf_list.head)
			mask |= POLLIN | POLLRDNORM;	/* readable */
				n_hdlc->tx_free_buf_list.head)
		if (!tty_is_writelocked(tty) &&
			mask |= POLLOUT | POLLWRNORM;	/* writable */

	spin_lock_init(&n_hdlc->tx_buf_list.spinlock);
 * @list - pointer to buffer list
static void n_hdlc_buf_put(struct n_hdlc_buf_list *list,
			   struct n_hdlc_buf *buf)
	spin_lock_irqsave(&list->spinlock,flags);

	buf->link=NULL;
	if (list->tail)
		list->tail->link = buf;
	else
		list->head = buf;
	list->tail = buf;
	(list->count)++;

	spin_unlock_irqrestore(&list->spinlock,flags);

	unsigned long flags;
}	/* end of n_hdlc_buf_put() */
 * @list - pointer to HDLC buffer list
static struct n_hdlc_buf* n_hdlc_buf_get(struct n_hdlc_buf_list *list)
{
	spin_lock_irqsave(&list->spinlock,flags);

	buf = list->head;
	struct n_hdlc_buf *buf;
	if (buf) {
		list->head = buf->link;
		(list->count)--;
	if (buf) {
	}
	if (!list->head)
		list->tail = NULL;

	spin_unlock_irqrestore(&list->spinlock,flags);
	}
	return buf;

		inode->i_gid = dir->i_gid;
		if (S_ISDIR(mode))
			mode |= S_ISGID;
	} else
				fdput(f);
	int mapping = (*event_map)[config];
	return mapping == HW_OP_UNSUPPORTED ? -ENOENT : mapping;
	lock_sock(sk);
	if (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))
	err = -EINVAL;
	if (sk->sk_state != TCP_CLOSE)
	if (len <= 0x7f) {
	if (move_group) {
		if (gctx->task == TASK_TOMBSTONE) {
		}
	} else {
		mutex_unlock(&gctx->mutex);
	if (move_group)
	mutex_unlock(&ctx->mutex);
		mutex_unlock(&gctx->mutex);
	if (move_group)
	mutex_unlock(&ctx->mutex);
		ops->destroy(dev);
		return ret;
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
int xt_compat_check_entry_offsets(const void *base,
			     void __user **dstptr, unsigned int *size);
				  unsigned int target_offset,
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e, e->target_offset,
					    e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
/* see xt_check_entry_offsets */
int xt_compat_check_entry_offsets(const void *base,
				  unsigned int target_offset,
{
	const struct compat_xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
{
	const struct xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
	char *command, *args = value;
	struct apparmor_audit_data aad = {0,};
	size_t arg_size;
	/* args points to a PAGE_SIZE buffer, AppArmor requires that
	 * the buffer must be null terminated or have size <= PAGE_SIZE -1
	 * so that AppArmor can null terminate them
	 */
	if (args[size - 1] != '\0') {
		if (size == PAGE_SIZE)
			return -EINVAL;
		args[size] = '\0';
	}

	args = value;
	args = strim(args);
		return -EINVAL;
	if (!args)
	args = skip_spaces(args);
		return -EINVAL;
	if (!*args)
		return -EINVAL;
		error = size;
	return error;
	aad.error = -EINVAL;
	aad.info = name;
	aa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);
	return -EINVAL;
	aa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);
}
		u32 vmacache_seqnum;                   /* per-thread vmacache */
		struct rb_root mm_rb;
#ifdef CONFIG_MMU
	u32 seqnum;
struct vmacache {
	struct vm_area_struct *vmas[VMACACHE_SIZE];
		VMACACHE_FULL_FLUSHES,
extern void vmacache_flush_all(struct mm_struct *mm);

	/* deal with overflows */
	if (unlikely(mm->vmacache_seqnum == 0))
		vmacache_flush_all(mm);
	pr_emerg("mm %px mmap %px seqnum %d task_size %lu\n"
{
#ifdef CONFIG_MMU
		mm, mm->mmap, mm->vmacache_seqnum, mm->task_size,
#ifdef CONFIG_MMU
 * Flush vma caches for threads that share a given mm.
 *
 * The operation is safe because the caller holds the mmap_sem
 * exclusively and other threads accessing the vma cache will
 * have mmap_sem held at least for read, so no extra locking
 * is required to maintain the vma cache.
 */
void vmacache_flush_all(struct mm_struct *mm)
{
	struct task_struct *g, *p;

	count_vm_vmacache_event(VMACACHE_FULL_FLUSHES);

	rcu_read_lock();
	for_each_process_thread(g, p) {
		/*
		 * Only flush the vmacache pointers as the
		 * mm seqnum is already set and curr's will
		 * be set upon invalidation when the next
		 * lookup is done.
		 */
		if (mm == p->mm)
			vmacache_flush(p);
	}
	rcu_read_unlock();
}

/*
	if (io_data->ffs->ffs_eventfd &&
	    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))
		eventfd_signal(io_data->ffs->ffs_eventfd, 1);
		return -ENODEV;
	b->mtu = dev->mtu;

	case NETDEV_CHANGEMTU:
		tipc_reset_bearer(net, b);
#include "core.h"
#include <net/genetlink.h>
#endif	/* _TIPC_BEARER_H */
		ub->ifindex = dev->ifindex;
		b->mtu = dev->mtu - sizeof(struct iphdr)
				    tmp) {
			if (request->ssids[i].ssid_len >
		    (int)(req->tp_block_size -
			  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)
		if (po->tp_version >= TPACKET_V3 &&
			goto out;
fail:
		__bdevname(dev, b), PTR_ERR(bdev));
	if (*options && *options != ',') {
		       (char *) *data);
#define PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
#define __P100  PAGE_EXECONLY
#define __P011  PAGE_COPY
#define __P101  PAGE_READONLY_EXEC
#define __S100  PAGE_EXECONLY
#define __S011  PAGE_SHARED
#define __S101  PAGE_READONLY_EXEC
{
		if (!pte_special(pte) && pte_exec(pte))
	int fault, sig, code;
	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
			goto err_free_dev;
		if (err < 0)
			goto err_free_dev;
		if (err < 0)
 err_free_dev:
	free_netdev(dev);
static int
		union futex_key *key, struct futex_pi_state **ps,
		struct task_struct *task)
lookup_pi_state(u32 uval, struct futex_hash_bucket *hb,
{
			 * Another waiter already exists - bump up
			 * the refcount and return its pi_state:
			 * Userspace might have messed up non-PI and PI futexes
			 * When pi_state->owner is NULL then the owner died
			 * and another waiter is on the fly. pi_state->owner
			 * is fixed up by the task which acquires
			 * pi_state->rt_mutex.
			 *
			 * We do not check for pid == 0 which can happen when
			 * the owner died and robust_list_exit() cleared the
			 * TID.
			if (pid && pi_state->owner) {
				 * Bail out if user space manipulated the
				 * futex value.
				if (pid != task_pid_vnr(pi_state->owner))
					return -EINVAL;
			 * Protect against a corrupted uval. If uval
			 * is 0x80000000 then pid is 0 and the waiter
			 * bit is set. So the deadlock check in the
			 * calling code has failed and we did not fall
			 * into the check above due to !pid.
			if (task && pi_state->owner == task)
				return -EDEADLK;
			atomic_inc(&pi_state->refcount);

	 * the new pi_state to it, but bail out when TID = 0
	pi_state = alloc_pi_state();
	ret = lookup_pi_state(uval, hb, key, ps, task);
			ret = lookup_pi_state(ret, hb2, &key2, &pi_state, NULL);
	kgid_t group = current_egid();
	kgid_t low, high;
	kgid_t low, high;
			if (gid_lte(low, gid) && gid_lte(gid, high))
	unsigned long		tp_value;
	__u8			used_cp[16];	/* thread used copro */
#ifdef CONFIG_CRUNCH
	.macro set_tls_none, tp, tmp1, tmp2
#ifdef __ASSEMBLY__
	.endm
	.macro set_tls_v6k, tp, tmp1, tmp2
	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
	.endm
	.macro set_tls_v6, tp, tmp1, tmp2
	ldr	\tmp1, =elf_hwcap
	mcrne	p15, 0, \tp, c13, c0, 3		@ yes, set TLS register
	streq	\tp, [\tmp2, #-15]		@ set TLS value at 0xffff0ff0
	.endm
	.macro set_tls_software, tp, tmp1, tmp2
	mov	\tmp1, #0xffff0fff
#define set_tls		set_tls_none
#define has_tls_reg		1
#elif defined(CONFIG_CPU_V6)
#define set_tls		set_tls_v6
#define has_tls_reg		(elf_hwcap & HWCAP_TLS)
#elif defined(CONFIG_CPU_32v6K)
#define set_tls		set_tls_v6k
#define has_tls_reg		1
#else
#define set_tls		set_tls_software
#define has_tls_reg		0
#endif
#endif	/* __ASMARM_TLS_H */
	ldr	r3, [r2, #TI_TP_VALUE]
 THUMB(	str	lr, [ip], #4		   )
#ifdef CONFIG_CPU_USE_DOMAINS
	set_tls	r3, r4, r5
#endif
#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
#include <asm/mach/time.h>
		thread->tp_value = childregs->ARM_r3;
	if (clone_flags & CLONE_SETTLS)
			ret = put_user(task_thread_info(child)->tp_value,
		case PTRACE_GET_THREAD_AREA:
				       datap);
		thread->tp_value = regs->ARM_r0;
	case NR(set_tls):
		if (tls_emu)
	regs->uregs[reg] = current_thread_info()->tp_value;
		return 1;
	regs->ARM_pc += 4;
	struct platform_device *pdev = to_platform_device(dev);
	struct platform_device *pdev = to_platform_device(dev);
		unshare_flags |= CLONE_THREAD;
	if (unshare_flags & CLONE_NEWUSER)
#include <linux/projid.h>
	if (!ns_capable(user_ns, CAP_SYS_ADMIN))
	if (!handle->h_transaction) {
		err = jbd2_journal_stop(handle);
		return handle->h_err ? handle->h_err : err;
	if (!handle->h_transaction) {
	}
	err = handle->h_err;
	s->s_magic = ECRYPTFS_SUPER_MAGIC;
	ufs->upper_mnt = clone_private_mount(&upperpath);
	struct rcu_head		rcu;
};
	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
 * fasync_helper() is used by almost all character device drivers
 * to set up the fasync queue. It returns negative on error, 0 if it did
 * no changes and positive if it added/deleted the entry.
int fasync_helper(int fd, struct file * filp, int on, struct fasync_struct **fapp)
{
	struct fasync_struct *new = NULL;
	if (on) {
		new = kmem_cache_alloc(fasync_cache, GFP_KERNEL);
		if (!new)
			return -ENOMEM;
	}
	}
	/*
	 * We need to take f_lock first since it's not an IRQ-safe
	 * lock.
	 */
		if (fa->fa_file == filp) {
			if(on) {
				fa->fa_fd = fd;
				kmem_cache_free(fasync_cache, new);
			} else {
				*fp = fa->fa_next;
				kmem_cache_free(fasync_cache, fa);
				result = 1;
			}
			goto out;
		}
	for (fp = fapp; (fa = *fp) != NULL; fp = &fa->fa_next) {
	}
	if (on) {
		new->magic = FASYNC_MAGIC;
		new->fa_file = filp;
		new->fa_fd = fd;
		new->fa_next = *fapp;
		*fapp = new;
		result = 1;
	}
out:
	if (on)
		filp->f_flags |= FASYNC;
	else
		filp->f_flags &= ~FASYNC;
EXPORT_SYMBOL(fasync_helper);
		len = INT_MAX;
	sock = sockfd_lookup_light(fd, &err, &fput_needed);
		size = INT_MAX;
	sock = sockfd_lookup_light(fd, &err, &fput_needed);
