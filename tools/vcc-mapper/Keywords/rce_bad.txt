		if (inet->hdrincl)
			goto done;
			   inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
			   RT_SCOPE_UNIVERSE,
			   inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
			   inet_sk_flowi_flags(sk) |
			   daddr, saddr, 0, 0, sk->sk_uid);
	if (!inet->hdrincl) {
		rfv.msg = msg;
	if (inet->hdrincl)
		err = raw_send_hdrinc(sk, &fl4, msg, len,
	if (!fepriv)
		return;

	kfree(fepriv);
	struct pci_dev *pdev = to_pci_dev(dev);
	struct pci_dev *pdev = to_pci_dev(dev);
	return snprintf(buf, PAGE_SIZE, "%s\n", pdev->driver_override);
		ret = sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;
		ret = min_t(int, ret, scat->length - conn->c_xmit_data_off);
		return ret;
		scat = &rm->data.op_sg[sg];
	}
#include <linux/security.h>
#include <linux/pid.h>
	if ((creds->pid == task_tgid_vnr(current) || nsown_capable(CAP_SYS_ADMIN)) &&
	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
			ret = -EFAULT;
		if (unlikely(ret)) {
			*pagep = page;
			ret = -EFAULT;
		if (unlikely(ret)) {
			*pagep = page;
		if (unlikely(err == -EFAULT)) {
			up_read(&dst_mm->mmap_sem);
		if (unlikely(err == -EFAULT)) {
			void *page_kaddr;
	if (iov_count) {
static inline void __d_set_inode_and_type(struct dentry *dentry,
	const char *old_name;
	struct dentry *dentry = NULL, *trap;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
		fsnotify_oldname_free(old_name);
	if (error) {
		goto exit;
	fsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,
	d_move(old_dentry, dentry);
		d_is_dir(old_dentry),
	fsnotify_oldname_free(old_name);
		NULL, old_dentry);
	unlock_rename(new_dir, old_dir);
	const unsigned char *old_name;
	unsigned max_links = new_dir->i_sb->s_max_links;
	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
	dget(new_dentry);
		fsnotify_move(old_dir, new_dir, old_name, is_dir,
	if (!error) {
			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
	fsnotify_oldname_free(old_name);
	}
	else if (p_inode->i_fsnotify_mask & mask) {
		if (path)
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
		else
				       dentry->d_name.name, 0);
			ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
	}
#if defined(CONFIG_FSNOTIFY)	/* notify helpers */

/*
 * fsnotify_oldname_init - save off the old filename before we change it
 */
static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)
{
	return kstrdup(name, GFP_KERNEL);
}

/*
 * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init
 */
static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
	kfree(old_name);
}

#else	/* CONFIG_FSNOTIFY */

static inline const char *fsnotify_oldname_init(const unsigned char *name)
{
	return NULL;
}

static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
}

#endif	/*  CONFIG_FSNOTIFY */

int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
	if (ret == -EEXIST)
				    btrfs_inode_type(inode), index);
		goto fail_dir_item;
		return -ENOTEMPTY;
	down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);
	BUG_ON(ret == -EEXIST);
	if (ret) {
#define IPSKB_FORWARDED		1
#define IPSKB_XFRM_TUNNEL_SIZE	2
#define IPSKB_XFRM_TRANSFORMED	4
#define IPSKB_FRAG_COMPLETE	8
#define IPSKB_REROUTED		16
	if (rt->rt_flags&RTCF_DOREDIRECT && !opt->srr && !skb_sec_path(skb))
		ip_rt_send_redirect(skb);
	if (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&
	    (IN_DEV_SHARED_MEDIA(out_dev) ||
	     inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res)))) {
		flags |= RTCF_DOREDIRECT;
		do_cache = false;
	}
	    (IN_DEV_SHARED_MEDIA(out_dev) ||
		r->rtm_flags |= RTM_F_NOTIFY;
	lock_sock(sk);
	lock_sock(sk);
	for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;

Symbols/Function Pointers
#define PTR1 ((void*)0x01234567)
#define PTR2 ((void*)(long)(int)0xfedcba98)

#if BITS_PER_LONG == 64
#define PTR1_ZEROES "000000000"
#define PTR1_SPACES "         "
#define PTR1_STR "1234567"
#define PTR2_STR "fffffffffedcba98"
#define PTR_WIDTH 16
#else
#define PTR1_ZEROES "0"
#define PTR1_SPACES " "
#define PTR1_STR "1234567"
#define PTR2_STR "fedcba98"
#define PTR_WIDTH 8
#endif
#define PTR_WIDTH_STR stringify(PTR_WIDTH)

static void __init
	test(PTR1_ZEROES PTR1_STR " " PTR2_STR, "%p %p", PTR1, PTR2);
	/*
	 * The field width is overloaded for some %p extensions to
	 * pass another piece of information. For plain pointers, the
	 * behaviour is slightly odd: One cannot pass either the 0
	 * flag nor a precision to %p without gcc complaining, and if
	 * one explicitly gives a field width, the number is no longer
	 * zero-padded.
	 */
	test("|" PTR1_STR PTR1_SPACES "  |  " PTR1_SPACES PTR1_STR "|",
	     "|%-*p|%*p|", PTR_WIDTH+2, PTR1, PTR_WIDTH+2, PTR1);
	test("|" PTR2_STR "  |  " PTR2_STR "|",
	     "|%-*p|%*p|", PTR_WIDTH+2, PTR2, PTR_WIDTH+2, PTR2);
{
	/*
	 * Unrecognized %p extensions are treated as plain %p, but the
	 * alphanumeric suffix is ignored (that is, does not occur in
	 * the output.)
	 */
	test("|"PTR1_ZEROES PTR1_STR"|", "|%p0y|", PTR1);
	test("|"PTR2_STR"|", "|%p0y|", PTR2);
}
{
}
#include <net/addrconf.h>
#ifdef CONFIG_BLOCK
	spec.flags |= SMALL;
	if (spec.field_width == -1) {
		spec.field_width = default_width;
		spec.flags |= ZEROPAD;
	}
	spec.base = 16;
	return number(buf, end, (unsigned long) ptr, spec);
}
	blk_free_flush_queue(q->fq);
	return -ENOMEM;
       ecryptfs_opt_unlink_sigs, ecryptfs_opt_mount_auth_tok_only,
       ecryptfs_opt_err };
	{ecryptfs_opt_mount_auth_tok_only, "ecryptfs_mount_auth_tok_only"},
static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options)
			break;
	struct path path;
	rc = ecryptfs_parse_options(sbi, raw_data);
	}
	ecryptfs_set_superblock_lower(s, path.dentry->d_sb);
		unsigned long ptr_size;
		struct rlimit *rlim;
		rlim = current->signal->rlim;
		if (size > READ_ONCE(rlim[RLIMIT_STACK].rlim_cur) / 4)
			goto fail;
static long native_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
static int get_v4l2_window32(struct v4l2_window *kp, struct v4l2_window32 __user *up)
{
	u32 n;
	compat_caddr_t p;
	    copy_from_user(&kp->w, &up->w, sizeof(up->w)) ||
	    get_user(kp->field, &up->field) ||
	    get_user(kp->chromakey, &up->chromakey) ||
	    get_user(kp->clipcount, &up->clipcount) ||
	    get_user(kp->global_alpha, &up->global_alpha))
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
		return -EFAULT;
	if (kp->clipcount > 2048)
		return -EFAULT;
		return -EINVAL;
	if (!kp->clipcount) {
		kp->clips = NULL;
		return 0;
	}
		return -EINVAL;
	n = kp->clipcount;
	kclips = compat_alloc_user_space(n * sizeof(*kclips));
	kp->clips = kclips;
	while (n--) {
	uclips = compat_ptr(p);
		if (copy_in_user(&kclips->c, &uclips->c, sizeof(uclips->c)))
		if (put_user(n ? kclips + 1 : NULL, &kclips->next))
			return -EFAULT;
			return -EFAULT;
static int put_v4l2_window32(struct v4l2_window *kp, struct v4l2_window32 __user *up)
{
	u32 n = kp->clipcount;

	if (copy_to_user(&up->w, &kp->w, sizeof(kp->w)) ||
	    put_user(kp->field, &up->field) ||
	    put_user(kp->chromakey, &up->chromakey) ||
	    put_user(kp->clipcount, &up->clipcount) ||
	    put_user(kp->global_alpha, &up->global_alpha))
	compat_caddr_t p;
		return -EFAULT;

	if (!kp->clipcount)
		return -EFAULT;
		return 0;
	while (n--) {
	uclips = compat_ptr(p);
		if (copy_in_user(&uclips->c, &kclips->c, sizeof(uclips->c)))
static int __get_v4l2_format32(struct v4l2_format *kp, struct v4l2_format32 __user *up)
{
	if (get_user(kp->type, &up->type))
{
		return -EFAULT;
	switch (kp->type) {
	case V4L2_BUF_TYPE_VIDEO_CAPTURE:
		return copy_from_user(&kp->fmt.pix, &up->fmt.pix,
				      sizeof(kp->fmt.pix)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_VIDEO_OUTPUT:
	case V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:
		return copy_from_user(&kp->fmt.pix_mp, &up->fmt.pix_mp,
				      sizeof(kp->fmt.pix_mp)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:
	case V4L2_BUF_TYPE_VIDEO_OVERLAY:
		return get_v4l2_window32(&kp->fmt.win, &up->fmt.win);
	case V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:
	case V4L2_BUF_TYPE_VBI_CAPTURE:
		return copy_from_user(&kp->fmt.vbi, &up->fmt.vbi,
				      sizeof(kp->fmt.vbi)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_VBI_OUTPUT:
	case V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:
		return copy_from_user(&kp->fmt.sliced, &up->fmt.sliced,
				      sizeof(kp->fmt.sliced)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:
	case V4L2_BUF_TYPE_SDR_CAPTURE:
		return copy_from_user(&kp->fmt.sdr, &up->fmt.sdr,
				      sizeof(kp->fmt.sdr)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_SDR_OUTPUT:
	case V4L2_BUF_TYPE_META_CAPTURE:
		return copy_from_user(&kp->fmt.meta, &up->fmt.meta,
				      sizeof(kp->fmt.meta)) ? -EFAULT : 0;
	case V4L2_BUF_TYPE_META_CAPTURE:
	default:
static int get_v4l2_format32(struct v4l2_format *kp, struct v4l2_format32 __user *up)
{
	return __get_v4l2_format32(kp, up);
		return -EFAULT;
}
static int get_v4l2_create32(struct v4l2_create_buffers *kp, struct v4l2_create_buffers32 __user *up)
{
	    copy_from_user(kp, up, offsetof(struct v4l2_create_buffers32, format)))
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
		return -EFAULT;
	return __get_v4l2_format32(&kp->format, &up->format);
		return -EFAULT;
}
static int __put_v4l2_format32(struct v4l2_format *kp, struct v4l2_format32 __user *up)
{
	if (put_user(kp->type, &up->type))
{
		return -EFAULT;
	switch (kp->type) {
	case V4L2_BUF_TYPE_VIDEO_CAPTURE:
		return copy_to_user(&up->fmt.pix, &kp->fmt.pix,
	case V4L2_BUF_TYPE_VIDEO_OUTPUT:
				    sizeof(kp->fmt.pix)) ? -EFAULT : 0;
		return copy_to_user(&up->fmt.pix_mp, &kp->fmt.pix_mp,
	case V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:
				    sizeof(kp->fmt.pix_mp)) ? -EFAULT : 0;
		return copy_to_user(&up->fmt.vbi, &kp->fmt.vbi,
	case V4L2_BUF_TYPE_VBI_OUTPUT:
				    sizeof(kp->fmt.vbi)) ? -EFAULT : 0;
		return copy_to_user(&up->fmt.sliced, &kp->fmt.sliced,
	case V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:
				    sizeof(kp->fmt.sliced)) ? -EFAULT : 0;
		return copy_to_user(&up->fmt.sdr, &kp->fmt.sdr,
	case V4L2_BUF_TYPE_SDR_OUTPUT:
				    sizeof(kp->fmt.sdr)) ? -EFAULT : 0;
		return copy_to_user(&up->fmt.meta, &kp->fmt.meta,
	case V4L2_BUF_TYPE_META_CAPTURE:
				    sizeof(kp->fmt.meta)) ? -EFAULT : 0;
static int put_v4l2_format32(struct v4l2_format *kp, struct v4l2_format32 __user *up)
{
static int put_v4l2_create32(struct v4l2_create_buffers *kp, struct v4l2_create_buffers32 __user *up)
{
	    copy_to_user(up, kp, offsetof(struct v4l2_create_buffers32, format)) ||
	    copy_to_user(up->reserved, kp->reserved, sizeof(kp->reserved)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
static int get_v4l2_standard32(struct v4l2_standard *kp, struct v4l2_standard32 __user *up)
{
	    get_user(kp->index, &up->index))
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
		return -EFAULT;
static int put_v4l2_standard32(struct v4l2_standard *kp, struct v4l2_standard32 __user *up)
{
	    put_user(kp->index, &up->index) ||
	    put_user(kp->id, &up->id) ||
	    copy_to_user(up->name, kp->name, sizeof(up->name)) ||
	    copy_to_user(&up->frameperiod, &kp->frameperiod,
			 sizeof(kp->frameperiod)) ||
	    put_user(kp->framelines, &up->framelines) ||
	    copy_to_user(up->reserved, kp->reserved, sizeof(kp->reserved)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
static int get_v4l2_plane32(struct v4l2_plane __user *up, struct v4l2_plane32 __user *up32,
			    enum v4l2_memory memory)
	void __user *up_pln;
	compat_long_t p;
{
		if (get_user(p, &up32->m.userptr))
			return -EFAULT;
		up_pln = compat_ptr(p);
		if (put_user((unsigned long)up_pln, &up->m.userptr))
	case V4L2_MEMORY_USERPTR:
			return -EFAULT;
static int put_v4l2_plane32(struct v4l2_plane __user *up, struct v4l2_plane32 __user *up32,
			    enum v4l2_memory memory)
		if (copy_in_user(&up32->m.fd, &up->m.fd,
				 sizeof(up->m.fd)))
	case V4L2_MEMORY_DMABUF:
			return -EFAULT;
static int get_v4l2_buffer32(struct v4l2_buffer *kp, struct v4l2_buffer32 __user *up)
{
{
	struct v4l2_plane32 __user *uplane32;
	    get_user(kp->index, &up->index) ||
	    get_user(kp->type, &up->type) ||
	    get_user(kp->flags, &up->flags) ||
	    get_user(kp->memory, &up->memory) ||
	    get_user(kp->length, &up->length))
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
		return -EFAULT;
	if (V4L2_TYPE_IS_OUTPUT(kp->type))
		if (get_user(kp->bytesused, &up->bytesused) ||
		    get_user(kp->field, &up->field) ||
		    get_user(kp->timestamp.tv_sec, &up->timestamp.tv_sec) ||
		    get_user(kp->timestamp.tv_usec, &up->timestamp.tv_usec))
			return -EFAULT;
	if (V4L2_TYPE_IS_MULTIPLANAR(kp->type)) {
		unsigned int num_planes;
		if (kp->length == 0) {
			kp->m.planes = NULL;
			/* num_planes == 0 is legal, e.g. when userspace doesn't
			 * need planes array on DQBUF*/
			return 0;
		} else if (kp->length > VIDEO_MAX_PLANES) {
			return -EINVAL;
		}
		}
			       kp->length * sizeof(*uplane32)))
		if (!access_ok(VERIFY_READ, uplane32,
			return -EFAULT;
		/* We don't really care if userspace decides to kill itself
		 * by passing a very big num_planes value */
		uplane = compat_alloc_user_space(kp->length * sizeof(*uplane));
		kp->m.planes = (__force struct v4l2_plane *)uplane;
		for (num_planes = 0; num_planes < kp->length; num_planes++) {
			ret = get_v4l2_plane32(uplane, uplane32, kp->memory);
			if (ret)
			++uplane;
			++uplane32;
				return ret;
		}
		switch (kp->memory) {
	} else {
		case V4L2_MEMORY_MMAP:
			if (get_user(kp->m.offset, &up->m.offset))
		case V4L2_MEMORY_OVERLAY:
				return -EFAULT;
		case V4L2_MEMORY_USERPTR:
			{
				compat_long_t tmp;

				if (get_user(tmp, &up->m.userptr))
					return -EFAULT;
			break;
				kp->m.userptr = (unsigned long)compat_ptr(tmp);
			}
			break;
			break;
		case V4L2_MEMORY_DMABUF:
			if (get_user(kp->m.fd, &up->m.fd))
		case V4L2_MEMORY_DMABUF:
				return -EFAULT;
static int put_v4l2_buffer32(struct v4l2_buffer *kp, struct v4l2_buffer32 __user *up)
{
{
	struct v4l2_plane32 __user *uplane32;
	int num_planes;
	    put_user(kp->index, &up->index) ||
	    put_user(kp->type, &up->type) ||
	    put_user(kp->flags, &up->flags) ||
	    put_user(kp->memory, &up->memory))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
	if (put_user(kp->bytesused, &up->bytesused) ||
	    put_user(kp->field, &up->field) ||
	    put_user(kp->timestamp.tv_sec, &up->timestamp.tv_sec) ||
	    put_user(kp->timestamp.tv_usec, &up->timestamp.tv_usec) ||
	    copy_to_user(&up->timecode, &kp->timecode, sizeof(kp->timecode)) ||
	    put_user(kp->sequence, &up->sequence) ||
	    put_user(kp->reserved2, &up->reserved2) ||
	    put_user(kp->reserved, &up->reserved) ||
	    put_user(kp->length, &up->length))
		return -EFAULT;
	if (V4L2_TYPE_IS_MULTIPLANAR(kp->type)) {
		num_planes = kp->length;
		if (num_planes == 0)
		uplane = (__force struct v4l2_plane __user *)kp->m.planes;
		if (get_user(p, &up->m.planes))
		while (--num_planes >= 0) {
			ret = put_v4l2_plane32(uplane, uplane32, kp->memory);
			if (ret)
		switch (kp->memory) {
	} else {
		case V4L2_MEMORY_MMAP:
			if (put_user(kp->m.offset, &up->m.offset))
		case V4L2_MEMORY_OVERLAY:
				return -EFAULT;
			if (put_user(kp->m.userptr, &up->m.userptr))
		case V4L2_MEMORY_USERPTR:
				return -EFAULT;
			if (put_user(kp->m.fd, &up->m.fd))
		case V4L2_MEMORY_DMABUF:
				return -EFAULT;
static int get_v4l2_framebuffer32(struct v4l2_framebuffer *kp, struct v4l2_framebuffer32 __user *up)
{
	u32 tmp;
{
	    get_user(kp->capability, &up->capability) ||
	    get_user(kp->flags, &up->flags) ||
	    copy_from_user(&kp->fmt, &up->fmt, sizeof(up->fmt)))
	    get_user(tmp, &up->base) ||
		return -EFAULT;
	kp->base = (__force void *)compat_ptr(tmp);
static int put_v4l2_framebuffer32(struct v4l2_framebuffer *kp, struct v4l2_framebuffer32 __user *up)
{
	u32 tmp = (u32)((unsigned long)kp->base);
{
	    put_user(tmp, &up->base) ||
	    put_user(kp->capability, &up->capability) ||
	    put_user(kp->flags, &up->flags) ||
	    copy_to_user(&up->fmt, &kp->fmt, sizeof(up->fmt)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
/* The 64-bit v4l2_input struct has extra padding at the end of the struct.
   Otherwise it is identical to the 32-bit version. */
static inline int get_v4l2_input32(struct v4l2_input *kp, struct v4l2_input32 __user *up)
{
	if (copy_from_user(kp, up, sizeof(*up)))
{
		return -EFAULT;
static inline int put_v4l2_input32(struct v4l2_input *kp, struct v4l2_input32 __user *up)
{
	if (copy_to_user(up, kp, sizeof(*up)))
{
		return -EFAULT;
static int get_v4l2_ext_controls32(struct file *file,
				   struct v4l2_ext_controls *kp,
				   struct v4l2_ext_controls32 __user *up)
static int get_v4l2_ext_controls32(struct file *file,
{
	unsigned int n;
	struct v4l2_ext_control __user *kcontrols;
	compat_caddr_t p;
	    get_user(kp->which, &up->which) ||
	    get_user(kp->count, &up->count) ||
	    get_user(kp->error_idx, &up->error_idx) ||
	    copy_from_user(kp->reserved, up->reserved, sizeof(kp->reserved)))
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
		return -EFAULT;
	if (kp->count == 0) {
		kp->controls = NULL;
		return 0;
	} else if (kp->count > V4L2_CID_MAX_CTRLS) {
		return -EFAULT;
		return -EINVAL;
	}
	if (!access_ok(VERIFY_READ, ucontrols, kp->count * sizeof(*ucontrols)))
	ucontrols = compat_ptr(p);
		return -EFAULT;
	kcontrols = compat_alloc_user_space(kp->count * sizeof(*kcontrols));
	kp->controls = (__force struct v4l2_ext_control *)kcontrols;
	for (n = 0; n < kp->count; n++) {
		return -EFAULT;
		u32 id;
			return -EFAULT;
		if (get_user(id, &kcontrols->id))
			return -EFAULT;
		if (ctrl_is_pointer(file, id)) {
				   struct v4l2_ext_controls *kp,
static int put_v4l2_ext_controls32(struct file *file,
				   struct v4l2_ext_controls32 __user *up)
	struct v4l2_ext_control __user *kcontrols =
		(__force struct v4l2_ext_control __user *)kp->controls;
	int n = kp->count;
	compat_caddr_t p;
	    put_user(kp->which, &up->which) ||
	    put_user(kp->count, &up->count) ||
	    put_user(kp->error_idx, &up->error_idx) ||
	    copy_to_user(up->reserved, kp->reserved, sizeof(up->reserved)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
	if (!kp->count)
		return 0;
	if (get_user(p, &up->controls))
	if (!access_ok(VERIFY_WRITE, ucontrols, n * sizeof(*ucontrols)))
	ucontrols = compat_ptr(p);
		return -EFAULT;
	while (--n >= 0) {
		u32 id;
		if (get_user(id, &kcontrols->id))
			return -EFAULT;
			return -EFAULT;
		if (ctrl_is_pointer(file, id))
			return -EFAULT;
		ucontrols++;
static int put_v4l2_event32(struct v4l2_event *kp, struct v4l2_event32 __user *up)
{
	    put_user(kp->type, &up->type) ||
	    copy_to_user(&up->u, &kp->u, sizeof(kp->u)) ||
	    put_user(kp->pending, &up->pending) ||
	    put_user(kp->sequence, &up->sequence) ||
	    put_user(kp->timestamp.tv_sec, &up->timestamp.tv_sec) ||
	    put_user(kp->timestamp.tv_nsec, &up->timestamp.tv_nsec) ||
	    put_user(kp->id, &up->id) ||
	    copy_to_user(up->reserved, kp->reserved, sizeof(kp->reserved)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
static int get_v4l2_edid32(struct v4l2_edid *kp, struct v4l2_edid32 __user *up)
{
	u32 tmp;
{
	    get_user(kp->pad, &up->pad) ||
	    get_user(kp->start_block, &up->start_block) ||
	    get_user(kp->blocks, &up->blocks) ||
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
	    get_user(tmp, &up->edid) ||
	    copy_from_user(kp->reserved, up->reserved, sizeof(kp->reserved)))
	    get_user(tmp, &up->edid) ||
		return -EFAULT;
	kp->edid = (__force u8 *)compat_ptr(tmp);
static int put_v4l2_edid32(struct v4l2_edid *kp, struct v4l2_edid32 __user *up)
{
	u32 tmp = (u32)((unsigned long)kp->edid);
{
	    put_user(kp->pad, &up->pad) ||
	    put_user(kp->start_block, &up->start_block) ||
	    put_user(kp->blocks, &up->blocks) ||
	    put_user(tmp, &up->edid) ||
	    copy_to_user(up->reserved, kp->reserved, sizeof(up->reserved)))
	if (!access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		return -EFAULT;
static long do_video_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
	union {
		struct v4l2_format v2f;
		struct v4l2_buffer v2b;
		struct v4l2_framebuffer v2fb;
		struct v4l2_input v2i;
		struct v4l2_standard v2s;
		struct v4l2_ext_controls v2ecs;
		struct v4l2_event v2ev;
		struct v4l2_create_buffers v2crt;
		struct v4l2_edid v2edid;
		unsigned long vx;
		int vi;
	} karg;
	void __user *up = compat_ptr(arg);
	int compatible_arg = 1;
		err = get_user(karg.vi, (s32 __user *)up);
	case VIDIOC_S_OUTPUT:
		compatible_arg = 0;
	case VIDIOC_G_OUTPUT:
		compatible_arg = 0;
		err = get_v4l2_edid32(&karg.v2edid, up);
	case VIDIOC_S_EDID:
		compatible_arg = 0;
		err = get_v4l2_format32(&karg.v2f, up);
	case VIDIOC_TRY_FMT:
		compatible_arg = 0;
		err = get_v4l2_create32(&karg.v2crt, up);
	case VIDIOC_CREATE_BUFS:
		compatible_arg = 0;
		err = get_v4l2_buffer32(&karg.v2b, up);
	case VIDIOC_DQBUF:
		compatible_arg = 0;
		err = get_v4l2_framebuffer32(&karg.v2fb, up);
	case VIDIOC_S_FBUF:
		compatible_arg = 0;
	case VIDIOC_G_FBUF:
		compatible_arg = 0;
		err = get_v4l2_standard32(&karg.v2s, up);
	case VIDIOC_ENUMSTD:
		compatible_arg = 0;
		err = get_v4l2_input32(&karg.v2i, up);
	case VIDIOC_ENUMINPUT:
		compatible_arg = 0;
		err = get_v4l2_ext_controls32(file, &karg.v2ecs, up);
	case VIDIOC_TRY_EXT_CTRLS:
		compatible_arg = 0;
	case VIDIOC_DQEVENT:
		compatible_arg = 0;
	else {
		mm_segment_t old_fs = get_fs();

		set_fs(KERNEL_DS);
		err = native_ioctl(file, cmd, (unsigned long)&karg);
		set_fs(old_fs);
	}
		err = native_ioctl(file, cmd, (unsigned long)up);
	/* Special case: even after an error we need to put the
	   results back for these ioctls since the error_idx will
	   contain information on which control failed. */
	switch (cmd) {
		if (put_v4l2_ext_controls32(file, &karg.v2ecs, up))
	case VIDIOC_TRY_EXT_CTRLS:
			err = -EFAULT;
		if (put_v4l2_edid32(&karg.v2edid, up))
	case VIDIOC_S_EDID:
			err = -EFAULT;
		err = put_user(((s32)karg.vi), (s32 __user *)up);
	case VIDIOC_G_OUTPUT:
		break;
		err = put_v4l2_framebuffer32(&karg.v2fb, up);
	case VIDIOC_G_FBUF:
		break;
		err = put_v4l2_event32(&karg.v2ev, up);
	case VIDIOC_DQEVENT:
		break;
		err = put_v4l2_edid32(&karg.v2edid, up);
	case VIDIOC_G_EDID:
		break;
		err = put_v4l2_format32(&karg.v2f, up);
	case VIDIOC_TRY_FMT:
		break;
		err = put_v4l2_create32(&karg.v2crt, up);
	case VIDIOC_CREATE_BUFS:
		break;
		err = put_v4l2_buffer32(&karg.v2b, up);
	case VIDIOC_DQBUF:
		break;
		err = put_v4l2_standard32(&karg.v2s, up);
	case VIDIOC_ENUMSTD:
		break;
		err = put_v4l2_input32(&karg.v2i, up);
	case VIDIOC_ENUMINPUT:
		break;
		if (!mp->ports && !mp->mglist &&
		    netif_running(br->dev))
		if (!mp->ports && !mp->mglist &&
		    netif_running(br->dev))
static void create_pit_timer(struct kvm_kpit_state *ps, u32 val, int is_period)
	struct kvm_timer *pt = &ps->pit_timer;
			create_pit_timer(ps, val, 0);
		if (!(ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)) {
		}
			create_pit_timer(ps, val, 1);
		if (!(ps->flags & KVM_PIT_FLAGS_HPET_LEGACY)){
		}
	struct cm_id_private *cm_id_priv;

	struct list_head auto_asconf_splist;
	spinlock_t addr_wq_lock;
	struct sk_buff_head pd_lobby;
	local_bh_disable();
	bh_lock_sock(sk);
	local_bh_enable();
	bh_unlock_sock(sk);
	if (val == 0 && sp->do_auto_asconf) {
	}
	return 0;
	sock_prot_inuse_add(net, sk->sk_prot, 1);
	if (net->sctp.default_auto_asconf) {
	if (net->sctp.default_auto_asconf) {
	local_bh_enable();
/* Cleanup any SCTP per socket resources.  */
static void sctp_destroy_sock(struct sock *sk)
		inet_sk_copy_descendant(newsk, oldsk);
		inet_sk_copy_descendant(newsk, oldsk);
	flush_spe_to_thread(src);
	char buffer[sizeof("4294967296 65635")];
	u_int16_t port;
	exp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;
		nf_ct_helper_log(skb, exp->master, "all ports in use");
	if (port == 0) {
		return NF_DROP;
	ret = nf_nat_mangle_tcp_packet(skb, exp->master, ctinfo,
				       protoff, matchoff, matchlen, buffer,
				       strlen(buffer));
	if (ret != NF_ACCEPT) {
		nf_ct_helper_log(skb, exp->master, "cannot mangle packet");
	if (ret != NF_ACCEPT) {
		nf_ct_unexpect_related(exp);
	}
	return ret;
				    midi->out_ep->name, err);
			return err;
{
	kfree(req->buf);
	kfree(req->buf);
	usb_ep_free_request(ep, req);
	if (vmx->nested.vmxon) {
{
			return 1;
		kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,
		return 1;
	if (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,
void unix_inflight(struct file *fp);
void unix_notinflight(struct file *fp);
void unix_gc(void);
	short			max;
	struct file		*fp[SCM_MAX_FD];
		fpl->max = SCM_MAX_FD;
	}
	}
	return num;
			fput(fpl->fp[i]);
		kfree(fpl);
		new_fpl->max = new_fpl->count;
	}
		unix_notinflight(scm->fp->fp[i]);
	for (i = scm->fp->count-1; i >= 0; i--)
}
		unix_inflight(scm->fp->fp[i]);
	for (i = scm->fp->count - 1; i >= 0; i--)
	return max_level;
void unix_inflight(struct file *fp)
{
	}
	spin_unlock(&unix_gc_lock);
void unix_notinflight(struct file *fp)
{
	}
	spin_unlock(&unix_gc_lock);
	lock_sock(sk);
	if (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))
	err = -EINVAL;
	if (sk->sk_state != TCP_CLOSE)
		sk->sk_send_head = NULL;
}
	if (len <= 0x7f) {
	if (move_group) {
		if (gctx->task == TASK_TOMBSTONE) {
		}
	} else {
		mutex_unlock(&gctx->mutex);
	if (move_group)
	mutex_unlock(&ctx->mutex);
		mutex_unlock(&gctx->mutex);
	if (move_group)
	mutex_unlock(&ctx->mutex);
int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
			       "iommu failed to map pfn=%llx\n", pfn);
			goto unmap_pages;
unmap_pages:
	return r;
	[NL80211_ATTR_P2P_OPPPS] = { .type = NLA_U8 },
	[NL80211_ATTR_ACL_POLICY] = {. type = NLA_U32 },
	list_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {
		if (ack->subh.addip_hdr->serial == serial) {
	UMOUNT_PROPAGATE = 2,
};
	}
	if (iter < CIPSO_V4_TAG_MAXCNT)
		doi_def->tags[iter] = CIPSO_V4_TAG_INVALID;
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
int xt_compat_check_entry_offsets(const void *base,
			     void __user **dstptr, unsigned int *size);
				  unsigned int target_offset,
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e, e->target_offset,
					    e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);
	if (err)
	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
/* see xt_check_entry_offsets */
int xt_compat_check_entry_offsets(const void *base,
				  unsigned int target_offset,
{
	const struct compat_xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
int xt_check_entry_offsets(const void *base,
			   unsigned int target_offset,
{
	const struct xt_entry_target *t;
	if (target_offset + sizeof(*t) > next_offset)
	if (io_data->ffs->ffs_eventfd &&
	    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))
		eventfd_signal(io_data->ffs->ffs_eventfd, 1);
			return -ENOMEM;
				    unsigned int txqs, unsigned int rxqs);
#define alloc_netdev(sizeof_priv, name, name_assign_type, setup) \
static int dev_get_valid_name(struct net *net,
			      struct net_device *dev,
			      const char *name)
{
}
			sk_wait_data(sk, &timeo, NULL);
	u8 last_lock;
	struct i2c_client *i2c_client_demod;
	u8 obuf[0x40], ibuf[0x40];
	struct dvb_usb_device *d = i2c_get_adapdata(adap);
		return -ENODEV;
	if (mutex_lock_interruptible(&d->i2c_mutex) < 0)
		return -EAGAIN;
			obuf[0] = msg[0].buf[0] + 0x36;
			obuf[1] = 3;
			obuf[2] = 0;
			if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 0, 0) < 0)
		case SU3000_STREAM_CTRL:
				err("i2c transfer failed.");
			obuf[0] = 0x10;
			if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 2, 0) < 0)
		case DW2102_RC_QUERY:
				err("i2c transfer failed.");
			msg[0].buf[1] = ibuf[0];
			msg[0].buf[0] = ibuf[1];
				err("i2c transfer failed.");
			break;
			obuf[0] = 0x08;
			obuf[1] = msg[0].addr;
			obuf[2] = msg[0].len;
			memcpy(&obuf[3], msg[0].buf, msg[0].len);
			if (dvb_usb_generic_rw(d, obuf, msg[0].len + 3,
						ibuf, 1, 0) < 0)
				err("i2c transfer failed.");
		obuf[0] = 0x09;
		obuf[1] = msg[0].len;
		obuf[2] = msg[1].len;
		obuf[3] = msg[0].addr;
		memcpy(&obuf[4], msg[0].buf, msg[0].len);

		if (dvb_usb_generic_rw(d, obuf, msg[0].len + 4,
					ibuf, msg[1].len + 1, 0) < 0)
			err("i2c transfer failed.");
		memcpy(msg[1].buf, &ibuf[1], msg[1].len);
		break;
	}
	mutex_unlock(&d->i2c_mutex);
	u8 obuf[] = {0xde, 0};
	struct dw2102_state *state = (struct dw2102_state *)d->priv;
	if (i && !state->initialized) {
		state->initialized = 1;
		return dvb_usb_generic_rw(d, obuf, 2, NULL, 0, 0);
	}
	return 0;
}
static int su3000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x02;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(ds3000_attach, &su3000_ds3000_config,
					&d->dev->i2c_adap);
	if (d->fe_adap[0].fe == NULL)
		return -EIO;
	if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
				&dw2104_ts2020_config,
		info("Attached DS3000/TS2020!");
static int t220_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x87, 0 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x86;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x80;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x80;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(cxd2820r_attach, &cxd2820r_config,
					&d->dev->i2c_adap, NULL);
	if (d->fe_adap[0].fe != NULL) {
		if (dvb_attach(tda18271_attach, d->fe_adap[0].fe, 0x60,
					&d->dev->i2c_adap, &tda18271_config)) {
			info("Attached TDA18271HD/CXD2820R!");
static int m88rs2000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[] = { 0x51 };
	u8 ibuf[] = { 0 };
{
	if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	d->fe_adap[0].fe = dvb_attach(m88rs2000_attach, &s421_m88rs2000_config,
					&d->dev->i2c_adap);
	if (d->fe_adap[0].fe == NULL)
		return -EIO;
	if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
				&dw2104_ts2020_config,
		info("Attached RS2000/TS2020!");
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x02;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 0;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0xe;
	obuf[1] = 0x83;
	obuf[2] = 1;
	if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
		err("command 0x0e transfer failed.");
	obuf[0] = 0x51;
	if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 1, 0) < 0)
		err("command 0x51 transfer failed.");
	 !((__vma)->vm_flags & VM_NOHUGEPAGE))
	   ((__vma)->vm_flags & VM_HUGEPAGE))) &&			\
		goto out;
	VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
		    (vma->vm_flags & VM_NOHUGEPAGE)) {
			progress++;

		if (!vma->anon_vma || vma->vm_ops || vma->vm_file) {
			khugepaged_scan.address = vma->vm_end;
			progress++;
			continue;
		}
		VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));
		if (hstart >= hend) {
			progress++;
			continue;
		}
		hend = vma->vm_end & HPAGE_PMD_MASK;
		if (khugepaged_scan.address < hstart)
		if (khugepaged_scan.address > hend) {
			khugepaged_scan.address = hend + HPAGE_PMD_SIZE;
			progress++;
			continue;
		}
		BUG_ON(khugepaged_scan.address & ~HPAGE_PMD_MASK);
			khugepaged_scan.address = hstart;
	BUG_ON(khugepaged_scan.mm_slot != mm_slot);
	spin_lock(&khugepaged_mm_lock);
		BUG_ON(khugepaged_thread != current);
		mutex_unlock(&khugepaged_mutex);
		khugepaged_loop();
		BUG_ON(khugepaged_thread != current);
		khugepaged_loop();
	else if (vcpu->arch.apic_base & X2APIC_ENABLE) {
		msr_bitmap = vmx_msr_bitmap_nested;
		if (is_long_mode(vcpu))
	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
}
	if (enable_apicv) {
		for (msr = 0x800; msr <= 0x8ff; msr++)
			vmx_disable_intercept_msr_read_x2apic(msr);

		/* According SDM, in x2apic mode, the whole id reg is used.
		 * But in KVM, it only use the highest eight bits. Need to
		 * intercept it */
		vmx_enable_intercept_msr_read_x2apic(0x802);
		/* TMCCT */
		vmx_enable_intercept_msr_read_x2apic(0x839);
		/* TPR */
		vmx_disable_intercept_msr_write_x2apic(0x808);
		/* EOI */
		vmx_disable_intercept_msr_write_x2apic(0x80b);
		/* SELF-IPI */
		vmx_disable_intercept_msr_write_x2apic(0x83f);
	}
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		break;
	if (secs_per_zone > total_sections) {
		f2fs_msg(sb, KERN_INFO,
			"Wrong secs_per_zone (%u > %u)",
		f2fs_msg(sb, KERN_INFO,
			secs_per_zone, total_sections);
		skcipher_request_set_crypt(&ctx->req, sg, ctx->rsgl.sg, used,
	retval = -ENOEXEC;
		goto _ret;


		return -ENOEXEC;
	retval = security_bprm_check(bprm);
			read_unlock(&binfmt_lock);
			retval = fn(bprm);
			/*
			 * Restore the depth counter to its starting value
			 * in this call, so we don't have to rely on every
			 * load_binary function to restore it on return.
			 */

		    (int)(req->tp_block_size -
			  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)
		if (po->tp_version >= TPACKET_V3 &&
			goto out;
	struct rb_node *node, *prev;
	NET_INC_STATS(sock_net(sk), LINUX_MIB_OFOPRUNED);
	node = &tp->ooo_last_skb->rbnode;
		rb_erase(node, &tp->out_of_order_queue);
		tcp_drop(sk, rb_to_skb(node));
		tcp_drop(sk, rb_to_skb(node));
		node = prev;
fail:
		__bdevname(dev, b), PTR_ERR(bdev));
	if (*options && *options != ',') {
		       (char *) *data);
#define PAGE_EXECONLY		__pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)
#define __P100  PAGE_EXECONLY
#define __P011  PAGE_COPY
#define __P101  PAGE_READONLY_EXEC
#define __S100  PAGE_EXECONLY
#define __S011  PAGE_SHARED
#define __S101  PAGE_READONLY_EXEC
{
		if (!pte_special(pte) && pte_exec(pte))
	int fault, sig, code;
	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
static int __net_init sctp_net_init(struct net *net)
	/* Initialize the control inode/socket for handling OOTB packets.  */
	if ((status = sctp_ctl_sock_init(net))) {
		pr_err("Failed to initialize the SCTP control sock\n");
		goto err_ctl_sock_init;
	}

err_ctl_sock_init:
	sctp_dbg_objcnt_exit(net);
	sctp_proc_exit(net);
static void __net_exit sctp_net_exit(struct net *net)
	/* Free the control endpoint.  */
	inet_ctl_sock_destroy(net->sctp.ctl_sock);

	status = sctp_v4_protosw_init();
	if (status)
err_add_protocol:
	sctp_v6_protosw_exit();
err_protosw_init:
	sctp_v4_pf_exit();
		usb_autopm_put_interface_async(to_usb_interface(hub->intfdev));
}
	u32 now;
	struct tcp_sock *tp = tcp_sk(sk);
	/* Then check the check host-wide RFC 5961 rate limit. */
	now = jiffies / HZ;
		challenge_count = 0;
	}
	}
		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);
			int data_len = elt->length -
					sizeof(struct oz_get_desc_rsp) + 1;
			u16 offs = le16_to_cpu(get_unaligned(&body->offset));
			u16 total_size =
				(struct oz_get_desc_rsp *)usb_hdr;
				le16_to_cpu(get_unaligned(&body->total_size));
			break;
			if (olen == sizeof(efs))
		case L2CAP_CONF_EFS:
			    efs.stype != chan->local_stype)
				return -ECONNREFUSED;
			l2cap_add_conf_opt(&ptr, L2CAP_CONF_EFS, sizeof(efs),
					   (unsigned long) &efs, endptr - ptr);
			break;
static void print_bpf_insn(struct bpf_insn *insn)
{
		} else if (BPF_MODE(insn->code) == BPF_IMM) {
			verbose("(%02x) r%d = 0x%x\n",
				insn->code, insn->dst_reg, insn->imm);
				insn->src_reg, insn->imm);
		} else {
			print_bpf_insn(insn);
			verbose("%d: ", insn_idx);
		}
	struct nlmsghdr *nlh;
	int len, err = -ENOBUFS;
	mutex_unlock(nlk->cb_mutex);
	module_put(cb->module);
	mutex_unlock(nlk->cb_mutex);
	return 0;
	return sprintf(buf, "%s\n", dev->driver_override);
	char *driver_override, *old = dev->driver_override, *cp;
	char *line, *p;
{
	int i;
	ssize_t ret = -EFAULT;
	int i;
	size_t len = iov_length(iv, count);
	size_t len = iov_length(iv, count);
	line = kmalloc(len + 1, GFP_KERNEL);
	if (line == NULL)
		return -ENOMEM;
	/*
	 * copy all vectors into a single string, to ensure we do
	 * not interleave our log line with other printk calls
	 */
	p = line;
	for (i = 0; i < count; i++) {
		if (copy_from_user(p, iv[i].iov_base, iv[i].iov_len))
	for (i = 0; i < count; i++) {
			goto out;
		p += iv[i].iov_len;
			goto out;
	}
	p[0] = '\0';
	}
	ret = printk("%s", line);
	/* printk can add a prefix */
	if (ret > len)
		ret = len;
out:
	kfree(line);
out:
	return ret;
#ifdef CONFIG_PRINTK
asmlinkage __printf(1, 0)
int vprintk(const char *fmt, va_list args);
asmlinkage __printf(1, 2) __cold
#define __LOG_BUF_LEN	(1 << CONFIG_LOG_BUF_SHIFT)

 * logbuf_lock protects log_buf, log_start, log_end, con_start and logged_chars
 * It is also used in interesting ways to provide interlocking in
 * console_unlock();.
 */
static DEFINE_RAW_SPINLOCK(logbuf_lock);

#define LOG_BUF_MASK (log_buf_len-1)
#define LOG_BUF(idx) (log_buf[(idx) & LOG_BUF_MASK])

/*
 * The indices into log_buf are not constrained to log_buf_len - they
 * must be masked before subscripting
 */
static unsigned log_start;	/* Index into log_buf: next char to be read by syslog() */
static unsigned con_start;	/* Index into log_buf: next char to be sent to consoles */
static unsigned log_end;	/* Index into log_buf: most-recently-written-char + 1 */

/*
#ifdef CONFIG_PRINTK
static char __log_buf[__LOG_BUF_LEN];
static int log_buf_len = __LOG_BUF_LEN;
static unsigned logged_chars; /* Number of chars produced since last read+clear operation */
static int saved_console_loglevel = -1;
static char *log_buf = __log_buf;
	VMCOREINFO_SYMBOL(log_end);
	VMCOREINFO_SYMBOL(logged_chars);
	VMCOREINFO_SYMBOL(log_buf_len);
}
	unsigned start, dest_idx, offset;

	offset = start = min(con_start, log_start);
	dest_idx = 0;
	while (start != log_end) {
		start++;
		dest_idx++;
	}
	log_start -= offset;
	con_start -= offset;
	log_end -= offset;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
int do_syslog(int type, char __user *buf, int len, bool from_file)
	unsigned i, j, limit, count;
	int do_clear = 0;
	char c;
{
	int error;
							(log_start - log_end));
		error = wait_event_interruptible(log_wait,
		if (error)
		i = 0;
		raw_spin_lock_irq(&logbuf_lock);
		while (!error && (log_start != log_end) && i < len) {
			c = LOG_BUF(log_start);
			log_start++;
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,buf);
			buf++;
			i++;
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (!error)
			error = i;
			goto out;
		break;
		do_clear = 1;
	case SYSLOG_ACTION_READ_CLEAR:
		count = len;
		if (count > log_buf_len)
			count = log_buf_len;
		raw_spin_lock_irq(&logbuf_lock);
		if (count > logged_chars)
			count = logged_chars;
		if (do_clear)
			logged_chars = 0;
		limit = log_end;
		/*
		 * __put_user() could sleep, and while we sleep
		 * printk() could overwrite the messages
		 * we try to copy to user space. Therefore
		 * the messages are copied in reverse. <manfreds>
		 */
		for (i = 0; i < count && !error; i++) {
			j = limit-1-i;
			if (j + log_buf_len < log_end)
				break;
			c = LOG_BUF(j);
			raw_spin_unlock_irq(&logbuf_lock);
			error = __put_user(c,&buf[count-1-i]);
			cond_resched();
			raw_spin_lock_irq(&logbuf_lock);
		}
		raw_spin_unlock_irq(&logbuf_lock);
		if (error)
			break;
		error = i;
		if (i != count) {
			int offset = count-error;
			/* buffer overflow during copy, correct user buffer. */
			for (i = 0; i < error; i++) {
				if (__get_user(c,&buf[i+offset]) ||
				    __put_user(c,&buf[i])) {
					error = -EFAULT;
					break;
				}
				cond_resched();
			}
		}
		}
		break;
		logged_chars = 0;
		break;
	case SYSLOG_ACTION_CLEAR:
		error = log_end - log_start;
	case SYSLOG_ACTION_SIZE_UNREAD:
		break;
	syslog_data[2] = log_buf + log_end -
		(logged_chars < log_buf_len ? logged_chars : log_buf_len);
	syslog_data[3] = log_buf + log_end;
	syslog_data[1] = log_buf + log_buf_len;
}
/*
 * Call the console drivers on a range of log_buf
 */
static void __call_console_drivers(unsigned start, unsigned end)
{
	struct console *con;

	for_each_console(con) {
		if (exclusive_console && con != exclusive_console)
			continue;
		if ((con->flags & CON_ENABLED) && con->write &&
				(cpu_online(smp_processor_id()) ||
				(con->flags & CON_ANYTIME)))
			con->write(con, &LOG_BUF(start), end - start);
	}
}

 * Write out chars from start to end - 1 inclusive
 */
static void _call_console_drivers(unsigned start,
				unsigned end, int msg_log_level)
{
	trace_console(&LOG_BUF(0), start, end, log_buf_len);

	if ((msg_log_level < console_loglevel || ignore_loglevel) &&
			console_drivers && start != end) {
		if ((start & LOG_BUF_MASK) > (end & LOG_BUF_MASK)) {
			/* wrapped write */
			__call_console_drivers(start & LOG_BUF_MASK,
						log_buf_len);
			__call_console_drivers(0, end & LOG_BUF_MASK);
		} else {
			__call_console_drivers(start, end);
		}
	}
}

/*
 * Parse the syslog header <[0-9]*>. The decimal value represents 32bit, the
 * lower 3 bit are the log level, the rest are the log facility. In case
 * userspace passes usual userspace syslog messages to /dev/kmsg or
 * /dev/ttyprintk, the log prefix might contain the facility. Printk needs
 * to extract the correct log level for in-kernel processing, and not mangle
 * the original value.
 *
 * If a prefix is found, the length of the prefix is returned. If 'level' is
 * passed, it will be filled in with the log level without a possible facility
 * value. If 'special' is passed, the special printk prefix chars are accepted
 * and returned. If no valid header is found, 0 is returned and the passed
 * variables are not touched.
 */
static size_t log_prefix(const char *p, unsigned int *level, char *special)
{
	unsigned int lev = 0;
	char sp = '\0';
	size_t len;

	if (p[0] != '<' || !p[1])
		return 0;
	if (p[2] == '>') {
		/* usual single digit level number or special char */
		switch (p[1]) {
		case '0' ... '7':
			lev = p[1] - '0';
			break;
		case 'c': /* KERN_CONT */
		case 'd': /* KERN_DEFAULT */
			sp = p[1];
			break;
		default:
			return 0;
		}
		len = 3;
	} else {
		/* multi digit including the level and facility number */
		char *endp = NULL;

		lev = (simple_strtoul(&p[1], &endp, 10) & 7);
		if (endp == NULL || endp[0] != '>')
			return 0;
		len = (endp + 1) - p;
	}

	/* do not accept special char if not asked for */
	if (sp && !special)
		return 0;

	if (special) {
		*special = sp;
		/* return special char, do not touch level */
		if (sp)
			return len;
	}

	if (level)
		*level = lev;
	return len;
}

/*
static void call_console_drivers(unsigned start, unsigned end)
{
	unsigned cur_index, start_print;
	static int msg_level = -1;

	BUG_ON(((int)(start - end)) > 0);

	cur_index = start;
	start_print = start;
	while (cur_index != end) {
		if (msg_level < 0 && ((end - cur_index) > 2)) {
			/* strip log prefix */
			cur_index += log_prefix(&LOG_BUF(cur_index), &msg_level, NULL);
			start_print = cur_index;
		}
		while (cur_index != end) {
			char c = LOG_BUF(cur_index);

			cur_index++;
			if (c == '\n') {
				if (msg_level < 0) {
					/*
					 * printk() has already given us loglevel tags in
					 * the buffer.  This code is here in case the
					 * log buffer has wrapped right round and scribbled
					 * on those tags
					 */
					msg_level = default_message_loglevel;
				}
				_call_console_drivers(start_print, cur_index, msg_level);
				msg_level = -1;
				start_print = cur_index;
				break;
			}
		}
	}
	_call_console_drivers(start_print, end, msg_level);
}
{
static void emit_log_char(char c)
{
	LOG_BUF(log_end) = c;
	log_end++;
	if (log_end - log_start > log_buf_len)
		log_start = log_end - log_buf_len;
	if (log_end - con_start > log_buf_len)
		con_start = log_end - log_buf_len;
	if (logged_chars < log_buf_len)
		logged_chars++;
}
#if defined(CONFIG_PRINTK_TIME)
static bool printk_time = 1;
#else
static bool printk_time = 0;
#endif
module_param_named(time, printk_time, bool, S_IRUGO | S_IWUSR);

static bool always_kmsg_dump;
module_param_named(always_kmsg_dump, always_kmsg_dump, bool, S_IRUGO | S_IWUSR);

/**
 * printk - print a kernel message
 * @fmt: format string
 *
 * This is printk().  It can be called from any context.  We want it to work.
 *
 * We try to grab the console_lock.  If we succeed, it's easy - we log the output and
 * call the console drivers.  If we fail to get the semaphore we place the output
 * into the log buffer and return.  The current holder of the console_sem will
 * notice the new output in console_unlock(); and will send it to the
 * consoles before releasing the lock.
 *
 * One effect of this deferred printing is that code which calls printk() and
 * then changes console_loglevel may break. This is because console_loglevel
 * is inspected when the actual printing occurs.
 *
 * See also:
 * printf(3)
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */

asmlinkage int printk(const char *fmt, ...)
{
	va_list args;
	int r;

#ifdef CONFIG_KGDB_KDB
	if (unlikely(kdb_trap_printk)) {
		va_start(args, fmt);
		r = vkdb_printf(fmt, args);
		va_end(args);
		return r;
	}
#endif
	va_start(args, fmt);
	r = vprintk(fmt, args);
	va_end(args);

	return r;
}

/* cpu currently holding logbuf_lock */
static volatile unsigned int printk_cpu = UINT_MAX;

	printk_cpu = UINT_MAX;
	}
	if (wake)
static const char recursion_bug_msg [] =
		KERN_CRIT "BUG: recent printk recursion!\n";
static int recursion_bug;
static int new_text_line = 1;
static char printk_buf[1024];
asmlinkage int vprintk(const char *fmt, va_list args)
{
	int printed_len = 0;
	int current_log_level = default_message_loglevel;
{
	unsigned long flags;
	char *p;
	size_t plen;
	char special;
	int this_cpu;
	if (unlikely(printk_cpu == this_cpu)) {
	printk_cpu = this_cpu;
	raw_spin_lock(&logbuf_lock);
	if (recursion_bug) {
		recursion_bug = 0;
		strcpy(printk_buf, recursion_bug_msg);
		printed_len = strlen(recursion_bug_msg);
		recursion_bug = 0;
	}
	/* Emit the output into the temporary buffer */
	printed_len += vscnprintf(printk_buf + printed_len,
				  sizeof(printk_buf) - printed_len, fmt, args);
	p = printk_buf;
	/* Read log level and handle special printk prefix */
	plen = log_prefix(p, &current_log_level, &special);
	if (plen) {
		p += plen;
		switch (special) {
		case 'c': /* Strip <c> KERN_CONT, continue line */
			plen = 0;
			break;
		case 'd': /* Strip <d> KERN_DEFAULT, start new line */
			plen = 0;
		default:
			if (!new_text_line) {
				emit_log_char('\n');
				new_text_line = 1;
			}
	/*
	 * Copy the output into log_buf. If the caller didn't provide
	 * the appropriate log prefix, we insert them here
	 */
	for (; *p; p++) {
		if (new_text_line) {
			new_text_line = 0;

			if (plen) {
				/* Copy original log prefix */
				int i;

				for (i = 0; i < plen; i++)
					emit_log_char(printk_buf[i]);
				printed_len += plen;
			} else {
				/* Add log prefix */
				emit_log_char('<');
				emit_log_char(current_log_level + '0');
				emit_log_char('>');
				printed_len += 3;
			}
			if (printk_time) {
				/* Add the current time stamp */
				char tbuf[50], *tp;
				unsigned tlen;
				unsigned long long t;
				unsigned long nanosec_rem;

				t = cpu_clock(printk_cpu);
				nanosec_rem = do_div(t, 1000000000);
				tlen = sprintf(tbuf, "[%5lu.%06lu] ",
						(unsigned long) t,
						nanosec_rem / 1000);

				for (tp = tbuf; tp < tbuf + tlen; tp++)
					emit_log_char(*tp);
				printed_len += tlen;
			}
			if (!*p)
				break;
		}
		emit_log_char(*p);
		if (*p == '\n')
			new_text_line = 1;
	}
	 * Try to acquire and then immediately release the
	 * console semaphore. The release will do all the
	 * actual magic (print out buffers, wake up klogd,
	 * etc).
	 * The console_trylock_for_printk() function
	 * will release 'logbuf_lock' regardless of whether it
	 * actually gets the semaphore or not.
EXPORT_SYMBOL(printk);
}
EXPORT_SYMBOL(vprintk);
#else
static void call_console_drivers(unsigned start, unsigned end)
{
 * Delayed printk facility, for scheduler-internal messages:
 * If there is output waiting for klogd, we wake it up.
{
	unsigned long flags;
	unsigned _con_start, _log_end;
	unsigned wake_klogd = 0, retry = 0;
	unsigned long flags;
	for ( ; ; ) {
again:
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		wake_klogd |= log_start - log_end;
		if (con_start == log_end)
			break;			/* Nothing to print */
		_con_start = con_start;
		_log_end = log_end;
		con_start = log_end;		/* Flush */
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock(&logbuf_lock);
		raw_spin_unlock(&logbuf_lock);
		stop_critical_timings();	/* don't trace print latency */
		call_console_drivers(_con_start, _log_end);
		stop_critical_timings();	/* don't trace print latency */
		start_critical_timings();
	if (con_start != log_end)
		retry = 1;
	raw_spin_lock(&logbuf_lock);
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
		con_start = log_start;
		raw_spin_lock_irqsave(&logbuf_lock, flags);
		raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	unsigned long end;
	unsigned chars;
{
	struct kmsg_dumper *dumper;
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	end = log_end & LOG_BUF_MASK;
	chars = logged_chars;
	raw_spin_unlock_irqrestore(&logbuf_lock, flags);
	raw_spin_lock_irqsave(&logbuf_lock, flags);
	if (chars > end) {
		s1 = log_buf + log_buf_len - chars + end;
		l1 = chars - end;
		s2 = log_buf;
		l2 = end;
	} else {
		s2 = log_buf + end - chars;
		l2 = chars;
	}
	}
	struct kref f_ref;
	SCSI_LOG_TIMEOUT(4, sg_printk(KERN_INFO, sdp,
			    max_sectors_bytes(sdp->device->request_queue));
			sg_remove_scat(sfp, &sfp->reserve);
		if (!sg_res_in_use(sfp) && dxfer_len <= rsv_schp->bufflen)
	if (md) {
		else {
			if (res)
				return res;
				return res;
		}
		}
	kref_init(&sfp->f_ref);
	unsigned long		tp_value;
	__u8			used_cp[16];	/* thread used copro */
#ifdef CONFIG_CRUNCH
	.macro set_tls_none, tp, tmp1, tmp2
#ifdef __ASSEMBLY__
	.endm
	.macro set_tls_v6k, tp, tmp1, tmp2
	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
	.endm
	.macro set_tls_v6, tp, tmp1, tmp2
	ldr	\tmp1, =elf_hwcap
	mcrne	p15, 0, \tp, c13, c0, 3		@ yes, set TLS register
	streq	\tp, [\tmp2, #-15]		@ set TLS value at 0xffff0ff0
	.endm
	.macro set_tls_software, tp, tmp1, tmp2
	mov	\tmp1, #0xffff0fff
#define set_tls		set_tls_none
#define has_tls_reg		1
#elif defined(CONFIG_CPU_V6)
#define set_tls		set_tls_v6
#define has_tls_reg		(elf_hwcap & HWCAP_TLS)
#elif defined(CONFIG_CPU_32v6K)
#define set_tls		set_tls_v6k
#define has_tls_reg		1
#else
#define set_tls		set_tls_software
#define has_tls_reg		0
#endif
#endif	/* __ASMARM_TLS_H */
	ldr	r3, [r2, #TI_TP_VALUE]
 THUMB(	str	lr, [ip], #4		   )
#ifdef CONFIG_CPU_USE_DOMAINS
	set_tls	r3, r4, r5
#endif
#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
#include <asm/mach/time.h>
		thread->tp_value = childregs->ARM_r3;
	if (clone_flags & CLONE_SETTLS)
			ret = put_user(task_thread_info(child)->tp_value,
		case PTRACE_GET_THREAD_AREA:
				       datap);
		thread->tp_value = regs->ARM_r0;
	case NR(set_tls):
		if (tls_emu)
	regs->uregs[reg] = current_thread_info()->tp_value;
		return 1;
	regs->ARM_pc += 4;
	unsigned int main_segs, blocks_per_seg;
	int i;
	fsmeta += le32_to_cpu(raw_super->segment_count_sit);
	fsmeta += le32_to_cpu(raw_super->segment_count_nat);
	fsmeta = le32_to_cpu(raw_super->segment_count_ckpt);
	fsmeta += le32_to_cpu(ckpt->rsvd_segment_count);
	if (unlikely(f2fs_cp_error(sbi))) {
	case I2C_SMBUS_I2C_BLOCK_DATA:
		if (read_write == I2C_SMBUS_READ) {
			if (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {
				dev_err(&adapter->dev,
					"Invalid block write size %d\n",
					data->block[0]);
				return -EINVAL;
			}
			return ret;
	}
	if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {
		ret = ext4_jbd2_file_inode(handle, inode);
		if (ret) {
			unlock_page(page);
			put_page(page);
			goto errout;
		}
	}

	struct platform_device *pdev = to_platform_device(dev);
	struct platform_device *pdev = to_platform_device(dev);
	int ret;
	unsigned long flags;
	 * Only remove the buffer from done_list if v4l2_buffer can handle all
	 * the planes.
				if (!peers(n, last_dest)) {
				}
			}
		}
		pmd_t *pmd)
static void touch_pmd(struct vm_area_struct *vma, unsigned long addr,
{
	/*
	 * We should set the dirty bit only for FOLL_WRITE but for now
	 * the dirty bit in the pmd is meaningless.  And if the dirty
	 * bit will become meaningful and we'll only set it with
	 * FOLL_WRITE, an atomic set_bit will be required on the pmd to
	 * set the young bit, instead of the current set_pmd_at.
	 */
	_pmd = pmd_mkyoung(pmd_mkdirty(*pmd));
	if (pmdp_set_access_flags(vma, addr & HPAGE_PMD_MASK,
				pmd, _pmd,  1))
	if (pmdp_set_access_flags(vma, addr & HPAGE_PMD_MASK,
		update_mmu_cache_pmd(vma, addr, pmd);
		touch_pmd(vma, addr, pmd);
	if (flags & FOLL_TOUCH)
		pud_t *pud)
static void touch_pud(struct vm_area_struct *vma, unsigned long addr,
{
	/*
	 * We should set the dirty bit only for FOLL_WRITE but for now
	 * the dirty bit in the pud is meaningless.  And if the dirty
	 * bit will become meaningful and we'll only set it with
	 * FOLL_WRITE, an atomic set_bit will be required on the pud to
	 * set the young bit, instead of the current set_pud_at.
	 */
	_pud = pud_mkyoung(pud_mkdirty(*pud));
	if (pudp_set_access_flags(vma, addr & HPAGE_PUD_MASK,
				pud, _pud,  1))
	if (pudp_set_access_flags(vma, addr & HPAGE_PUD_MASK,
		update_mmu_cache_pud(vma, addr, pud);
		touch_pud(vma, addr, pud);
	if (flags & FOLL_TOUCH)
		touch_pmd(vma, addr, pmd);
	if (flags & FOLL_TOUCH)
	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
	if (!handle->h_transaction) {
		err = jbd2_journal_stop(handle);
		return handle->h_err ? handle->h_err : err;
	if (!handle->h_transaction) {
	}
	err = handle->h_err;
	s->s_magic = ECRYPTFS_SUPER_MAGIC;
	ufs->upper_mnt = clone_private_mount(&upperpath);
	struct rcu_head		rcu;
};
	key_ref_t key_ref, skey_ref;
	int rc;
				if (unlikely(t == 0)) {
					while (unlikely(*ip == 0)) {
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
			if (unlikely(t == 2)) {
				while (unlikely(*ip == 0)) {
				NEED_IP(2);
	struct socket_wq	peer_wq;
};
static int unix_writable(const struct sock *sk)
		}
		sock_put(skpair); /* It may now die */
	init_waitqueue_head(&u->peer_wait);
	unix_insert_socket(unix_sockets_unbound(sk), sk);
		unix_peer(sk) = other;
		unix_state_double_unlock(sk, other);
	int data_len = 0;
	unix_state_lock(other);
	unix_state_lock(other);
	err = -EPERM;
	if (sock_flag(other, SOCK_DEAD)) {
		err = 0;
		unix_state_lock(sk);
			unix_peer(sk) = NULL;
			unix_state_unlock(sk);
	if (unix_peer(other) != sk && unix_recvq_full(other)) {
		if (!timeo) {
			err = -EAGAIN;
			goto out_unlock;
		}
		timeo = unix_wait_for_peer(other, timeo);
		err = sock_intr_errno(timeo);
		if (signal_pending(current))
			goto out_free;
		goto restart;
	}
	if (sock_flag(other, SOCK_RCVTSTAMP))
out_unlock:
	unix_state_unlock(other);
			sock_poll_wait(file, &unix_sk(other)->peer_wait, wait);
		incoming_msg = (struct nlmsghdr *)kvp_recv_buffer;
		if (flags & __GFP_ZERO)
			memset(ptr, 0, size);
		if (flags & __GFP_ZERO)
			memset(addr, 0, size);
		addr = page_address(page);
		return addr;
	__u8  pad02[2];
	__u32 pushbuf;
	__u8  chid;
	__u64 offset;
	__u8  pad01[6];
	__u32 pushbuf;
	__u8  chid;
	__u32 ilength;
	__u64 ioffset;
};
	__u8  pad04[4];
	__u32 pushbuf;
	__u64 ioffset;
};
	__u8  pad01[3];
	__u32 pushbuf;
	__u8  version;
};
	__u8  pad01[2];
	__u32 pushbuf;
	__u8  head;
};
	__u8  pad01[2];
	__u32 pushbuf;
	__u8  head;
};
	struct nvif_object *parent;
#define nvif_object(a) (a)->object
	struct nvkm_client_notify *notify[16];
};
static inline struct nvkm_client *
	struct nvkm_object *object;
};
struct nvkm_object *nvkm_handle_ref(struct nvkm_object *, u32 name);

	struct nvkm_dmaobj *pushdma;
				  int bar, u32 addr, u32 size, u32 push,
				  struct nvkm_oclass *,
				  u64 engmask, int len, void **);
			args.kepler.pushbuf = chan->push.ctxdma.handle;
			args.kepler.engine  = engine;
			args.kepler.ilength = 0x02000;
			args.nv50.pushbuf = chan->push.ctxdma.handle;
			args.nv50.version = 0;
			args.nv50.ilength = 0x02000;
	args.pushbuf = chan->push.ctxdma.handle;
	args.version = 0;
	args.offset = chan->push.vma.offset;
	ret = nvif_object_init(&device->object, args->pushbuf,
			       NV_DMA_FROM_MEMORY, &(struct nv_dma_v0) {
	ret = nv50_chan_create(device, disp, oclass, head, data, size,
		client->object.parent = NULL;
	client->object.parent = &client->object;
	if (size >= sizeof(*args) && args->v0.version == 0) {
		args->v0.owner = NVIF_IOCTL_V0_OWNER_ANY;
		args->v0.path_nr = 0;
		while (args->v0.path_nr < ARRAY_SIZE(args->v0.path)) {
			args->v0.path[args->v0.path_nr++] = object->handle;
			if (object->parent == object)
				break;
			object = object->parent;
		}
	object->parent = parent;
	if (object->parent) {
		if (!(args = kmalloc(sizeof(*args) + size, GFP_KERNEL))) {
		args->new.token = (unsigned long)(void *)object;
		args->new.route = parent->client->route;
		args->new.handle = handle;
int
	client->debug = nvkm_dbgopt(dbg, "CLIENT");
	return 0;
	handle->priv = ~0;
{
	struct nvkm_handle *item, *temp;
	}
	list_del(&handle->head);
struct nvkm_object *
nvkm_handle_ref(struct nvkm_object *parent, u32 name)
{
	struct nvkm_object *object = NULL;
	struct nvkm_handle *handle;

	while (!nv_iclass(parent, NV_NAMEDB_CLASS))
		parent = parent->parent;

	handle = nvkm_namedb_get(nv_namedb(parent), name);
	if (handle) {
		nvkm_object_ref(handle->object, &object);
		nvkm_namedb_put(handle);
	}

	return object;
}

	nvif_ioctl(handle->object, "new vers %d handle %08x class %08x "
nvkm_ioctl_path(struct nvkm_handle *parent, u32 type, u32 nr, u32 *path,
		void *data, u32 size, u8 owner, u8 *route, u64 *token)
	struct nvkm_handle *handle = parent;
	struct nvkm_namedb *namedb;
	struct nvkm_object *object;
		nvif_ioctl(object, "path 0x%08x\n", path[nr]);
			nvif_debug(object, "cannot have children (path)\n");
		    !(handle = nvkm_namedb_get(namedb, path[nr]))) {
			nvif_debug(object, "handle 0x%08x not found\n", path[nr]);
		nvkm_namedb_put(handle);
	if (owner != NVIF_IOCTL_V0_OWNER_ANY && owner != handle->route) {
		nvif_ioctl(object, "object route != owner\n");
		if (nvkm_ioctl_v0[type].version == 0)
		nvif_ioctl(object, "vers %d type %02x path %d owner %02x\n",
		       struct nvkm_oclass *oclass, u32 pushbuf, int head,
		       struct nvkm_object *engine,
		       int length, void **pobject)
{
	struct nv50_disp_dmac *dmac;
	dmac->pushdma = (void *)nvkm_handle_ref(parent, pushbuf);
	if (!dmac->pushdma)
		return -ENOENT;
		return -ENOENT;
	switch (nv_mclass(dmac->pushdma)) {
	case 0x0002:
		if (dmac->pushdma->limit - dmac->pushdma->start != 0xfff)
	case 0x003d:
			return -EINVAL;
		switch (dmac->pushdma->target) {
		case NV_MEM_TARGET_VRAM:
			dmac->push = 0x00000001 | dmac->pushdma->start >> 8;
		case NV_MEM_TARGET_VRAM:
			break;
			dmac->push = 0x00000003 | dmac->pushdma->start >> 8;
		case NV_MEM_TARGET_PCI_NOSNOOP:
			break;
	nvkm_object_ref(NULL, (struct nvkm_object **)&dmac->pushdma);
				   "pushbuf %08x\n",
		nvif_ioctl(parent, "create disp core channel dma vers %d "
			   args->v0.version, args->v0.pushbuf);
				   "pushbuf %08x head %d\n",
		nvif_ioctl(parent, "create disp base channel dma vers %d "
			   args->v0.version, args->v0.pushbuf, args->v0.head);
				   "pushbuf %08x head %d\n",
		nvif_ioctl(parent, "create disp overlay channel dma vers %d "
			   args->v0.version, args->v0.pushbuf, args->v0.head);
	struct nvkm_dmaobj *pushdma;
			  int bar, u32 addr, u32 size, u32 pushbuf,
			  struct nvkm_oclass *oclass,
			  u64 engmask, int len, void **ptr)
{
	struct nvkm_fifo *fifo = (void *)engine;
	chan->pushdma = (void *)nvkm_handle_ref(parent, pushbuf);
	if (!chan->pushdma)
		return -ENOENT;
		return -ENOENT;
	dmaeng = (void *)chan->pushdma->base.engine;
	switch (chan->pushdma->base.oclass->handle) {
	case NV_DMA_FROM_MEMORY:
	ret = dmaeng->bind(chan->pushdma, parent, &chan->pushgpu);
	if (ret)
	nvkm_object_ref(NULL, (struct nvkm_object **)&chan->pushdma);
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel gpfifo vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "ioffset %016llx ilength %08x\n",
		nvif_ioctl(parent, "create channel gpfifo vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "ioffset %016llx ilength %08x\n",
		nvif_ioctl(parent, "create channel gpfifo vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "ioffset %016llx ilength %08x engine %08x\n",
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel dma vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "offset %016llx\n", args->v0.version,
		nvif_ioctl(parent, "create channel gpfifo vers %d pushbuf %08x "
	if (nvif_unpack(args->v0, 0, 0, false)) {
				   "ioffset %016llx ilength %08x\n",
#include "blk-mq.h"
	if (q->mq_ops) {
		spin_lock_irqsave(&fq->mq_flush_lock, flags);
		spin_lock_irqsave(&fq->mq_flush_lock, flags);
		flush_rq->tag = -1;
	 * be in flight at the same time.
	if (q->mq_ops) {
		flush_rq->mq_ctx = first_rq->mq_ctx;
		flush_rq->tag = first_rq->tag;
	}
		     	rq = blk_mq_tag_to_rq(hctx->tags, off + bit);
		     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {
			if (rq->q == hctx->queue)
			rq = blk_mq_tag_to_rq(tags, off + bit);
		     bit = find_next_bit(&bm->word, bm->depth, bit + 1)) {
			fn(rq, data, reserved);
#endif
static inline bool is_flush_request(struct request *rq,
		struct blk_flush_queue *fq, unsigned int tag)
{
	return ((rq->cmd_flags & REQ_FLUSH_SEQ) &&
			fq->flush_rq->tag == tag);
}

	struct request *rq = tags->rqs[tag];
	/* mq_ctx of flush rq is always cloned from the corresponding req */
	struct blk_flush_queue *fq = blk_get_flush_queue(rq->q, rq->mq_ctx);

	if (!is_flush_request(rq, fq, tag))
		return rq;

	return fq->flush_rq;
{
}
	struct request		*flush_rq;
	spinlock_t		mq_flush_lock;
	if (o_direct) {
		iocb->private = &overwrite;
