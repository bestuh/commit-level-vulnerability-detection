\documentclass[
	ngerman,
	ruledheaders=section,%Ebene bis zu der die Überschriften mit Linien abgetrennt werden, vgl. DEMO-TUDaPub
	class=report,% Basisdokumentenklasse. Wählt die Korrespondierende KOMA-Script Klasse
	thesis={type=bachelor},% Dokumententyp Thesis, für Dissertationen siehe die Demo-Datei DEMO-TUDaPhd
	accentcolor=9c,% Auswahl der Akzentfarbe
	custommargins=true,% Ränder werden mithilfe von typearea automatisch berechnet
	marginpar=false,% Kopfzeile und Fußzeile erstrecken sich nicht über die Randnotizspalte
	%BCOR=5mm,%Bindekorrektur, falls notwendig
	parskip=half-,%Absatzkennzeichnung durch Abstand vgl. KOMA-Sript
	fontsize=11pt,%Basisschriftgröße laut Corporate Design ist mit 9pt häufig zu klein
%	logofile=example-image, %Falls die Logo Dateien nicht vorliegen
]{tudapub}




% Der folgende Block ist nur bei pdfTeX auf Versionen vor April 2018 notwendig
\usepackage{iftex}
\ifPDFTeX
	\usepackage[utf8]{inputenc}%kompatibilität mit TeX Versionen vor April 2018
\fi

%%%%%%%%%%%%%%%%%%%
%Sprachanpassung & Verbesserte Trennregeln
%%%%%%%%%%%%%%%%%%%
\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}% Anführungszeichen vereinfacht
\usepackage{microtype}
 \usepackage[backend=biber]{biblatex}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{float}


%%%%%%%%%%%%%%%%%%%
%Literaturverzeichnis
%%%%%%%%%%%%%%%%%%%
\usepackage{biblatex}   % Literaturverzeichnis
\bibliography{citations} %DEMO-TUDaBibliography} %


%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Tabellen
%%%%%%%%%%%%%%%%%%%
%\usepackage{array}     % Basispaket für Tabellenkonfiguration, wird von den folgenden automatisch geladen
\usepackage{tabularx}   % Tabellen, die sich automatisch der Breite anpassen
%\usepackage{longtable} % Mehrseitige Tabellen
%\usepackage{xltabular} % Mehrseitige Tabellen mit anpassarer Breite
\usepackage{booktabs}   % Verbesserte Möglichkeiten für Tabellenlayout über horizontale Linien

%%%%%%%%%%%%%%%%%%%
%Paketvorschläge Mathematik
%%%%%%%%%%%%%%%%%%%
%\usepackage{mathtools} % erweiterte Fassung von amsmath
%\usepackage{amssymb}   % erweiterter Zeichensatz
%\usepackage{siunitx}   % Einheiten

%Formatierungen für Beispiele in diesem Dokument. Im Allgemeinen nicht notwendig!
\let\file\texttt
\let\code\texttt
\let\tbs\textbackslash

\usepackage{pifont}% Zapf-Dingbats Symbole
\newcommand*{\FeatureTrue}{\ding{52}}
\newcommand*{\FeatureFalse}{\ding{56}}
\setlength\parindent{24pt}

\begin{document}

\Metadata{
	title=TUDaThesis - Abschlussarbeiten im CD der TU Darmstadt,
	author=Jan Philipp Wagner
}

\title{TUDaThesis -- Abschlussarbeiten im CD der TU Darmstadt}
\subtitle{\LaTeX{} using TU Darmstadt's Corporate Design}
\author[J.Wagner]{Jan Philipp Wagner}%optionales Argument ist die Signatur,
\birthplace{Geburtsort}%Geburtsort, bei Dissertationen zwingend notwendig
\reviewer{Gutachter 1}%Gutachter

%Diese Felder erden untereinander auf der Titelseite platziert.
%\department ist eine notwendige Angabe, siehe auch dem Abschnitt `Abweichung von den Vorgaben für die Titelseite'
\department{ce} % Das Kürzel wird automatisch ersetzt und als Studienfach gewählt, siehe Liste der Kürzel im Dokument.
\institute{Institut}
\group{SPIN}

\submissiondate{\today}
\examdate{\today}

%	\tuprints{urn=1234,printid=12345}
%	\dedication{Für alle, die \TeX{} nutzen.}

\maketitle

\affidavit

\tableofcontents

\chapter{Abstract}


	(This abstract follows the structure you proposed. I left the key points in for now.)\\\\
	\textbf{Background}\\
	Static analysis of software deals with the task of finding vulnerabilities in source code without executing it.\\
	\textbf{State of the Art}\\
	In the recent past, progress has been made in this field by analyzing \textit{git commits} instead of specific files. Here, the state of the art uses machine learning- and text mining techniques to predict whether a specific commit is likely to make the software vulnerable based on its text features and metadata.\\
	\textbf{problem with the state of the art}\\
	These tools, although promising, have not seen widespread adoption. Reasons for this might be the their unavailability to the masses along with the fairly limited dataset used for their validation.\\
	\textbf{my goal}\\
	Therefore, this thesis has the goal of creating such a tool of our own that has been validated and tested on ground truth data and is available to everyone.\\
	\textbf{challenges}\\
	This is especially challenging because ground truth data for commits that introduced vulnerabilities is extremely rare.\\
	\textbf{approach}\\
	We approach this problem by scouring the web for datasets that have been constructed manually as well as automatically tracing vulnerabilities back to their origin.\\
	\textbf{results + conclusion}\\
	Our results show that even though automatic tracing of vulnerabilities is harder than previous research suggested,...
	(results are yet to come)
	\chapter{Introduction}
	(moved the old abstract to the introduction for now, will revise after the rest is done)\\\\
	Static analysis of software deals with the task of finding vulnerabilities in source code without executing it. While most approaches to this field of research in the past focused on predicting vulnerabilities at the file/component level, there also have been a few studies examining change-level predictions. Perl et al. introduced a static analysis tool that uses machine learning- and text mining techniques to predict whether a specific \textit{commit} to a repository is prone to be vulnerable or not \cite{perl2015vccfinder}. Since their tools are able to flag far fewer false positives than conventional static analysis tools (e.g. 99\% less than Flawfinder \cite{flawfinder}), their approach seems very promising. This rather large increase in precision is achieved by not only examining features extracted from the source code, like prior machine learning approaches to static analysis did before \cite{Scandariato}, but also taking into account additional meta-information retrieved from the commits, such as the experience of the contributing developer. In this thesis, we want to evaluate these commit-based static analysis tools and train our own classifier on a self-made dataset to possibly further improve upon the works in \cite{perl2015vccfinder}.

	\section{Contributions}
	\chapter{Related Work}
	(taken straight from the exposé, needs revision)\\\\
				\section{Vccfinder}
	In [1] Perl et al. introduced vccfinder. As it was the first static analysis tool to perform change-level predictions, it constitutes the basis for this thesis. In the paper it was shown that their software produces 99\% less false positives than conventional static analysis tools. This is achieved by combining concepts from machine learning (Support Vector Machines) and natural language processing (bag of words). Contrary to other machine learning approaches to static analysis in the past, the tool not only analyzes the source code but also takes useful commit metadata into account. It was trained on the first ever large scale database linking CVEs to VCCs, also contributed by Perl et al.\\\\


	\section{Vuldigger}
	Vuldigger \cite{vuldigger} by Yang et al. aimed to improve upon the tool developed in \cite{perl2015vccfinder}. In their paper, they present an advanced heuristic for linking fixing commits to their VCCs and highlight additional data (e.g. distinguishing between total lines of code added/deleted) for training. Also, contrary to \cite{perl2015vccfinder} they used a Random Forests technique for classifying instead of a Support Vector Machine. Their tool also lays an emphasis on effort-awareness, meaning it takes into account the actual effort that would be required auditing the flagged commits. (vccfinder macht das auch irgendwie mit precision over recall)\\\\

	\section{Flawfinder}
	Flawfinder [3] is a more conventional static analysis tool for the file/component level. As it is very light weight and also scans C/C++ files, it was ntural to compare results with \cite{perl2015vccfiner} \cite{vuldigger}.\\\\



	\section{Meneely}
	Meneely et al. and Shin et al. analyzed metadata from different code repositories in relation to CVEs in [6, 7, 8]. They manually mapped CVEs to vulnerability-contributing commits for the Mozilla Firefox Browser, Apache HTTP server and parts of the RHEL Linux kernel. However, they did not use this data to predict vulnerabilities.

	\section{Scandariator}
	In \cite{scandariato} Scandariator et al. were the first to use a machine learning approach for predicting software vulnerabilities based on text mining of source code. Also using a bag of words model, they examined 20 Android apps over the course of two years and demonstrated that their approach has good performance for both precision and recall.\\\\

	\chapter{Data}
	One of the key factors for achieving the goal of this thesis is a sound dataset. On the one hand we need data to evaluate the performance of vccfinder and on the other hand we need that same data to train and validate our own classifier. Specifically, we are looking for a dataset of commits that introduced vulnerabilities in the past. We will refer to these commits as Vulnerability Contributing Commits or \textit{VCCs}. The goal is to use these VCCs to train a classifier that is able to distinguish between commits, that did not introduce vulnerabilities and commits that did so. After that, the classifier can be used to predict whether a new commit is prone to be vulnerable as well as finding not yet discovered vulnerabilities in older commits.\\
	We can obtain VCCs in two different ways which are outlined in the following subsections.

	\section{Manually identified VCCs}
	The straight forward approach is to look for datasets of VCCs that have been manually identified by security researchers. Unfortunately, datasets like that are rare to come across and thus their size might not be sufficient for training a classifier. Still, we were able to find two such datasets:
	\subsection{Ubuntu CVE Tracker}

	The largest dataset for a single project that we were able to extract is from the Ubuntu CVE Tracker \cite{cve-tracker}. In this project, the Ubuntu Security Team gathered lots of Common Vulnerabilities and Exposures (CVE) data for vulnerabilities affecting the Linux kernel. Here we can find, among other things, mappings from the VCC of a vulnerability to the commit that fixed it (\textit{fixing commit}) (\ref{fig:CVE-2016-0728}). With the help of Python and some regular expressions this enabled us to build our first dataset containing 790 of these mappings.

	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/cve-tracker.png}
	  \caption{excerpt from retired/CVE-2016-0728 from the Ubuntu CVE Tracker}
	  \label{fig:CVE-2016-0728}
	\end{figure}

	\subsection{Vulnerability History Project}
	We also were able to extract mappings from fixing commits to VCCs from Andrew Meneelys Vulnerability History Project \cite{vulnerabilityHistoryProject}. He is a security researcher currently working at the Rochester Institute of Technology. A lot of his prior work also focused on examining VCCs \cite{meneely}. His project contains mappings for Chromium, Apache HTTPD, Apache Struts and Apache Tomcat. Using it, we were able to add 843 new mappings that bring a variety of projects as well as a variety of programming languages (C, C++ and Java instead of just C) to our dataset.\\
	However, it should be noted that Meneelys definition of a VCC is slightly different from ours. In his paper \cite{meneely} it is stated that:
	\begin{quote}
	"Some vulnerabilities might have multiple regions in a given file where a fix was required. In that case, we treat each of those regions with a separate detection script\footnote[1]{Although Meneely et al. did use detection scripts, they also manually reviewed the mappings they produced.} to maximize the number of potential VCCs we can find."
	\end{quote}
	Thus his approach is, by design, prone to generate multiple VCCs to a single vulnerability. In contrary, the Ubuntu CVE Tracker looks for a single point of failure when flagging a commit as VCC, which seems more fitting for our purposes. Nevertheless, we think that Meneelys mappings can still be beneficial in training our classifier.

	\section{Automated tracing of VCCs}
	The second approach is the one also used in \cite{perl2015vccfinder}. Here, the idea is to first find a large amount of fixing commits and then use a \textit{git blame} based heuristic to automatically map these fixing commits to their VCCs. \textit{Git blame} finds which commit last changed a specific line in a given file and so it can be used for tracing a vulnerability back to its origin.\\
	The reason why we use this approach is that fixing commits are much easier to identify than VCCs. That is because most of the time the fixing commit is mentioned in the National Vulnerability Database (NVD) entry for a vulnerability or the commit message of a fixing commit mentions the CVE ID of the vulnerability it fixed. On the other hand, a commit that introduces a vulnerability usually does not mention so in its commit message, for obvious reasons. More on how we obtained fixing commits can be read in Manuel Bracks Bachelor thesis \cite{ManuelsThesis}.\\
	Due to the fact that this approach is automated, we can use it to build a much larger dataset then what is possible by manually finding VCCs. Perl et al. claim that they had great success with their heuristic, therefore we want to explore it a bit more in the next chapter.


	\chapter{Heuristic}
	In their paper, Perl et al. claim that the heuristic they use to map a fixing commit to its VCC yields a 96.9\% accuracy. They determine this by taking a 15\% random sample of all VCCs their heuristic produced and manually checked them. The heuristic is as follows:\\\\
	1. Ignore changes in documentation such as release notes or change logs.\\
	2. For each deletion, blame the line that was deleted.\\
	3. For every continuous block of code inserted in the fixing commit, \textit{blame} the lines before and after the block.\\
	4. Finally, mark the commit \textit{vulnerable} that was blamed most in the steps above. If two commits were blamed for the same amount of lines, blame both.\\\\
	They also mention that further improving this heuristic is an "interesting avenue" for future research.	This next section is dedicated to that exact purpose. The two main goals are evaluating the heuristic and the claims made about it and also possibly improving it.


	\section{Evaluation}

	\subsection{Comparing the mappings}
	Upon request, Henning Perl (author of \cite{perl2015vccfinder}) provided us with the dataset they produced utilizing their heuristic. We implement a clone of the heuristic according to the description in the paper in Python. To test whether or not our clone is an accurate representation of what was used for vccfinder, we check whether we are able to reproduce their data with our own implementation.\\
	The dataset contains 740 mappings of fixing commit to VCC from various repositories. If we run our implementation of the vccfinder heuristic over the same fixing commits, we find that in 97\% of the cases, we can trace the vulnerability back to the same VCC. We chalk up the missing 3\% to manual remapping done by Perl et al. to clean up their data. Thus we deem our implementation of the vccfinder heuristic an accurate clone of the one used by Perl et al.\\
	However, when comparing these mappings to the mappings that we extracted from the Ubuntu CVE tracker, we find a big discrepancy. Out of all the fixing commits that were present in both sets, only 57.46\% are mapped to the same VCC. This means that 42.54\% of the time the heuristic used by Perl et al. produces a different mapping than the manual analysis by the Ubuntu team.\\
	In order to determine which mappings are true, we manually analyze 10 fixing commits that were mapped to a different VCC by the heuristic and the Ubuntu team. We find that in 9 out of ten cases, the Ubuntu mapping was accurate, while the one produced by the heuristic was not \cite{nikosreport} and in one case both possible VCCs were valid. Further testing against the Ubuntu dataset showed, that the accuracy of the vccfinder heuristic is closer to 50\% than the 96.9\% claimed in \cite{perl2015vccfinder} and even that is only true when accounting for manual analysis when two or more commits are blamed the same amount of time. Thus we can conclude that producing mappings from fixing commit to VCC seems much harder than what is implied in previous work, which likely fell into the trap of confirmation bias.\\ Fortunately, our prediction model can deal with some amount of noise, meaning that the heuristic is not completely useless for our purposes. Still, noise in the validation- or test set is unacceptable. This is why we will use the data produced by the heuristic only for training our classifier not for validating or testing it.

	\section{Analysis: What makes a Vulnerability hard to trace?}
		In the previous section, we showed that the heuristic for tracing vulnerabilities back to their VCCs used in \cite{perl2015vccfinder} has a much lower accuracy than what was previously assumed. One step towards possibly improving the accuracy is to determine the characteristics of commits that are accurately mapped versus ones that are not. For that, we identified two groups of fixing commits within the Ubuntu dataset:\\\\
			The first group consists of all fixing commits for which the heuristic was accurate, meaning that it blamed the corresponding VCC the most often out of all possible commits. We call this group \textit{easy commits}. It consists of 406 different commits.\\\\
			The second group consists of the fixing commits for which the heuristic did not blame the corresponding VCC a single time. We call this group \textit{hard commits}. It consists of 241 different commits.\\\\
	We analyzed the commits in each group by looking at the Common Weakness Enumeration (CWE) ID, Common Vulnerability Scoring System (CVSS) score and type of vulnerability (e.g. DoS, Overflow, etc.) they fixed, as well as the number of Lines Of Code (LOC) they added/removed/changed and how many files they altered.

	  	\subsection{CWE ID}
			The CWE ID of a vulnerability describes a common software weakness that is being abused in it. The whole dataset contains 44 different CWE IDs of which 31 are also present in \textit{hard commits} and 34 in \textit{easy commits}.\\ One might hypothesize that vulnerabilities fixed in \textit{hard/easy commits} are more common to have a specific CWE ID. To explore this aspect we used Pearson's chi-squared test. The null hypothesis holds iff the observed frequency of a specific CWE ID does not significantly ($\alpha$ = 0.05) deviate from the expected frequency. We obtain the expected frequencies by looking at the general distribution of CWE IDs across all commits in the dataset. We found that we have to reject the null hypothesis in two cases:\\\\
	Easy commits are significantly more likely to fix vulnerabilities with CWE 190 (Integer Overflow or Wraparound) ($\rho$ = 0.0361).\\\\
	Hard commits are significantly more likely to fix vulnerabilities with CWE 264 (Permissions, Privileges, and Access Controls) ($\rho$ = 0.0212).\\\\
	Additionally two weaker correlations can be found when choosing $\alpha$ = 0.1:\\\\
	Easy commits are more likely to fix vulnerabilities with CWE 200 (Information Exposure) ($\rho$ = 0.0675).\\\\
	Hard commits are less likely to fix vulnerabilities with CWE 190 ($\rho$ = 0.0568).\\\\
	Hard commits are more likely to fix vulnerabilities with CWE 665 (Improper Initialization) ($\rho$ = 0.0844).\\
	Hard commits are more likely to fix vulnerabilities with CWE 704 (Incorrect Type Conversion or Cast) ($\rho$ = 0.0844).\\

		\subsection{Number Of Files Altered}
		One could also hypothesize that the heuristic has a harder time finding the correct VCC if the fixing commit changed a lot of files instead of, for example, only making alterations in a single file. To explore this hypothesis we used a single sample t-test. The null hypothesis holds iff there is no significant ($\alpha$ = 0.05) difference between the mean of the numbers of altered files in easy/hard commits compared to all commits. We have to reject the null hypothesis for easy commits:\\\\
		Easy commits alter significantly less files ($\rho$ = 0.0335).

		\subsection{LOC Added}
		To explore whether or not the mean of numbers of added LOC in easy/hard commits differ compared to all commits, we used a single sample t-test again. The null hypothesis holds iff the means do not differ significantly. Again, we have to reject this null hypothesis for easy commits:\\\\
		Easy commits add significantly less lines ($\rho$ = 0.0007).

		\subsection{LOC Removed/Changed, CVSS Score, Vulnerability Type}
		When looking at the numbers of LOC removed/changed in commits and the CVSS score of the vulnerabilities that were fixed, we again used a single sample t-test \iffalse why single?? \fi but did not find any correlations. The same goes for the vulnerability type where we used Pearson's chi-squared test again.
	        \subsection{Limitations}
		The dataset we extracted from the Ubuntu CVE Tracker is decently large and shows strong indication that it is highly accurate \cite{nikosreport}. However, we have to consider some limitations when using it:\\\\
		\textbf{Source bias:} Since the entire dataset consists of vulnerabilities found in the Linux kernel, there is a heavy bias towards certain types of vulnerabilities. For example, the dataset does not contain a single XSS vulnerability (for obvious reasons), even though they make up 12.5\% of all reported vulnerabilities as a whole \cite{vulnerabilities-by-types}. \iffalse A hypothesis can be made, that the accuracy of the heuristic observed on this dataset can be regarded as a lower bound since a lot of the vulnerabilities found in the Linux kernel deal with permissions and concurrency, which are types of vulnerabilities that we deem \colorbox{yellow}{hard for the heuristic to trace}.\fi\\\\
		\textbf{Completeness:} Even when looking only at the Linux kernel, our dataset is far from complete. There are loads of pairs of fixing commit and VCC that the Ubuntu CVE Tracker did not map. Of the 2357 vulnerabilities found in the Linux kernel \cite{vulnerability-list} we cover about one third.\\\\
		\textbf{Quality:} Even though we deem the quality of the mappings by the Ubuntu CVE Tracker as very high, we cannot assume that they are 100\% accurate. Mapping fixing commits to their VCC is by no means a trivial task and often even professionals have differing opinions on where and when a vulnerability was first introduced.
		\subsection{Conclusion}
		From these findings, ideas can be drawn on how we might be able to improve the vccfinder heuristic.\\
		Judging from the strongest correlation we found, that easy commits add far less LOC than hard commits, we can hypothesize that blames due to additions of code are less likely to point to the correct VCC. Thus we might want to take blames stemming from added lines of code less into consideration.\\
		Knowing that easy commits change far fewer files does not help us improve the heuristic directly. Nonetheless, this gives us information about which kind of fixing commits we can more likely map accurately to their VCC and so might help build a training set with less noise for our classifier.\\
		\iffalse {The correlations} found in 5.2.1 can also be helpful for improving the heuristic. However, we cannot use them to discriminate against CVEs with a specific CWE ID when building our dataset. That is because that would bias our classifier towards only flagging commits that introduced a vulnerability that abuses the common weaknesses we favored. Still, knowing that commits fixing a vulnerability of a specific CWE are harder to map than others might indicate that we should change our approach for mapping them.\\\fi
	But it should also be noted that only 129 commits were neither \textit{hard} or \textit{easy}. This suggests that the margin the heuristic can be improved is rather low, as only these commits are likely to additionally be mapped correctly by an improved heuristic.\\
		All in all this analysis taught us quite a few things about what the strong and weak points of the heuristic used in \cite{perl2015vccfinder} are and we can use this knowledge for improving it.

	\section{Improving the heuristic}
	\subsection{Approach}
	In order to improve the heuristic in a meaningful way, we use the concept of training-, validation and test sets from the area of machine learning. We choose to use this approach even though our heuristic does not utilize machine learning techniques, because we want to ensure that it will also perform well on commits that are not present in our dataset.\\
	Since we deemed the Linux kernel dataset we extracted from the Ubuntu CVE tracker very high quality, our testn set consists purely of mappings from it. More specifically, we choose the mappings for the newest fixing commits (all from 2019) so that we assure that our heuristic will work on future commits as well.\\
	The validation set also contains only mappings from the Ubuntu dataset and the training set consists of the remaining Ubuntu mappings and the mappings created by Andrew Meneely.
	\subsection{Improvement ideas}
	Before diving into what actually ended up increasing the performance of the heuristic, in this section we want to discuss all the different ideas that we tried out and why we tried them. What follows is a list of these ideas: \\\\
	\textbf{-w flag:} Setting the -w flag when using \textit{git blame} ignores whitespace when comparing the parents version with the childs version of a line to find where the lines came from.\\
	Rationale: Since adding/removing whitespaces does not change the functionality of the code (at least in the languages we are considering), using the -w flag might yield better results.   \\\\
	\textbf{-M flag:} Setting the -M flag when using \textit{git blame} detects moved or copied lines within a file. \\
	Rationale: If a line was copied from within the file, we might want to blame the commit that added the original line and not the copy.\\\\
	\textbf{-C flag:} Setting the -C flag when using \textit{git blame} detects lines moved or copied from other files that were modified in the same commit. \\
	Rationale: If a line was copied from another file, we might want to blame the commit that added the original line and not the copy.\\\\
	\textbf{Increasing the blame radius:} Instead of only blaming one line before and after inserted blocks, blame multiple lines before and after each block. \\
	Rationale: In our analysis (5.2) we concluded that the vccfinder heuristic often times does not blame the true VCC a single time, thus maybe we should try to blame more lines. \\\\
	\textbf{Ignore comments/log files:} Comments or log files should not be taken into account.\\
	Rationale: Comments/logs do not change the functionality of the code, thus should not be taken into account when determining the VCC.\\\\
	\textbf{Weighted blaming:} Take specific lines more heavily into account.\\
	Rationale: We might be able to find a general rule for which lines are more likely to lead us to a VCC and thus should be taken more heavily into account.
	\begin{quote}
	\textbf{Weight lines containing a specific keyword more heavily:}\\
	Rationale: For vulnerabilities of a specific type/with a specific CWE some lines containing certain keywords might be more important in determining the VCC than others (e.g. removals of lines containing "free" in a fix for a "use after free" vulnerability).\\\\
	\textbf{Weight deletions more heavily}\\
	Rationale: Deletions might be more important for determining the VCC. This is also what the analysis (5.2) suggests.\\\\
	\textbf{Weight additions more heavily}\\
	Rationale: We should still test if additions might be more important for determining the VCC.
	\textbf{Analyze commit message:} If specific files or functions are mentioned in the commit message of the fixing commit, weight blames in these files/functions more heavily.\\
	Rationale: If the commit message of the fixing commits mentions specific files, these might be more likely to contain lines that lead to the VCC.\\\\
	\textbf{Analyze NVD summary:} If specific files or functions are mentioned in the NVD summary of the vulnerability, weight blames in these files/functions more heavily.\\
	Rationale: If the NVD summary of the vulnerability mentions specific files, these might be more likely to contain lines that lead to the VCC.\\\\
	\textbf{Adjust weights accordings to CWE:} If a vulnerability has a specific CWE, adjust weights for blames from deletions/additions.\\
	Rationale: The importance of blames due to deletions/additions might correlate with the CWE of the vulnerability.\\\\
	\end{quote}

	\subsection{Own Heuristic}
	In order to improve upon the heuristic used for vccfinder, we add the aforementioned ideas to its code one by one. Then we check if the heuristic performs better on both training- and validation set compared to the standard heuristic. If the accuracy is lower than it was before on either set, we scrap the idea. After finding which ideas actually have a positive impact on the heuristic on their own, we combine them again one by one. The combination of ideas that performs the best is our final heuristic. During this process we notice two main observations:\\\\
	\textbf{Observation number 1:}\\
	The maximum accuracy that could \textit{theoretically} be achieved with this method is around 70\%. That is because, in 30\% of the cases we examined, the true VCC was not blamed a single time by the heuristic \footnote{Increasing the blame radius did not have a significant effect on that.}. This means that in those cases the true VCC can never be the most blamed commit. Moreover, this 70\% accuracy would only be possible if the accumulated weight for the true VCC was the highest out of all commits every time, which is very unlikely. This suggested to us very early on, that the margin of how much the heuristic can be improved is relatively small.\\\\
	\textbf{Observation number 2:}\\
	Possibly what surprised us the most while working on this, is that ignoring lines that do not change the functionality of the code (i.e. comments and log files) does not increase the accuracy but rather decreases it. This at first seems counter intuitive. If removing/adding comments can never fix a vulnerability, then why should blaming them lead to the origin of the vulnerability? A possible explanation for this is that, often times when a vulnerability is being fixed, comments/logs that were made when introducing that vulnerability are also removed. Blaming those removals then also leads to the true VCC.\\\\
	\textbf{What worked?}\\
	Unfortunately, a lot of the ideas listed in 5.4 did not improve the heuristic on the training- as well as the validation set. Oftentimes we were able to increase the accuracy on the training set but then the accuracy on the validation set would decrease. This is equivalent to over fitting a classifier and indicates bad generalization, meaning that the heuristic would likely also perform worse on entirely new commits. However, two of the ideas from the previous section did improve the heuristic on both sets:\\\\
	1. Setting the -w flag when blaming.\\
	2. Analyzing the NVD summary for a given vulnerability. For that we parse the CVE summary with regular expressions to search for mentions of vulnerable functions. Changes to functions mentioned are then weighted three times as heavy as regular changes.\\\\
	Like mentioned in observation 2, we could also slightly increase the accuracy by not ignoring changes in non-code files/lines. Though the overall improvements to the heuristic were very minor. Table 1 shows a comparison of our heuristic and the one used in \cite{perl2015vccfinder}. It should be noted that when the heuristic blames multiple commits the same amount of times, we create a mapping for each commit. This is because in practice we cannot manually analyze all of these cases as this happens quite frequently. This further lowers the accuracy.
	\\(full results are on the predserver which is down at the time of writing, will update later)
	\\\begin{tabularx}{385pt}{c|c|c|c}
		\toprule
		\textbf{Heuristic}&\textbf{Training}&\textbf{Validation}&\textbf{Testing}\\
		\midrule
		vccfinder&&&\\\midrule
		ours&True: 434, False: 655&True: 151, False: 209&\\
		\bottomrule
	\end{tabularx}


	\subsection{Confidence value}
	Since the amount of noise even our improved heuristic produces is rather high, we are concerned that this might negatively impact the training of our classifier. This is why we assign a confidence value for each VCC the heuristic finds. The process of deriving whether or not we are confident in a VCC is as follows:\\\\
	1. If multiple commits are blamed the most or if the CWE of the vulnerability is one that we showed in 5.2 is \textit{hard} for the heuristic (CWE 264, 665, 704), then we are \textit{not confident}.\\
	2. If 1. is not true and the difference in blames for the most blamed commit and the second most blamed commit is greater than the total blames divided by three and the number of different commits blamed by the heuristic is less than 5, we are \textit{confident}.\\\\
	(Results for testing the confidence value are also on the predserver, will update later)
	\iffalse Testing this confidence value on our ground truth dataset showed that it should reduce noise by 70\% while only reducing true VCCs by 30\%. \fi
	\section{Building the dataset for our classifier}
	With the finished heuristic, we can build the dataset on which we will train, validate and test our classifier.\\
	\subsection{Validation and Test set}
	Like we established earlier, we do not tolerate any noise in our validation and test set. This is why, for those sets we choose to only use VCCs found by the Ubuntu Security Team. We also decide to use a temporal split between validating and testing, to make sure that our classifier will keep the same level of performance on newer commits. Additionally, we want to simulate a real world scenario, therefore we use every commit that was made to the Linux kernel repository in 2016 as our validation set and every commit that was made to the Linux kernel repository in 2017 as our test set. With this split we can accurately answer the question: "How would our classifier have fared, if it had been used for the development of the Linux kernel in 2017?". We choose these years, because we don't have a sufficient number of VCCs for a more recent time frame yet. The tables below show the distribution of VCCs to non-VCCs for both sets. Note, that every non-VCC is actually an unclassified commit, because there is a possibility that the commit introduced a vulnerability that we do not know about/has not been linked to it yet.\\
	\\\\\\Validation:\\\begin{tabularx}{205pt}{l|c|c}
		\toprule
		\textbf{Repository}&\textbf{VCCs}&\textbf{non-VCCs}\\
		&ground truth&unclassified\\
		\midrule
		linux&50&62293\\
		\bottomrule
	\end{tabularx}\\
	\\\\Testing:\\\begin{tabularx}{205pt}{l|c|c}
		\toprule
		\textbf{Repository}&\textbf{VCCs}&\textbf{non-VCCs}\\
		&ground truth&unclassified\\
		\midrule
		linux&51&65309\\
		\bottomrule
	\end{tabularx}

	\subsection{Training set}
	For the training set we use our heuristic on fixing commits accumulated by Manuel Brack for his thesis \cite{ManuelThesis}. In total we find (add number when it's final) VCCs across 8 different repositories. We also added the commits found by Andrew Meneely for chromium and HTTPD. We categorize these VCCs in ground truth, confident and not confident in order to assign different weights to them later in the training process. For the non-VCCs we simply choose a number of random commits for each repository that do not overlap with the known VCCs. It should be noted that we do not include a single commit from the Linux kernel in our training set. This is because we want to ensure that our classifier generalizes well for all possible repositories and not only performs well on the ones it is trained on.\\
	\\Training:\\\begin{tabularx}{335pt}{l|ccc|c}
		\toprule
		\textbf{Repository}&&\textbf{VCCs}&&\textbf{non-VCCs}\\
		&not confident&confident&ground truth&unclassified\\
		\midrule
		chromium&1368&324&89&tbd\\
		httpd&137&66&121&tbd\\
		gecko-dev&1940&536&0&tbd\\
		FFmpeg&105&58&0&tbd\\
		openssl&72&22&0&tbd\\
		postgres&56&8&0&tbd\\
		wireshark&148&84&0&tbd\\
		tcpdump&99&16&0&tbd\\
		\bottomrule
	\end{tabularx}\\\\

	\chapter{Classifier}
	This last chapter deals with
	closely based on vccfinder with some key improvements
	\section{Linear Support Vector Machine}
	Just like vccfinder we choose a linear Support Vector Machine (SVM) as our method for classifying commits. This has two main reasons:\\\\
	- SVMs are able to scale even with large amounts of data\\
	- SVMs can provide reasons for their classifications\\\\
	To explain why this is the case we have to go into detail how SVMs work.

	\subsection{Support Vector Classifiers in the first Dimension}
	Assume we want to classify people into male and female solely based on their height. The image below shows possible training data for such a problem, with red dots representing females and blue dots representing males.
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/plot.png}
	\end{figure}
	\\To decide whether an unlabeled data point (the gender of the person is unknown) should be classified as male or female, a threshold can be chosen. Every person with a height below the threshold is classified as female and every person with a height above the threshold is classified as male. The orange line shows a possible threshold that fits our training data.
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/plot3.png}
	\end{figure}
	\\This, however, is a rather poor choice, because an unlabeled data point with a height just above the threshold would be classified as male even though it is much closer to the female data points.
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/plot7.png}
	\end{figure}
	\\A better choice would be to set the threshold at the midpoint between the inner edges of each cluster. The distance between these edges and the threshold is called the \textit{margin} (green), thus this is a \textit{Maximal Margin Classifier}. Now, the unlabeled data point gets classified as female which is much more likely according to our training data.
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/plot2.png}
	\end{figure}
	\\This type of classifier is applicable in some scenarios but problems arise when the training data contains outliers. In the next example, a male with below average height in the training set causes the threshold with the maximal margin to be too close to the cluster of females again.
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/plot5.png}
	\end{figure}
	\\To fix this issue, a classifier has to be allowed to purposely miss-classify data points from the training data in order to achieve a better performance when classifying unknown data. This is called a \textit{Soft Margin Classifier} or \textit{Support Vector Classifier}. The two data points at the edges and every data point within the margin are called support vectors hence the name.
	\begin{figure}[!h]
	  \includegraphics[width=\linewidth]{Images/plot8.png}
	\end{figure}

	\subsection{Support Vector Classifiers in higher Dimensions}
	Assume that in addition to our height data, we also knew the weight of each person in the training set. This data can be visualized in a two dimensional coordinate system.
	\begin{figure}[!h]
		\subfigure{\includegraphics[width=.5\linewidth]{Images/plot9.png}}
		\subfigure{\includegraphics[width=.5\linewidth]{Images/plot10.png}}
	\end{figure}
	\\Instead of an explicit value, the Support Vector Classifier would now form an one dimensional straight line and base its decisions on the distance of a new data point to that line. For three dimensional data the SVC would form a two dimensional plane and for n dimensional data the SVC would form an n-1 dimensional hyperplane. The hyperplane can be written as a set of points ${\vec{x}}$ satisfying
	\begin{align*}
		{\vec{w}}\cdot{\vec{x}}-b=0
	\end{align*}
	where ${\vec{w}}$ is the normal vector to the hyperplane, $\frac{b}{||{\vec{w}}||}$ is the offset of the hyperplane from the origin along the vector ${\vec{w}}$ and the support vectors satisfy ${\vec{w}}\cdot{\vec{x}}-b=\pm1$ depending on which class they are in.\\
	For our prediction model we are using a \textit{linear} Support Vector Machine. Linear SVMs are a type of SVC that assumes that the data it deals with is linearly separable. Thanks to this assumption they are able to process very high dimensional feature vectors in a moderate amount of time. In addition to that they can also provide a humanly understandable reason for why they make a decision. This is because one can easily see which feature had the biggest impact in a decision by checking which one has the greatest distance to the hyperplane.
	\section{Features}
	Our SVM classifier will make decisions on whether or not a commit is prone to be vulnerable based on a high dimensional feature vector representing the commit. To calculate these features we first have to extract a bunch of raw data from each commit (see table below). In the next subsections we will discuss some of the more interesting features in detail.\\
	\begin{table}[H]
	\begin{tabularx}{.9\linewidth}{l|l|l}
		\toprule
		\textbf{Raw data}&\textbf{Description}&\textbf{Extraction}\\\midrule
		\multirow{2}{90pt}{\small{past different authors}}&\multirow{2}{200pt}{\small{Number of authors that changed the same files as the commit in the past}}&\small{git rev-list}\\\\\\
		\small{file count}&{\small{Number of different files the commit changed}}&\small{git diff}\\\\
		\small{past changes}&\multirow{2}{200pt}{\small{Number of times the files in the commit were changed in the past}}&\small{git rev-list}\\\\\\
		\small{hunk count}&{\small{Number of hunks in the commit}}&\small{git diff}\\\\
		\multirow{2}{90pt}{\small{author contribution percentage}}&\multirow{2}{200pt}{\small{Percentage of commits to the repository by the author}}&\small{git rev-list}\\\\\\
		\small{author touch count}&\multirow{2}{200pt}{\small{Number of times the author worked on the same files in the past}}&\small{git rev-list}\\\\\\
		\small{authored day}&{\small{Day on which the commit was authored}}&\small{git show}\\\\
		\small{authored hour}&\multirow{2}{200pt}{\small{Hour of the day on which the commit was authored}}&\small{git show}\\\\\\
		\small{commit message}&{\small{The commit message of the commit}}&\small{git show}\\\\
		\small{added code}&{\small{Lines of code that were added by the commit}}&\small{git show}\\\\
		\small{deleted code}&{\small{Lines of code that were deleted by the commit}}&\small{git show}\\
		\bottomrule
	\end{tabularx}
	\\\small{*Find the exact commands for extraction in the GitHub repository}
	\end{table}

	\subsection{Metadata features}
	The key idea brought forward in \cite{vccfinder} is that combining GitHub metadata with code data can be beneficial in finding prone to be vulnerable commits. This is why metadata features are a vital element of our classifier. Some of the interesting features include:\\\\
	Feature: Average past different authors\\
	Calculation: $\frac{past\ different\ authors}{file\ count}$\\
	Rationale: A high value of this feature may indicate that the experience of working on the files changed by the commit is distributed across many developers.\\\\
	Feature: Author contribution percentage\\
	Calculation: \textit{raw data point}\\
	Rationale: New contributors to the repository are more likely to make mistakes \cite{}.\\\\
	Feature: Author experience\\
	Calculation: $\frac{author\ touch\ count}{past\ changes}$\\
	Rationale: If the authors experience with the files in the commit is low, the probability that he introduces a vulnerability may be higher.\\\\
	Feature: Authored hour\\
	Calculation: \textit{raw data point}\\
	Rationale: If a commit was authored in a 4 am grind compared to more usual working hours, the developer might have been unconcentrated and thus could have made a mistake.\\\\
	Find a full list of the extracted metadata features in the appendix.

	\subsection{Code metric features}
	We also want to consider how often specific C/C++ keywords are being used in the commit. The rationale behind this is that it has been shown that some keywords (e.g. \textit{goto}) are more prone to error than others \cite{}. To access this data we split \textit{added code} and \textit{deleted code} at the C/C++ delimiters (\textbackslash s + - * / ; > < = ( ) [ ] \{ \}) and simply counted the occurrences of each keyword. In addition to considering the total occurrences of each keyword, we also added the average use in each file to our feature vector. We also keep track on how functions are inserted/removed. To measure this we use following regular expression to find function declarations in the code:\\\\
	\begin{footnotesize}
	\verb/\s*(?:(?:inline|static)\s+){0,2}\w+\s+\*?\s*\w+\s*\([^!@#$+%^;]+?\)\s*\{/
	\end{footnotesize}

	\subsection{Code/Commit message features}
	For code- and commit message features we utilize three separate bag-of-words models. Bag-of-words come from the area of natural language processing and are a method of representing text while disregarding grammar and word order. Our three bags-of-words consist of a set of tokens $S_{add},\ S_{del},\ S_{msg}$ respectively, where each token represents the occurrence of a specific word in a text.\\\\
	$S_{add}$ : Each individual word that was added in a commit contained in the training set.\\\\
	$S_{del}$ : Each individual word that was removed in a commit contained in the training set.\\\\
	$S_{msg}$ : Each individual word that was used in a commit message of the commits contained in the training set.\\\\
	For $S_1$ and $S_2$ we again use the C/C++ delimiters to determine where a word stops and for $S_3$ we use the stop words of the English language. We also remove all comments from added- and deleted code before extracting the tokens as comments do not change functionality. This is done by utilizing the GNU Compiler Collection:\\\\
	\$ gcc -fpreprocessed -dD -E code.c\\\\
	We can add these bags-of-words in the feature vector of a commit $c$ by appending $b(c, s)\ \forall s\in S_{add|del|msg}$, with\\
	\[b(c, s) =
	\begin{cases}
		1 & if\ token\ s\ is\ in\ c\\
		0 & else.
	\end{cases}
	\]
	to it. This results in very large but sparse feature vectors but like discussed in the previous section, this high dimensionality does not pose a problem for our linear SVM.
It should be noted that we initially adjusted the appended values for term frequency times inverse document-frequency but discarded this approach when it resulted in lower performance for reasons discussed in the next section.\\

	\section{Evaluation}
	\section{Comparison}
	\subsection{Flawfinder}
	Flawfinder (version 2.0.11) is a mature, open source static analysis tool for C and C++ code. It comes with an integrated database of functions that are known to make code prone to be vulnerable. To detect vulnerabilities or "flaws", the tool tries to match the source code against these functions. For example, the use of \textit{strcpy()} may indicate a risk of a buffer overflow vulnerability while \textit{chmod()} is often associated with race conditions. It can detect strings and comments too and thus is able to ignore them. Since Flawfinder is also able to operate on commits, it makes sense to compare its performance to the one of our classifier.\\
	To review a commit with Flawfinder, one has to create a \textit{unified diff} for each file the commit changed. For this, the \textit{git diff} command can be used to compare the commits version of a file to its parents version. The generated diff is then given to Flawfinder using the -P flag. Flawfinder can then analyze the files but only reports hits that relate to lines that were changed in the commit. To automate this process we wrote this bash script (\ref{fig:bash})
	\begin{figure}[h]
	  \includegraphics[width=\linewidth]{Images/flawfinder.png}
	  \label{fig:bash}
	\end{figure}
	\\\\\\The table below shows the results of running the script over every commit in our test set. Flawfinder also has an option to reduce false positives, that we can enable by using the -F flag. As the table shows, it successfully reduces the reported false positives by 22\% but also misses out on one of the true positives.\\\\
	\begin{tabularx}{\linewidth}{lcccccc}
		\toprule
		&\multirow{2}{50pt}{\centering\textbf{True Positives}}&\multirow{2}{50pt}{\centering\textbf{False Positives}}&\multirow{2}{50pt}{\centering\textbf{False Negatives}}&\multirow{2}{50pt}{\centering\textbf{True Negatives}}&\textbf{Precision}&\textbf{Recall}\\\\
		\midrule
		\textbf{Flawfinder}&10&4569&41&60740&0.002&0.196\\
		with -F&9&3573&42&61735&0.0025&0.1764\\
		\bottomrule
	\end{tabularx}

	\subsection{Vccfinder}
	- Precision + Recall Graph\\
	- Replicate to the best of our abilities. Key differences:\\
	- Unterschiede -> features, preprocessing, feature selection, hyperparameters, Bag of words\\
	\section{Results}
	- flagged unclassifieds\\
	- neue gefundene VCCs\\
	(- takeaways)\\
	\section{Limitations}
	- biased weil confident\\
	\section{Conclusion}


	\iffalse
	\begin{tabularx}{\linewidth}{@{}p{.28\linewidth}*6{>{\centering\arraybackslash}X}@{}}
		\toprule
		\textbf{Feature}&\textbf{mean VCCs}&\textbf{mean others}&\textbf{median VCCs}&\textbf{median others}&\textbf{U}&\textbf{effect size}\\\midrule
		Additions&1050.42&49.5&115.0&8.0&100569.5&1.67\\
		Deletions&56.67&25.47&6.0&4.0&274219.0&0.09\\
		Past changes&501.95&233.53&181.5&58.5&195956.5&0.35\\\midrule
		Future changes&763.69&221.43&299.5&51.0&149374.0&0.5\\\midrule
		Past different authors&93.0&42.54&46.5&17.0&189609.0&0.37\\\midrule
		Future different authors&123.49&39.77&61.5&17.0&141173.0&0.53\\\midrule
		Hunk count&25.26&10.51&11.0&4.0&157240.0&0.48\\\midrule
		Added functions&22.85&0.83&2.0&0.0&130367.0&0.57\\\midrule
		Deletedfunctions&0.75&0.2&0.0&0.0&280250.5&0.07\\\midrule
		Contributions in project&0.3&0.16&0.04&0.06&280408.5&0.07\\
		\bottomrule
	\end{tabularx}

	\begin{tabularx}{\linewidth}{@{}p{.08\linewidth}*6{>{\centering\arraybackslash}X}@{}}
		\toprule
		\textbf{Keyword}&\textbf{mean VCCs}&\textbf{mean others}&\textbf{median VCCs}&\textbf{median others}&\textbf{U}&\textbf{effect size}\\\midrule

		if&69.8&4.25&13.0&1.0&101521.5&0.66\\\midrule
		int&35.86&2.74&6.0&0.0&137202.0&0.54\\\midrule
		struct&87.17&5.94&13.0&0.0&120209.0&0.6\\\midrule
		return&37.91&2.14&6.0&0.0&125843.0&0.58\\\midrule
		static&21.0&1.85&3.0&0.0&144671.0&0.52\\\midrule
		void&17.74&1.64&2.0&0.0&160691.5&0.47\\\midrule
		unsigned&10.43&0.97&1.0&0.0&179083.0&0.41\\\midrule
		goto&10.27&0.47&1.0&0.0&155651.0&0.48\\\midrule
		sizeof&9.87&0.42&1.0&0.0&168316.5&0.44\\\midrule
		break&9.54&0.31&0.0&0.0&194050.0&0.36\\\midrule
		char&7.12&0.33&0.0&0.0&218243.0&0.28\\
		\bottomrule
	\end{tabularx}
	\fi

	\chapter{Appendix}

	\section{Introduction}
	During their research, Jan Wagner and Manuel Brack noticed some discrepancies between Vulnerability-Contributing-Commits (VCCs) as reported by the VCCFinder heuristic and the Ubuntu Kernel Security Team. Ten of those disputed CVEs were randomly picked for further manual analysis. The picked CVEs, along with their fixing and reported VCCs, are listed in the following table.
	\begin{figure}[ht]
	  \includegraphics[width=\linewidth]{Images/table_nikos.png}
	  \label{fig:table_nikos}
	\end{figure}


	\section{Analysis}
	Results of the manual analysis follow.
	\subsection{CVE-2014-5206}
	Date: 08/18/2014
	\\Type: CWE-264 -- Permissions, Privileges, and Access Controls
	\\Description: The do\_remount function in fs/namespace.c in the Linux kernel through 3.16.1 does not maintain the MNT\_LOCK\_READONLY bit across a remount of a bind mount, which allows local users to bypass an intended read-only restriction and defeat certain sandbox protection mechanisms via a ``mount -o remount'' command within a user namespace.
	\\Notes: VCCFinder blames a commit that cleans up code. It changes the expression for the mount flag from a literal constant to a bitwise operation on constants defined in the header file. No functionality difference should exist.
	\\Reason for discrepancy: This is an interesting one. It seems that the vulnerability is introduced in the commit blamed by the Ubuntu security team that allows non-root users to perform operations on namespaces. Before then, there was no reason to protect the flags. Specifically in the SYSCALL definition of umount.
	\\Verdict: The commit blamed by the Ubuntu teams seems to be the VCC. It seems very difficult for a blame-based heuristic to pinpoint this commit. The VCC pinpointed by VCCFinder is definitely wrong though, as it is just refactoring. Namespaces are a very high-risk location for those kind of vulnerabilities. This VCC is also responsible for the related CVE-2014-5207. Interestingly, these vulnerabilities did not affect the Debian stable distribution, where by default user namespaces are disabled. Commits that change a lot of if statements that affect system calls should be considered high-risk, especially when they advertise introducing intrinsically high-risk functionalities. Maybe a good recommendation would be, when dealing with access control bugs to look more into changes to if statements. Another recommendation would be to cosider such files (i.e. files that regulate access) as high-risk. Another observation is that such kinds of semantic flaws can only be automatically found when metadata, such as the commit messages are available.
	\subsection{CVE-2013-4127}
	Date: 07/29/2013
	\\Type: CWE-399 -- Resource Management Errors
	\\Description: Use-after-free vulnerability in the vhost\_net\_set\_backend function in\\ drivers/vhost/net.c in the Linux kernel through 3.10.3 allows local users to cause a denial of service (OOPS and system crash) via vectors involving powering on a virtual machine.
	\\ Notes: This is a use-after-free vulnerability and thus should be able to be detected automatically pretty easily. The VCC proposed by VCCFinder is not correct. It seems to only be copying code from one file to another. The commit proposed by the Ubuntu security team seems to be the correct VCCs. This is also noted in the commit message of the fix.
	\\Reason for discrepancy: It is impossible to trace code back to its original file after it has been copied to another file. That being said, in this specific case, the vulnerability was introduced in the file that the code was copied to.
	\\Verdict: The VCC of the Ubuntu Security Team is correct. If a heuristic finds code copying, then maybe it would make sense to look into previous commits of the source file. Naturally, for a use-after-free vulnerability, we can expect the VCC to free memory.
	\subsection{CVE-2013-1819}
	Date: 03/06/2013
	\\Type:CWE-20 -- Improper Input Validation
	\\ Description: The \_xfs\_buf\_find function in fs/xfs/xfs\_buf.c in the Linux kernel before 3.7.6 does not validate block numbers, which allows local users to cause a denial of service (NULL pointer dereference and system crash) or possibly have unspecified other impact by leveraging the ability to mount an XFS filesystem containing a metadata inode with an invalid extent map.
	\\ Notes: The correct commit is also blamed by the VCCFinder heuristic but is not the most dominant.
	\\Reason for discrepancy: Empty line contributes to the mistake. Also, blaming where the declaration of variables takes place could generally be bad practice.
	\\Verdict: The Ubuntu Security Team seems to have the correct commit. VCCFinder produces a wrong commit, however we can learn from this mistake.
	\subsection{CVE-2013-7348}
	Date: 04/01/2014
	\\Type: CWE-399 -- Resource Management Errors
	\\ Description: Double free vulnerability in the ioctx\_alloc function in fs/aio.c in the Linux kernel before 3.12.4 allows local users to cause a denial of service (system crash) or possibly have unspecified other impact via vectors involving an error condition in the aio\_setup\_ring function.
	\\ Notes: A double-free vulnerability can be easily detected by automated tools. We expect to blame the commit that introduces the 2nd free. In this case, it is more complicated than that. The vulnerability is introduced by the Ubuntu commit. This commit does not add a new free command. It changes the point where the error handling code ``goes-to''.
	\\Reason for discrepancy: The fix was to remove one of the free instructions. The more general free instruction was removed, however the vulnerability was introduced in the more specific function. The phrase in the commit message: ``clean up ioctx\_alloc()'s error path'' is what points to the problem.
	\\Verdict: The Ubuntu commit is correct, however it was not trivial at all to see. Go-to in errror handling seems to be dangerous. It is difficult to see how a blame-based heuristic would find the right commit.
	\subsection{CVE-2014-5045}
	Date: 08/01/2014
	\\Type: CWE-59 -- Improper Link Resolution Before File Access (`Link Following')
	\\ Description: The mountpoint\_last function in fs/namei.c in the Linux kernel before 3.15.8 does not properly maintain a certain reference count during attempts to use the umount system call in conjunction with a symlink, which allows local users to cause a denial of service (memory consumption or use-after-free) or possibly have unspecified other impact via the umount program.
	\\ Notes: This is simple. The Ubuntu-blamed commit created the file and introduced the vulnerability.
	\\Reason for discrepancy: The VCCFinder commit, as its message says, is massaging the code.
	\\Verdict: The Ubuntu commit is the correct one. The VCCFinder commit just refactored the code in the area. Maybe going 2 steps back in time would have been beneficial. It does not seem very difficult to find this VCC automatically. The patterns existed before.
	\subsection{CVE-2012-1097}
	Date: 05/17/2012
	\\Type: NVD-CWE-Other
	\\ Description: The regset (aka register set) feature in the Linux kernel before 3.2.10 does not properly handle the absence of .get and .set methods, which allows local users to cause a denial of service (NULL pointer dereference) or possibly have unspecified other impact via a (1) PTRACE\_GETREGSET or (2) PTRACE\_SETREGSET ptrace call.
	\\ Notes: This is a strange one. The two commits blamed by the Ubuntu team and VCCFinder were made the same day by the same person. The VCCFinder commit just adds some code to the first commit. This vulnerability in general has to do with error-checking. The reason behind it is that some developers did not follow what the expected behaviour of the functions were based on their header-file definitions. It is tough to blame a specific commit for such a vulnerability.
	\\Reason for discrepancy: The VCCFinder commit adds some code to the earlier Ubuntu commit. Another possible commit to blame is 4206d3aa1978e44f58bfa4e1c9d8d35cbf19c187.
	\\Verdict: This vulnerability appeared when function implementations started diverging from the definitions in the header files. All 3 possible blame-able commits could be valid.
	\subsection{CVE-2014-0196}
	Date: 05/07/2014
	\\Type: CWE-362 -- Concurrent Execution using Shared Resource with Improper Synchronization ('Race Condition')
	\\ Description: The n\_tty\_write function in drivers/tty/n\_tty.c in the Linux kernel through 3.14.3 does not properly manage tty driver access in the "LECHO \& !OPOST" case, which allows local users to cause a denial of service (memory corruption and system crash) or gain privileges by triggering a race condition involving read and write operations with long strings.
	\\ Notes: This is an interesting case. There was a lot of discussion in the Red Hat issue tracking system about this vulnerability\footnote{\url{ https://bugzilla.redhat.com/show_bug.cgi?id=1094232}}. Such kinds of concurrency bugs can be difficult to analyze. This bug seems to be caused by an update to the pty driver file, while the fix was introduced in the tty base driver file.
	\\Reason for discrepancy: Concurrency bugs are sometimes difficult to handle. In this case it would have been better to blame the line in the middle of the added locks.
	\\Verdict: The Ubuntu commit seems to be the correct one. There has been a lot of discussion about this bug.
	\subsection{CVE-2013-1979}
	Date: 05/03/2013
	\\Type: CWE-264 -- Permissions, Privileges, and Access Controls
	\\ Description: The scm\_set\_cred function in include/net/scm.h in the Linux kernel before 3.8.11 uses incorrect uid and gid values during credentials passing, which allows local users to gain privileges via a crafted application.
	\\ Notes: This is again a semantic bug. There is a clear mention in the fixing commit that the introducing commit is the Ubuntu one.
	\\Reason for discrepancy: VCCFinder blames a commit that changed the code but not its functionality. The vulnerable code was in the called function body. The vulnerability was introduced 2 years before.
	\\Verdict: The Ubuntu commit is the correct one. It seems very difficult to spot this VCC with blame heuristics. VCCFinder does a good job of blaming the specific lines, however the funtionality of the blamed lines existed in a function call from earlier. It remains to be seen if it is worth it to look for such behavior (i.e. look if one of the blamed lines is a function call and then look if the blamed code was part of the function body...). In general, the approach where after pinpointing a VCC some checks are carried out to see if we should go even further back in time (maybe with git dissect?) could be beneficial. This is a very difficult case for blame-based VCCs.
	\subsection{CVE-2011-1479}
	Date: 06/21/2012
	\\Type: CWE-399 -- Resource Management Errors
	\\ Description: Double free vulnerability in the inotify subsystem in the Linux kernel before 2.6.39 allows local users to cause a denial of service (system crash) via vectors involving failed attempts to create files. NOTE: this vulnerability exists because of an incorrect fix for CVE-2010-4250.
	\\ Notes: As noted in the description, this is a regression bug. VCCFinder blames a lot of older commits, since the changes in the fix affect a lot of lines. The most blamed commit is much older (4 years older than the real VCC).
	\\Reason for discrepancy: The double-free is not direct. It happens via a function.
	\\Verdict: The Ubuntu commit is the correct one. It seems very difficult for heuristics to find this regression bug, since the fix changes a lot of things.
	\subsection{CVE-2013-4470}
	Date: 11/04/2013
	\\Type: CWE-264 -- Permissions, Privileges, and Access Controls
	\\ Description: The Linux kernel before 3.12, when UDP Fragmentation Offload (UFO) is enabled, does not properly initialize certain data structures, which allows local users to cause a denial of service (memory corruption and system crash) or possibly gain privileges via a crafted application that uses the UDP\_CORK option in a setsockopt system call and sends both short and long packets, related to the ip\_ufo\_append\_data function in net/ipv4/ip\_output.c and the ip6\_ufo\_append\_data function in net/ipv6/ip6\_output.c.
	\\ Notes: Another semantic bug (access control). There are 2 very similar fixing commits, one each for IPv4 and IPv6.
	\\Reason for discrepancy: There is no discrepancy according to the latest results. Maybe some change to the heuristic improved performance.
	\\Verdict: No discrepancy. The improvement seems to come from not taking into account the comments.
	\section{Discussion}
	\begin{itemize}
	    \item The Ubuntu commits are as close to a ground truth dataset as we can get
	    \item Manual analysis is difficult and it seems previous research fell in the trap of wanting to confirm what the heuristic produced.
	    \item Variable declarations may not need to be considered (as well as empty lines)
	    \item After having a candidate VCC, checking if you should blame one of its ancestors may be beneficial.
	    \item Semantic bugs (e.g. access control) seem to be a bit more difficult to trace.
	    \item Concurrency bugs seem difficult to trace.
	    \item Disregarding copied or refactored code may improve performance.
	    \item If a function was replaced, it may be beneficial to look inside the function and see if the functionality changed.
	    \item Not taking into account the comments may be beneficial.
	    \item For the use-case of commit-based static analysis, noise in the training data may even improve performance. Noise in the evaluation data is not acceptable. Hence, the suggestion would be to use the Ubuntu ground truth as the testing dataset and improve the heuristic based on the recommendations in this paper.
	    \item Some vulnerability types (semantic, concurrency) are inherently more difficult to trace.
	\end{itemize}
	\section{Conclusion} The VCCs from the Ubuntu Security Team seem to be as close to a ground truth as possible. They agree with the detailed manual analysis of the above-mentioned 10 bugs. There is no reason to believe that manual analysis by us would lead to better results than what is achieved by the Ubuntu Security Team (who takes in to account commit messages and RHL reports). Some vulnerabilities are inherently difficult to trace with automated heuristics. However, existing heuristics can be improved. Tracing the VCC manually seems much harder than implied in most previous work (maybe due to confirmation bias).

	\printbibliography

\end{document}
